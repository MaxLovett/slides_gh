<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Cross-Survey Analysis and Dynamic Comparative Public Opinion</title>
    <meta charset="utf-8" />
    <meta name="author" content="Yue Hu" />
    <meta name="date" content="2021-06-10" />
    <script src="dcpo_files/header-attrs-2.8/header-attrs.js"></script>
    <link href="dcpo_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="dcpo_files/tile-view-0.2.4/tile-view.css" rel="stylesheet" />
    <script src="dcpo_files/tile-view-0.2.4/tile-view.js"></script>
    <link href="dcpo_files/tachyons-4.12.0/tachyons.min.css" rel="stylesheet" />
    <link href="dcpo_files/panelset-0.2.4/panelset.css" rel="stylesheet" />
    <script src="dcpo_files/panelset-0.2.4/panelset.js"></script>
    <script src="dcpo_files/js-cookie-3.0.0/js.cookie.js"></script>
    <script src="dcpo_files/peerjs-1.3.1/peerjs.min.js"></script>
    <script src="dcpo_files/tiny.toast-1.0.0/toast.min.js"></script>
    <link href="dcpo_files/xaringanExtra-broadcast-0.2.4/broadcast.css" rel="stylesheet" />
    <script src="dcpo_files/xaringanExtra-broadcast-0.2.4/broadcast.js"></script>
    <link rel="stylesheet" href="zh-CN_custom.css" type="text/css" />
    <link rel="stylesheet" href="style_ui.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Cross-Survey Analysis and Dynamic Comparative Public Opinion
### Yue Hu
### 2021-06-10

---


class: inverse, bottom




+ Common problem
+ Methodological solution
+ A new weapon
+ [Un]expected findings 

# Overview

---

class: center, middle

.bg-black.golden.ba.shadow-5.ph4.mt3.center[
Social scientists are interested in complex, abstract concepts.
]


.large[Common objectives]    
Development, culture, democracy...

\+ 

.large[Rich data]    
WVS, Global Barometers, ?GSS, Pew ...


.large[.red[&amp;#10504;]]

.large[Clear patterns]


---

## E.g.: Democracy &amp;larr;?&amp;rarr; Public support

Support .navy[&amp;uarr;] &amp;rarr; democracy .navy[&amp;uarr;] .small[(e.g., Inglehart &amp; Welzel 2005, Claassen 2019)]    
Support .navy[&amp;uarr;] &amp;rarr; democracy .red[&amp;#8675;] .small[(e.g., Fails &amp; Pierce 2010)]

???

Countries with greater democratic support have been found to become stronger and more stable democracies

Inglehart, Ronald and Christian Welzel. 2005. *Modernization, Cultural Change, and Democracy*. Cambridge: Cambridge University Press. 

Fails, Matthew D. and Heather Nicole Pierce. 2010. “Changing Mass Attitudes and Democratic Deepening.” *Political Research Quarterly* 63(1):174–187.

(Insignificant positive with one-year survey, and insig negative with multiple years)

--

Democracy .navy[&amp;uarr;] &amp;rarr; support .navy[&amp;uarr;] .small[(e.g., Wuttke et. al. 2020) ]   
Democracy .navy[&amp;uarr;] &amp;rarr; support .red[&amp;darr;] .small[(e.g., Foa &amp; Mounk 2017)]  
Democracy .red[&amp;darr;] &amp;rarr; support .navy[&amp;uarr;] .small[(Claassen 2020)]

???
More experience with democracy yields more democratic support 
vs.
Long-established democracies are suffering from democratic fatigue 

Wuttke, Alexander, Christian Schimpf and Harald Schoen. 2020. “When the Whole Is Greater
than the Sum of Its Parts: On the Conceptualization and Measurement of Populist Attitudes and Other Multidimensional Constructs.” *American Political Science Review* 114(2):356–374.

Foa, Roberto Stefan and Yascha Mounk. 2017. “The Signs of Deconsolidation.” *Journal of Democracy* 28(1):5–15

thermostatic effect. 

---

## Puzzle Makers: Invisible Opinions

.red[Different] questions

.center[&lt;img src="../conference/2021-AsianPoliMeth/images/sparsingData.png" height = 450 /&gt;]

???
Why are there mixed results? 
Difficutlies in measuring. 
First, pub sp is latent variable, unobservable. Uncertainty. 
Sec, a big challenge is the sparced and fragmented data at aggregate level. 
churchihill items,  democracy has its probles, still the best one. 
milltary rule items, military should govern the country. what do you think. 
Worldwide survey project, 4 years. 
across countries, overtime. 

---

.red[Different] respondents

.pull-left[
&lt;img src="../conference/2021-AsianPoliMeth/images/demo_leaving.gif" height = 350 /&gt;

]

--

.pull-right[
&lt;img src="../conference/2021-AsianPoliMeth/images/demo_enter.gif" height = 350 /&gt;

]

???

Different respondents have different concepts of democracy and different way to support democracy. 
United Kindom, lawmakers leave. 
Swarming Capital defending democracy.  

---

class: center, middle, large

Different questions   
Different people   
&amp;dArr;    
.red[Incomparable] data   


--

w. 

.blue[Latent variable analysis]


---

class: inverse, bottom

+ Variable extraction
+ Individual IRT
+ Group IRT
+ DCPO

# Methodology Moment

---

.center[World of Latent Variable Analysis]

.pull-left[
### Factorial Models

1. Exploratory Factor Analysis
1. Confirmative Factor Analysis
1. Structural Equation Model
]

.pull-right[
### Typological Models

1. Item Response Theory (IRT)
1. Cross-Group Comparison(MrP, GIRT, DCPO)
]

--

.center[
## Operation

R&lt;sup&gt;\*&lt;/sup&gt;    
[`mirt`](https://github.com/philchalmers/mirt/wiki);    
[`DCPO`](https://github.com/fsolt/DCPO)
]

.footnote[[\*] R packages for IRT &gt; [50](https://www.tandfonline.com/doi/full/10.1080/15366367.2019.1586404?src=recsys).]

---

## IRT

Why not factor analyses?

--

1. The outcome is assumed .red[continual];
1. Different types (categorical, binary, ordinal, continual) of indicators are assumed .red[identical] in nature;
1. Ignoring .red[cross-group] differences;
1. EFA cannot address the .red[covariance] among indicators;
1. CFA: Simpler theory or .red[low-fit] model.

???

CFA理论通常简略，只涉及一部分indices，但实际可能很复杂；当囊括更多indices测量质量会高，但不符合理论。

---

class: middle


## Why IRT


1. Created for .red[noncontinuous] variables (binary &amp;rarr; ordinal + continuous);

--

1. Open to incorporate with .red[Bayesian] inference and no .red[scaling] issue;

--

1. Better dealing with .red["don't know"s] within the Bayesian framework

--

1. Being Enable .red[cross-group] comparison.

---

## Individual-Level IRT

Usage: Within-survey latent variable analysis

Objective:

1. Yes/No;
1. Responses that can convert to binary;
3. Ordinal responses(e.g., Liker scale questions).

---

class: large

### IRT Assumption

1. Monotonicity
1. Unidimensionality
1. Local independence
1. Parameter invariance

---

class: center, middle, large

# Monotonicity

Trait level &amp;uarr; &amp;prop; Pr(=1) &amp;uarr;

&lt;img src="../courses/governmentalBigData/images/irt_icc.png" height = 350 /&gt;

???

As the trait level is increasing, the probability of a correct response (=1) also increases.

单增趋势：随潜在变量增加，获得1的可能性也随之增加。

Item characteristic curve

比如随着能力的提高，回答正确答案的机会也就越高

---

class: center, middle, large

# Unidimensionality

One dominant latent variable &amp;rarr; Observed items

???

One dominant latent trait driving force for the responses observed for each item

+ 聚合的项目均指向同一个潜在变量。  
+ 基于理论  

--

Until the multidimensional IRT

---

class: center, middle, large

# Local Independency

.red[cov(items) &amp;larr; latent trait]

P(y&lt;sub&gt;iq&lt;/sub&gt;,y&lt;sub&gt;i'q&lt;/sub&gt; | &amp;theta;&lt;sub&gt;q&lt;/sub&gt;) = P(y&lt;sub&gt;iq&lt;/sub&gt; | &amp;theta;&lt;sub&gt;q&lt;/sub&gt;)P(y&lt;sub&gt;i'q&lt;/sub&gt; | &amp;theta;&lt;sub&gt;q&lt;/sub&gt;)

???

Responses given to the separate items in a test are mutually independent given a certain level of ability.

对于每一项目（e.g.,一道题）的响应(e.g., 选择的选项)间的关联性.red[只]来自.red[共同]的潜在变量。

换言之，控制潜在变量影响后，问题间响应相互独立

---

class: center, middle, large

# Parameter Invariance

Invariance among items    
Invariance among respondents (DIF)


???

+ Parameters在项目间不变
+ Parameters在响应人群间不变&lt;sup&gt;1&lt;/sup&gt;
    + 当进行Multiple Group IRT时尤可能被违反

Easy to be avoid when involving multi-group IRT

.footnote[&lt;sup&gt;\*&lt;/sup&gt; Check the differential item functioning (DIF) with Wald and likelihood-ratio approach.]

---

## IRT Models

### Individual IRT
Rasch Model (1PL)     
&amp;rarr; Two-Parameter Logistic Model (2PL)     
&amp;rarr; Three-Parameter Logistic Model (3PL)     
&amp;rarr; Four-Parameter Logistic Model (4PL)

### Group IRT

MIRT    
&amp;rarr; DGIRT    
&amp;rarr; DCPO

---

## Rasch Model (1PL)

+ y&lt;sub&gt;iq&lt;/sub&gt;&amp;isin;{0,1}: subject `i`'s score on question `q`
+ &amp;theta;&lt;sub&gt;i&lt;/sub&gt;&amp;isin;{-&amp;infin;, +&amp;infin;}: Unbounded latent trait
+ &amp;sigma;&lt;sub&gt;q&lt;/sub&gt;: Difficulty

.pull-left[.center[.orange[Pr(y&lt;sub&gt;iq&lt;/sub&gt; = 1)] = ]]
.pull-right[.navy[logist&lt;sup&gt;-1&lt;/sup&gt;(&amp;theta;&lt;sub&gt;i&lt;/sub&gt; - &amp;sigma;&lt;sub&gt;q&lt;/sub&gt;])]


???

Rasch /resh/  

Difficulty: 不同的问题回答肯定答案的难易度不一样, 通常显示为z scores， 低于0 表示比较简单
+ 当面临重大公共卫生威胁时，政府应该及时响应，采取果断措施
+ 政府是否可以牺牲少数民众安全和权力，来换取大多数社会成员的公共卫生安全时

--

.pull-left[
.orange[Item Response]
]

.pull-right[
.navy[Response Theory]
]

---

## An Simple Example (Bock &amp; Lieberman 1970)

Law School Admissions Test, sec 7  
5 yes/no questions

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;     Item.1 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;     Item.2 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;     Item.3 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;     Item.4 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;     Item.5 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Min.   :0.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Min.   :0.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Min.   :0.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Min.   :0.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Min.   :0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st Qu.:1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st Qu.:0.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st Qu.:1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st Qu.:0.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st Qu.:1.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Median :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Median :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Median :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Median :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Median :1.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mean   :0.828 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mean   :0.658 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mean   :0.772 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mean   :0.606 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mean   :0.843 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd Qu.:1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd Qu.:1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd Qu.:1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd Qu.:1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd Qu.:1.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Max.   :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Max.   :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Max.   :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Max.   :1.000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Max.   :1.000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???

[`mirt` Workshop 1](http://philchalmers.github.io/mirt/extra/mirt-Workshop-2015_Day-1.pdf)

---

## Difficulty Parameter


```r
m_lsat &lt;- mirt(df_lsat, model = 1, itemtype = "Rasch", verbose = FALSE)
coef(m_lsat, simplify = TRUE) %&gt;% 
  kable(format = "html")
```

&lt;table class="kable_wrapper"&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt; 

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; a1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; d &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; g &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; u &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Item.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8680718 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Item.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7909134 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Item.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.4608233 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Item.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5214399 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Item.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.9927710 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 &lt;/td&gt;
   &lt;td&gt; 

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; x &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; F1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 &lt;/td&gt;
   &lt;td&gt; 

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; F1 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; F1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.021944 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???

The difficulty parameter is the `d`

The rest parameters we'll talk in 2pl and 3pl

---

class: center, middle

.bg-black.golden.ba.shadow-5.ph4.mt3.center[
.large[Please always diagnose your results and .red[understand] what you are diagnosing.]

.tr[
— Dr. Hu
]]

---

## IRT Diagnoses

+ General level：Global fit
+ Item level：Item fit &amp; residual
+ Respondent level：Personal fit

---

## Global Fit&lt;sup&gt;1&lt;/sup&gt;

`$$G^2 = 2[\sum_l^s r_lln(\frac{r_l}{N\tilde{P}_l})]$$`

N: Number of respondents  
l: Possible options   
r: Number of respondents who respond with l

--

When item &gt; 10，M&lt;sup&gt;2&lt;/sup&gt;, M&lt;sup&gt;2&lt;/sup&gt;* (Modified version of G&lt;sup&gt;2&lt;/sup&gt;)


```r
M2(m_lsat)
```

```
##             M2 df          p     RMSEA   RMSEA_5   RMSEA_95      SRMSR       TLI
## stats 23.17287 10 0.01012618 0.0363126 0.0168065 0.05586376 0.04744033 0.9401263
##             CFI
## stats 0.9401263
```


.footnote[
[1] RMSEA, SRMSR, CFI, TLI also works
]

???

REDO: @Maydeu-Olivares2013

N is the number of subjects, L is number of possible response patterns, `\(P_ l\)` is the estimated probability of observing response pattern l, and `\(r_ l\)` is the number of subjects who have response pattern l

&amp;chi; sig   
Tucker-lewis index, 1 ideal, &lt; .95 poor   
Comparative fit index, the same   
Root mean square error of approximation, 0 perfect, &lt;.05 good, [.05, .08] ok, &gt; .1 poor    
Standardized root mean square residual &lt;.08 acceptable

---

## Item Characteristic Curves （ICC）

&lt;img src="dcpo_files/figure-html/icc-1.png" style="display: block; margin: auto;" /&gt;

???

检查各题affirmative的难易程度，看逐个是不是大体同一个趋势

---

## Test Charactersitic Curve

&lt;img src="dcpo_files/figure-html/tcc-1.png" style="display: block; margin: auto;" /&gt;

???

TCC： 所有ICC之和，体现how reliable, information 理想是形成一个钟形,顶尖处代表平均水平，据此对比个人&amp;theta;可以判断这人是否是outlier
SE(&amp;theta;) = (test)&lt;sup&gt;-1/2&lt;/sup&gt;

---

## Item Diagnostics

Covariation-based residuals


```r
residuals(m_lsat)
```

```
## LD matrix (lower triangle) and standardized values:
## 
##        Item.1 Item.2 Item.3 Item.4 Item.5
## Item.1     NA -0.017  0.020  0.022  0.019
## Item.2  0.292     NA  0.105 -0.042 -0.064
## Item.3  0.389 10.976     NA  0.007  0.007
## Item.4  0.474  1.801  0.055     NA -0.052
## Item.5  0.362  4.063  0.045  2.691     NA
```

???

看item residual的协变程度，多用于看multidimensionality, 不应有关联


---

Single item/person fit 


```r
# Item 
itemfit(m_lsat, fit_stats = "infit")
```

```
##     item outfit z.outfit infit z.infit
## 1 Item.1  0.744   -3.597 0.939  -1.025
## 2 Item.2  0.758   -7.500 0.826  -6.303
## 3 Item.3  0.711   -5.420 0.860  -3.202
## 4 Item.4  0.770   -8.877 0.818  -7.962
## 5 Item.5  0.797   -2.572 0.993  -0.081
```


```r
# Person
personfit(m_lsat)
```

```
##         outfit   z.outfit     infit     z.infit          Zh
## 1    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 2    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 3    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 4    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 5    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 6    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 7    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 8    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 9    0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 10   0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 11   0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 12   0.6420958 -0.9001784 0.6953214 -0.96202244  0.93429336
## 13   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 14   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 15   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 16   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 17   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 18   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 19   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 20   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 21   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 22   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 23   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 24   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 25   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 26   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 27   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 28   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 29   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 30   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 31   0.8178336 -0.6300162 0.8365515 -0.58965745  0.64983876
## 32   1.4702553  1.6126462 1.4447540  1.61101087 -1.73598829
## 33   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 34   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 35   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 36   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 37   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 38   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 39   1.3927122  1.1849894 1.3937871  1.27940982 -1.28637186
## 40   1.0428516  0.2464647 1.0613929  0.32004646 -0.21273742
## 41   1.0428516  0.2464647 1.0613929  0.32004646 -0.21273742
## 42   1.0428516  0.2464647 1.0613929  0.32004646 -0.21273742
## 43   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 44   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 45   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 46   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 47   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 48   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 49   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 50   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 51   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 52   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 53   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 54   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 55   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 56   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 57   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 58   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 59   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 60   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 61   1.0023472  0.1156527 0.9806210  0.03358782  0.03962718
## 62   1.6793416  1.8478659 1.5976464  1.80384093 -2.03724960
## 63   1.6793416  1.8478659 1.5976464  1.80384093 -2.03724960
## 64   1.6793416  1.8478659 1.5976464  1.80384093 -2.03724960
## 65   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 66   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 67   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 68   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 69   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 70   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 71   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 72   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 73   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 74   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 75   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 76   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 77   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 78   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 79   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 80   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 81   1.4319232  0.9040008 1.3542229  0.89387872 -0.83441775
## 82   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 83   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 84   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 85   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 86   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 87   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 88   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 89   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 90   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 91   1.3283603  1.1925623 1.3435562  1.29493298 -1.29902533
## 92   1.2804056  0.9003924 1.2754654  0.95116392 -0.90599298
## 93   1.2804056  0.9003924 1.2754654  0.95116392 -0.90599298
## 94   1.2804056  0.9003924 1.2754654  0.95116392 -0.90599298
## 95   1.2804056  0.9003924 1.2754654  0.95116392 -0.90599298
## 96   1.2804056  0.9003924 1.2754654  0.95116392 -0.90599298
## 97   1.9574000  2.4221949 1.8924908  2.48855647 -2.98286976
## 98   1.9574000  2.4221949 1.8924908  2.48855647 -2.98286976
## 99   1.9574000  2.4221949 1.8924908  2.48855647 -2.98286976
## 100  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 101  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 102  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 103  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 104  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 105  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 106  1.7753892  1.3847459 1.6548212  1.43482759 -1.53366231
## 107  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 108  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 109  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 110  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 111  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 112  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 113  1.5670351  1.5978712 1.4793247  1.50529970 -1.65687072
## 114  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 115  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 116  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 117  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 118  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 119  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 120  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 121  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 122  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 123  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 124  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 125  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 126  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 127  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 128  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 129  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 130  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 131  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 132  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 133  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 134  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 135  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 136  1.3208296  0.7321411 1.2189235  0.62398564 -0.55314427
## 137  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 138  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 139  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 140  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 141  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 142  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 143  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 144  2.2014754  1.9014751 1.8368418  1.73106099 -2.08890342
## 145  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 146  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 147  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 148  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 149  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 150  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 151  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 152  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 153  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 154  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 155  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 156  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 157  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 158  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 159  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 160  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 161  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 162  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 163  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 164  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 165  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 166  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 167  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 168  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 169  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 170  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 171  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 172  1.7248030  1.0070459 1.3171687  0.66469738 -0.64120315
## 173  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 174  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 175  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 176  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 177  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 178  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 179  0.8741944 -0.3967143 0.8874432 -0.37066077  0.44763366
## 180  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 181  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 182  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 183  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 184  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 185  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 186  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 187  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 188  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 189  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 190  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 191  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 192  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 193  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 194  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 195  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 196  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 197  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 198  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 199  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 200  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 201  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 202  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 203  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 204  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 205  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 206  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 207  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 208  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 209  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 210  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 211  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 212  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 213  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 214  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 215  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 216  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 217  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 218  0.7925432 -0.5792464 0.8207813 -0.54337697  0.61448416
## 219  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 220  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 221  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 222  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 223  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 224  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 225  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 226  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 227  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 228  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 229  1.4695376  1.3709638 1.4378067  1.39677740 -1.46239261
## 230  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 231  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 232  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 233  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 234  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 235  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 236  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 237  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 238  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 239  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 240  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 241  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 242  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 243  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 244  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 245  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 246  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 247  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 248  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 249  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 250  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 251  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 252  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 253  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 254  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 255  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 256  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 257  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 258  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 259  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 260  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 261  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 262  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 263  1.1241484  0.4025567 1.2099241  0.60533914 -0.40933625
## 264  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 265  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 266  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 267  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 268  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 269  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 270  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 271  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 272  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 273  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 274  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 275  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 276  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 277  1.0791727  0.3456501 1.0246406  0.18131966 -0.13639358
## 278  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 279  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 280  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 281  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 282  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 283  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 284  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 285  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 286  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 287  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 288  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 289  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 290  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 291  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 292  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 293  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 294  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 295  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 296  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 297  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 298  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 299  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 300  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 301  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 302  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 303  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 304  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 305  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 306  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 307  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 308  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 309  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 310  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 311  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 312  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 313  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 314  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 315  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 316  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 317  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 318  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 319  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 320  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 321  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 322  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 323  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 324  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 325  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 326  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 327  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 328  0.6695888 -0.5442290 0.7740264 -0.43771232  0.57118178
## 329  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 330  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 331  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 332  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 333  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 334  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 335  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 336  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 337  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 338  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 339  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 340  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 341  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 342  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 343  1.5502346  1.0775136 1.3919447  0.96589638 -0.96457736
## 344  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 345  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 346  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 347  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 348  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 349  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 350  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 351  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 352  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 353  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 354  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 355  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 356  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 357  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 358  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 359  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 360  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 361  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 362  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 363  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 364  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 365  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 366  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 367  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 368  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 369  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 370  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 371  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 372  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 373  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 374  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 375  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 376  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 377  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 378  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 379  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 380  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 381  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 382  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 383  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 384  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 385  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 386  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 387  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 388  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 389  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 390  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 391  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 392  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 393  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 394  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 395  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 396  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 397  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 398  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 399  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 400  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 401  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 402  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 403  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 404  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 405  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 406  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 407  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 408  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 409  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 410  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 411  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 412  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 413  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 414  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 415  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 416  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 417  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 418  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 419  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 420  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 421  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 422  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 423  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 424  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 425  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 426  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 427  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 428  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 429  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 430  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 431  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 432  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 433  0.6987004 -0.1458057 0.8877658  0.03007247  0.24566817
## 434  1.3572311  1.0967850 1.3194850  1.07556375 -1.08201373
## 435  1.3572311  1.0967850 1.3194850  1.07556375 -1.08201373
## 436  1.3572311  1.0967850 1.3194850  1.07556375 -1.08201373
## 437  1.3572311  1.0967850 1.3194850  1.07556375 -1.08201373
## 438  1.3572311  1.0967850 1.3194850  1.07556375 -1.08201373
## 439  1.3572311  1.0967850 1.3194850  1.07556375 -1.08201373
## 440  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 441  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 442  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 443  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 444  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 445  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 446  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 447  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 448  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 449  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 450  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 451  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 452  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 453  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 454  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 455  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 456  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 457  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 458  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 459  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 460  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 461  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 462  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 463  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 464  1.0130548  0.1990675 1.0746247  0.31312964 -0.12806278
## 465  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 466  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 467  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 468  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 469  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 470  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 471  1.8937006  1.5358478 1.6925430  1.49794029 -1.66382193
## 472  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 473  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 474  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 475  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 476  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 477  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 478  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 479  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 480  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 481  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 482  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 483  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 484  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 485  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 486  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 487  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 488  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 489  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 490  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 491  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 492  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 493  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 494  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 495  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 496  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 497  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 498  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 499  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 500  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 501  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 502  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 503  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 504  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 505  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 506  1.2172721  0.5209021 1.1877070  0.49012142 -0.30589776
## 507  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 508  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 509  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 510  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 511  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 512  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 513  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 514  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 515  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 516  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 517  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 518  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 519  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 520  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 521  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 522  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 523  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 524  1.4391410  0.9148544 1.2566453  0.70116074 -0.68330389
## 525  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 526  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 527  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 528  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 529  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 530  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 531  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 532  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 533  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 534  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 535  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 536  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 537  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 538  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 539  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 540  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 541  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 542  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 543  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 544  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 545  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 546  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 547  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 548  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 549  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 550  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 551  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 552  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 553  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 554  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 555  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 556  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 557  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 558  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 559  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 560  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 561  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 562  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 563  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 564  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 565  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 566  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 567  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 568  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 569  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 570  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 571  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 572  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 573  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 574  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 575  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 576  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 577  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 578  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 579  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 580  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 581  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 582  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 583  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 584  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 585  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 586  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 587  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 588  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 589  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 590  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 591  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 592  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 593  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 594  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 595  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 596  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 597  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 598  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 599  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 600  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 601  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 602  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 603  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 604  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 605  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 606  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 607  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 608  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 609  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 610  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 611  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 612  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 613  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 614  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 615  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 616  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 617  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 618  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 619  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 620  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 621  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 622  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 623  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 624  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 625  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 626  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 627  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 628  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 629  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 630  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 631  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 632  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 633  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 634  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 635  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 636  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 637  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 638  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 639  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 640  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 641  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 642  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 643  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 644  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 645  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 646  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 647  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 648  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 649  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 650  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 651  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 652  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 653  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 654  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 655  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 656  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 657  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 658  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 659  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 660  0.5553197 -0.3875340 0.7374012 -0.24077410  0.46753742
## 661  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 662  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 663  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 664  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 665  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 666  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 667  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 668  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 669  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 670  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 671  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 672  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 673  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 674  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 675  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 676  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 677  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 678  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 679  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 680  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 681  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 682  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 683  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 684  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 685  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 686  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 687  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 688  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 689  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 690  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 691  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 692  1.9240664  1.1715632 1.3496272  0.70665713 -0.74387339
## 693  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 694  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 695  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 696  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 697  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 698  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 699  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 700  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 701  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 702  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 703  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 704  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 705  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 706  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 707  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 708  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 709  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 710  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 711  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 712  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 713  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 714  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 715  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 716  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 717  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 718  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 719  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 720  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 721  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 722  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 723  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 724  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 725  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 726  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 727  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 728  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 729  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 730  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 731  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 732  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 733  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 734  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 735  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 736  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 737  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 738  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 739  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 740  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 741  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 742  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 743  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 744  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 745  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 746  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 747  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 748  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 749  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 750  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 751  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 752  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 753  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 754  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 755  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 756  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 757  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 758  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 759  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 760  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 761  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 762  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 763  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 764  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 765  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 766  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 767  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 768  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 769  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 770  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 771  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 772  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 773  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 774  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 775  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 776  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 777  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 778  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 779  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 780  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 781  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 782  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 783  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 784  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 785  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 786  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 787  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 788  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 789  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 790  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 791  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 792  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 793  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 794  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 795  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 796  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 797  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 798  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 799  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 800  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 801  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 802  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 803  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 804  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 805  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 806  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 807  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 808  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 809  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 810  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 811  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 812  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 813  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 814  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 815  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 816  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 817  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 818  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 819  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 820  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 821  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 822  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 823  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 824  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 825  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 826  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 827  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 828  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 829  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 830  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 831  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 832  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 833  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 834  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 835  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 836  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 837  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 838  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 839  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 840  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 841  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 842  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 843  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 844  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 845  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 846  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 847  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 848  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 849  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 850  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 851  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 852  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 853  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 854  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 855  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 856  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 857  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 858  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 859  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 860  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 861  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 862  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 863  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 864  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 865  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 866  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 867  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 868  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 869  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 870  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 871  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 872  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 873  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 874  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 875  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 876  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 877  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 878  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 879  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 880  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 881  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 882  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 883  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 884  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 885  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 886  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 887  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 888  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 889  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 890  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 891  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 892  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 893  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 894  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 895  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 896  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 897  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 898  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 899  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 900  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 901  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 902  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 903  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 904  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 905  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 906  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 907  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 908  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 909  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 910  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 911  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 912  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 913  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 914  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 915  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 916  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 917  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 918  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 919  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 920  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 921  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 922  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 923  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 924  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 925  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 926  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 927  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 928  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 929  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 930  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 931  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 932  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 933  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 934  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 935  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 936  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 937  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 938  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 939  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 940  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 941  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 942  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 943  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 944  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 945  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 946  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 947  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 948  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 949  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 950  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 951  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 952  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 953  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 954  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 955  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 956  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 957  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 958  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 959  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 960  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 961  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 962  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 963  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 964  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 965  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 966  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 967  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 968  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 969  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 970  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 971  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 972  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 973  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 974  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 975  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 976  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 977  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 978  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 979  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 980  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 981  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 982  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 983  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 984  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 985  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 986  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 987  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 988  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 989  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 990  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 991  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 992  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 993  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 994  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 995  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 996  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 997  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 998  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 999  0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
## 1000 0.1482615 -0.7478428 0.1847431 -0.99648352  0.80314589
```


???

Z&lt;sub&gt;h&lt;/sub&gt; &gt; 0 better

infit/outfit, close to 1 is good



---

## If There's a Problem


1. Check S-&amp;chi;&lt;sup&gt;2&lt;/sup&gt;, local dependency
1. Using more complex model types, e.g.,&amp;rarr; 2PL, 3PL;
1. Binary to polytomous or nominal response models
1. Non-parametric smoothing techniques (e.g., Mokken scaling)

---

## Cons of Rasch Model

Measurement error

.center[&lt;img src="../courses/governmentalBigData/images/irt_elementarySchool2G.png" height = 450 /&gt;]

???

人们对同一个题理解不同，回答出affirmative答案可能性也不同。

---

## Two-Parameter Logistic Model (2PL IRT)

.center[

**Rasch**: Pr(y&lt;sub&gt;iq&lt;/sub&gt; = 1) = logist&lt;sup&gt;-1&lt;/sup&gt;(&amp;theta;&lt;sub&gt;i&lt;/sub&gt; - &amp;sigma;&lt;sub&gt;q&lt;/sub&gt;)

**2PL**: Pr(y&lt;sub&gt;iq&lt;/sub&gt; = 1) = logist&lt;sup&gt;-1&lt;/sup&gt;(.red[&amp;kappa;&lt;sub&gt;q&lt;/sub&gt;]&amp;theta;&lt;sub&gt;i&lt;/sub&gt; - &amp;sigma;&lt;sub&gt;q&lt;/sub&gt;)

]

&amp;kappa;&lt;sub&gt;q&lt;/sub&gt;: Discrimination (Parameter of dispersion)

???

Discrimination: how well a question can different people from below to above;

Rule of thumb above 1 meaning a good question in terms of examination

--

### Another format

`$$Pr(y_{iq} = 1) = logist^{-1}[\frac{\theta_i - {\color{red}{\beta_q}}}{\color{red}{\alpha_q}}]$$`


&amp;beta;&lt;sub&gt;q&lt;/sub&gt;: &amp;sigma;&lt;sub&gt;q&lt;/sub&gt; &amp;frasl; &amp;kappa;&lt;sub&gt;q&lt;/sub&gt;, threshold("difficulty", location)  
&amp;alpha;&lt;sub&gt;q&lt;/sub&gt;: &amp;kappa;&lt;sub&gt;q&lt;/sub&gt;&lt;sup&gt;-1&lt;/sup&gt;, dispersion (slop)

???

Dispersion: magnitude of the measurement error, 有能力答对但打错by chance. 

---

.center[&lt;img src="../conference/2021-AsianPoliMeth/images/2pirt.png" height = 600 /&gt;]

???

Difficulty/discrimination: how well a question can different people from below to above

Dispersion: magnitude of the measurement error

---


```r
m_lsat2PL &lt;-  mirt(df_lsat, model = 1, itemtype = "2PL", verbose = FALSE)
coef(m_lsat2PL, simplify = TRUE)
```

```
## $items
##           a1     d g u
## Item.1 0.988 1.856 0 1
## Item.2 1.081 0.808 0 1
## Item.3 1.706 1.804 0 1
## Item.4 0.765 0.486 0 1
## Item.5 0.736 1.855 0 1
## 
## $means
## F1 
##  0 
## 
## $cov
##    F1
## F1  1
```

---

## Do We Really Need 2PL?

.center[&lt;img src="../courses/governmentalBigData/images/irt_OccamsRazor.gif" height = 100 /&gt;]

--

Likelihood-Ratio Test


```r
anova(m_lsat, m_lsat2PL) %&gt;%  
  select(AIC, SABIC, HQ, logLik, df, p) %&gt;% 
  kable(format = "html")
```

```
## 
## Model 1: mirt(data = df_lsat, model = 1, itemtype = "Rasch", verbose = FALSE)
## Model 2: mirt(data = df_lsat, model = 1, itemtype = "2PL", verbose = FALSE)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; AIC &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SABIC &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; HQ &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; logLik &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; df &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5341.802 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5352.192 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5352.994 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2664.901 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NaN &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NaN &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5337.610 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5354.927 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5356.263 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2658.805 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0159822 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## What If...

.center[&lt;img src="../courses/governmentalBigData/images/irt_purelyGuess.jpg" height = 400 /&gt;]


.center[

&amp;dArr;    
A lot of .red[low &amp;theta;] respondents

]

???

很多人全凭猜怎么办？

---

## Three-Parameter Logistic Model (3PL)

`$$Pr(y_{iq} = 1) = \color{red}{c_i + (1 - c_i)}logist^{-1}[\frac{(\theta_i - \beta_q)}{\alpha_q}]$$`

c&lt;sub&gt;i&lt;/sub&gt;：Item lower asymptote ("guessing")

--

 Tip: Recommend N &gt; 1,000

???

极大增加演算成本&amp;rarr;通常需要1000以上观测点

---

## What If...

.center[&lt;img src="../courses/governmentalBigData/images/irt_idontcare.gif" height = 400 /&gt;]

---

### Four-Parameter Logistic Model (4PL)

`$$Pr(y_{iq} = 1) = c_i + (\color{red}{d_i} - c_i)logist^{-1}[\frac{(\theta_i - \beta_q)}{\alpha_q}]$$`


c&lt;sub&gt;i&lt;/sub&gt;：Item lower asymptote ("guessing")    
d&lt;sub&gt;i&lt;/sub&gt;：Item .red[upper] asymptote ("carelessness"), d &lt; 1

--

 Tip: Considering we already need .red[1,000] for 3PL....

???

鉴于3PL已经需要1000-ish观测点……

---

## Extentions: Unidimentional to Multidimentional

~~Unidimensionality~~


Multidimentional IRT (MIRT, Phil Chalmers, 2015)

`$$Pr(y_{iq} = 1) = logist^{-1}[\frac{\boldsymbol{\theta_i} - \beta_q}{\boldsymbol{\alpha_q}}]$$`


.center[**&amp;theta;&lt;sub&gt;i&lt;/sub&gt;** and **&amp;alpha;&lt;sub&gt;q&lt;/sub&gt;**: Scalar &amp;rarr; Matrix。]

???

Pyschologist


1. Monotonicity
1. Unidimensionality
1. Local independence
1. Parameter invariance

---

## Extension: Binary to Ordinal

Logit &amp;rarr; Cumulative logit

Pr(y&lt;sub&gt;iq&lt;/sub&gt; = 1) &amp;rarr; `\(Pr(\frac{y_{iq}\leq c}{y_{iq}&gt;c})\)`

--

&lt;img src="dcpo_files/figure-html/twoDimension-1.png" style="display: block; margin: auto;" /&gt;

---

## Ordinal IRT: Three Oft-Used Modeling

1. (Modified) Graded Response Model (GRM)
    + Better for scoring rubrics, e.g., Likert

--

1. (Generalized) Partial Credit Model (PCM)，Rating Scale Model (RSM)
    + Quite similar to GRM
    + Not require boundary parameters to be ordered
    + Need larger sample size

--

1. Nominal Response Model
    + Better for categorical

???

https://stats.stackexchange.com/questions/402440/how-should-we-select-between-various-item-response-theory-models-e-g-rsm-grm

The rating scale model (RSM) and partial credit model (PCM) are Rasch models. The graded response model (GRM) and the generalized partial credit model are non-Rasch (multinomial).


https://studylib.net/doc/18099752/irt-models-for-polytomous-response-data

---

class: inverse, bottom

# Group IRT

---

## Fixing the Measurement Error Due to the Group Variance

.center[&lt;img src="../courses/governmentalBigData/images/countryBias.png" height = 300 /&gt;]

Multilevel Mixture IRT with Item Bias Effects (Stegmueller 2011)

`$$Pr(y_{iq} = 1) = logist^{-1}[\frac{\theta_i - {\beta_q}}{\color{red}{\alpha_q}}]$$`


Using random effect estimating &amp;alpha;&lt;sub&gt;q&lt;/sub&gt;


???

Daniel Stegmueller, Duke U, poli sci

&amp;alpha;: Dispersion: magnitude of the measurement error 

---

## Finding the Group-Level Latent Variables

Group mean of individual responses

.center[y&lt;sub&gt;kq&lt;/sub&gt; = &amp;Sigma;y&lt;sub&gt;ikq&lt;/sub&gt; &amp;frasl; n.]

--

Aggregation Issues:

1. The mean does not have much substantial influence to the result if the group `k` is too small.
1. Different `y`s have different contributions to the latent variable.

???

问题：

1. 如果群组过小，其平均值的代表意义不大
2. 不同的指标对于潜在变量贡献不一样

---

## Getting the Group Mean Right

Multilevel Regression and Post-stratification (MrP)

.red[Predict] a parameter of interest within .red[small domains] through .red[modeling the mean] of the variable of interest .red[conditional on poststratification counts]. 


???

Gelman, Andrew, and Thomas C. Little. 1997. “Poststratification Into Many Categories Using Hierarchical Logistic Regression.” Survey Methods 23: 127--135.


经过群组信息（地理、人口）加权的平均值


--

1. A population is .red[divided] into `H` strata of interest (for example states);

--

2. The parameters of interest are the .red[means in each strata] &amp;theta;&lt;sub&gt;h&lt;/sub&gt; (h = 1...H);

--

3. Every stratum is cross-classified by some .red[demographics] j &amp;isin; H, besides every .red[population count] N&lt;sub&gt;j&lt;/sub&gt; is known;

--

4. All of the population means &amp;mu;&lt;sub&gt;j&lt;/sub&gt; can be estimated by using some statistical technique, such as .red[multilevel regression].

.pull-left[
`$$\theta_h = \frac{\sum_{j \in h} N_j \mu_j }{\sum_{j \in h} N_j}$$`
]
.pull-right[
N: Population of the stratum (Census)
]

???

.Small[
1. 将总体（population）按群组（strata，如国家、地区）切分；
1. 估测对象为核心变量在每个群组中的平均值/比例， &amp;theta;&lt;sub&gt;h&lt;/sub&gt; (h &amp;isin; {1, H});
1. 已知各群组以人口变量j（如老年男性、青年女性等）划分，群组人口（N&lt;sub&gt;j&lt;/sub&gt;）或占总人口比；
1. 各组总体平均值&amp;mu;&lt;sub&gt;j&lt;/sub&gt;可通过multilevel model 进行估算。
]

---

## Example

Source: Economic variables of 2,396 industrial companies in a particular year

Goal: Average income of industries by .red[each of the five existing zones](&amp;theta;&lt;sub&gt;1~5&lt;/sub&gt;). 

Company levels and zones (the population, "company census")

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; A &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; B &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; C &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; D &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; E &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Big &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 30 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Medium &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 121 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 111 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 187 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 138 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Small &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 97 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 593 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 862 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???

https://www.r-bloggers.com/gelmans-mrp-in-r-what-is-this-all-about/


---

Fortunately, we know the .red[true] averages at this time.


&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Zone &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; income &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 652.28 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; B &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 320.75 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 331.02 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; D &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 684.98 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; E &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 767.39 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

A random sample of 1,000 from the 2,396 population

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="dcpo_files/figure-html/rawVsTrue-1.png" alt="Sample Means vs. True Means"  /&gt;
&lt;p class="caption"&gt;Sample Means vs. True Means&lt;/p&gt;
&lt;/div&gt;

---

class: Small

## Step I: Mr

.center[
Income = &amp;beta;&lt;sub&gt;0z&lt;/sub&gt; + &amp;beta;&lt;sub&gt;1z&lt;/sub&gt;Level&lt;sub&gt;iz&lt;/sub&gt; + &amp;epsilon;&lt;sub&gt;iz&lt;/sub&gt;

&amp;beta;&lt;sub&gt;0z&lt;/sub&gt; = &amp;gamma;&lt;sub&gt;00&lt;/sub&gt; + &amp;gamma;&lt;sub&gt;01&lt;/sub&gt;Zone&lt;sub&gt;z&lt;/sub&gt; + u&lt;sub&gt;0z&lt;/sub&gt;]

--

Output: Post-strata means


```r
# Step 1: &lt;&lt;MR&gt;&gt; - Multilevel regression
M1 &lt;- lmer(Income ~ Level + (1 | Zone), data = SLucy)
SLucy$Pred &lt;- predict(M1)

# Summary
sum &lt;- group_by(SLucy, Zone, Level) %&gt;% 
  summarise(mean2 = mean(Pred))
Mupred &lt;- matrix(sum$mean2, ncol = 5, nrow = 3)

rownames(Mupred) &lt;- levels(SLucy$Level)
colnames(Mupred) &lt;- levels(SLucy$Zone)

Mupred %&gt;% kable(format = "html", digits = 2)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; A &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; B &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; C &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; D &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; E &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Big &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1274.74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1148.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1189.59 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1238.51 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1251.95 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Medium &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 706.19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 580.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 621.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 669.96 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 683.40 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Small &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 372.95 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 246.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 287.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 336.72 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 350.16 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## Step II: P

N&lt;sub&gt;z&lt;/sub&gt; &amp;times; weighted mean / N&lt;sub&gt;z&lt;/sub&gt;


```r
colSums(Np * Mupred) / count(Lucy, Zone)$n
```

```
##        A        B        C        D        E 
## 656.4551 318.3753 326.6951 680.8675 754.5761
```

---

## Comparision

&lt;img src="dcpo_files/figure-html/mrpVsraw-1.png" style="display: block; margin: auto;" /&gt;

---

## What MrP Can't Solve

+ Question uniqueness &amp;times; Zones
+ Scale of questions
+ Measurement Error
+ Time trend

--

After, all, MrP is still a mean measurement of the latent variable.

--

### Solution

Dynamic Group-level IRT (DGIRT, Caughey &amp; Warshaw 2015)

???

Combining IRT and MrP plus dynamic element

---

## DGIRT

1. Estimate IRT within each group;    
&amp;rarr; .red[Latent variable] in
1. Embed the group IRT in a multilevel framework;    
&amp;rarr; .red[Group] in
1. Allowing hierarchical parameters to evolve according to a dynamic model;    
&amp;rarr; .red[Time] in
1. Weight the estimates with for groups.    
&amp;rarr; .red[MrP] in


---

## Latent Variable Estimates at the Group Level

Individual

`$$p_{iq} = logist^{-1}[\frac{\theta_i - {\beta_q}}{\alpha_q}]$$`

--

Group     
(k &amp;isin; {1...K})

`$$\eta_{ktq} = logit^{-1}(\frac{\color{red}{\bar{\theta}_{kt}}- \beta_q}{\sqrt{\alpha^2_q + \color{red}{(1.7\sigma_{kt})^2}}}).$$`

.center[
`\(\bar{\theta}_kt\)` and &amp;sigma;&lt;sub&gt;kt&lt;/sub&gt; is the mean and SD the latent variable in Group `k` at time `t`.
]

???

1.7: sd of probit is (&amp;pi;/3)&lt;sup&gt;1/2&lt;/sup&gt; for logit, while Long 1997 found it is more close to 1.7 in actual estimations.

Mislevy, Robert J. 1983. “Item Response Models for Grouped Data.” Journal of Educational Statistics 8(4): 271–88.

&amp;eta;: eta 

---

## Modeling Time and Space

`$$\bar{\theta}_k\sim N(\xi_t + \boldsymbol{x'_k\gamma}, \sigma^2_{\bar{\theta}})$$`

.center[&amp;xi;&lt;sub&gt;t&lt;/sub&gt; ~ N(&amp;xi;&lt;sub&gt;.red[t-1]&lt;/sub&gt;;&amp;sigma;&lt;sub&gt;&amp;gamma;&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;)]

.center[&amp;gamma;&lt;sub&gt;pt&lt;/sub&gt; ~ N(&amp;gamma;&lt;sub&gt;p,t-1&lt;/sub&gt;&amp;delta;&lt;sub&gt;t&lt;/sub&gt; + .red[**z'&lt;sub&gt;p.&lt;/sub&gt;&amp;eta;&lt;sub&gt;t&lt;/sub&gt;**], &amp;sigma;&lt;sub&gt;&amp;gamma;&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;)]

.center[n&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;kqt&lt;/sub&gt;]

???
 
&amp;xi;&lt;sub&gt;t&lt;/sub&gt;: xi

x 为群组级变量  
t-1, dynamic linear model  
**z'&lt;sub&gt;p.&lt;/sub&gt;&amp;eta;&lt;sub&gt;t&lt;/sub&gt;**: geography-level attributes, &amp;eta;是coefficients  
n&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;kqt&lt;/sub&gt;基于MrP

---

.pull-left[
### Pros

+ Including time, space, and questions
+ Increasing representativeness
]

--

.pull-right[
### Cons

+ Complicated
    + In weeks
]

???

Caughey &amp; Warshaw称会跑几个星期

--

### Simpler DGIRT (Claassen 2019)

Simplified 1. Only at the country level (no MrP then 😝)   
Simplified 2. From estimating &amp;alpha; (dispersion) to &amp;beta; (difficulty)  
Simplified 3. Ignoring polarization.


???

&amp;delta;&lt;sub&gt;kq&lt;/sub&gt;: 问题的difficulty随国家k变化。

---

## The Newest Development in Group IRT: DCPO

.pull-left[
.red[D]ynamic .red[C]omparative .red[P]ublic .red[O]pinion (Bayesian-Based)

### Complexity 

Claasseen 2019 &lt; DCPO &lt; DGIRT

]

.pull-right[
&lt;img src="../courses/governmentalBigData/images/fsolt.jpeg" height = 350 /&gt;

Frederick Solt (University of Iowa)
]

---


## DCPO

`$$Pr(y_{ktqr} = 1) = logist^{-1}[\frac{\bar{\theta}'_{kt} - {(\beta_{qr} + \color{orange}{\delta_{kq}})}}{\sqrt{\alpha^2_q + (1.7\times \color{red}{\sigma_{kt}})^2}}]$$`


.pull-left[
`\(\bar{\theta}\)`: Mean public opinion    
&amp;beta;: Question difficulty   
&amp;alpha;: Question dispersion
]

.pull-right[
.orange[&amp;delta;]: Country-specific item bias   
.red[&amp;sigma;:] Polarization in public opinion]

--

.green[Random walk priors] for opinion mean and sd

`$$\bar{\theta}_{ktqr}\sim N(\bar{\theta}'_{k, \color{green}{t-1}}, \sigma^2_{\bar{\theta}'}),\sigma_{kt}\sim LN(\sigma_{k, \color{green}{t-1}}, \sigma^2_{\sigma})$$`

???
D(dynamic)C(comparative)PO
model country specific-item to address equivalence. 
aggregate level public opinion 
polarizaiton 
ordinal scaled answers. 
The sophisticated measurement models ignite the hope on the studies of relationship between democratic development and public support. 

Claassen, Christopher. 2019. “Estimating Smooth Country–Year Panels of Public Opinion.” *Political Analysis* 27(1):1–20

Caughey, Devin, Tom O’Grady and Christopher Warshaw. 2019. “Policy Ideology in European
Mass Publics, 1981–2016.” American Political Science Review 113(3):674–693

Solt, Frederick. 2020b. “Modeling Dynamic Comparative Public Opinion.” SocArXiv
10.31235/osf.io/d5n9p.


---

background-image: url("../courses/governmentalBigData/images/irtCompare.png")
background-position: center
background-size: contain


???

Bounded: 使用logit归为0-1

---

## Validations

.center[&lt;img src="../courses/governmentalBigData/images/irtFitCompare.png" height = 300 /&gt;]

---

## Operation

1. Collect survey data and identify the survey items (.red[mostly automated]);
1. Generate raw dataset (using `DCPOtools`, .red[automated]);
1. Reformat dataset for model (using `DCPOtools`, .red[automated]);
1. Analyze the data with `DCPO`
1. Diagnosis for convergence with `shinystan`.


---

class: middle, center

## One More Thing and An .red[Important] Thing to Use DCPO .red[Correctly]!

No uncertainty engaged &amp;hArr; The latent variable is measured "perfectly".

&amp;dArr; 

Biased coefficients and standard errors.


--

.bg-black.golden.ba.shadow-5.ph4.mt3.center[
Uncertainty Matters!
]

---

### How to Cope?

Overimputations (Alternative: model of compositions)

1. .red[Repeated] Latent variable estimation (e.g., DCPO &amp;times; 1000);
1. Regressing with the .red[distribution];
1. .red[Combination] with Rubin's Rule.

???

Blackwell, Matthew, James Honaker, and Gary King. 2017. “A Unified Approach to Measurement Error and Missing Data: Overview.” Sociological Methods &amp; Research: 303–41.


---

## Going Against the Most Recent Findings 

Claassen 2020 AJPS; Claassen 2020 APSR

.pull-left[&lt;img src="../courses/governmentalBigData/images/irt_idontcare.gif" height = 400 /&gt;]

--

.pull-right[
### More Data together with Better Method

1988~2020 (144 countries, 33 years)

Comparing with Claassen (2020a, 2020b):

&gt; 26.2% .red[more] questions;     
33.3% .red[more] survey sources
]

---

background-image: url("../conference/2021-AsianPoliMeth/images/visualizeAJPS-1.png")
background-position: center
background-size: contain

## Findings

---

background-image: url("../conference/2021-AsianPoliMeth/images/visualizedAPSR-1.png")
background-position: center
background-size: contain

---

## Potential Explanations

.pull-left[
### Methodology

1. .red[Multidimensional] support;
1. Conceptual .red[Difference];
1. Relations with .red[other values].

]

???

survey questions do not ask respondents how they prioritize democracy relative to other values with which it may come into conflict, such as their partisanship.cannot capture the extent

--

.pull-right[
### Theory

1. Effectiveness .red[&gt;] existence
1. Support + .red[dissatisfaction]
1. .red[Elite] driven

]

???

It's is the combination of democratic support and dissatisfaction with current regime performance that generates demand for greater democracy

Qi, Lingling and Doh Chull Shin. 2011. “How Mass Political Attitudes Affect Democratization: Exploring the Facilitating Role Critical Democrats Play in the Process.” *International Political Science Review* 32(3):245–262

---

## Take-Home Points

1. Rethinking Democracy &amp;harr; Public support: .red[~~Sanguine~~] assessment
1. Cross-section time-series diverse-question method: .blue[DCPO]
    + `DCPO`
    + `DCPOtools`
    
&lt;iframe src="https://ctai.shinyapps.io/dmsweb/" height = 300 width = 1000&gt;&lt;/iframe&gt;
    

???
Uncertainty caused by measurement and in concept. 
without taking uncertainty into account, no way of knowing whether their conclusions are correct or not even their studies have been published in top journals. 

Sanguine, super optimistic fate of democracies rely on their public, but is not well grounded. 
practioners and researchers interested in democracy and understanding democracies should work hard to figour out what is going one. 
we point several potential explanations, but we dont' know which one is right. each of them is worth additional research. 

---

background-image: url("images/hives.png")
background-position: right bottom
background-size: 50%

class: inverse

# Thank you!

&lt;svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"&gt;&lt;/path&gt;  &lt;polyline points="22,6 12,13 2,6"&gt;&lt;/polyline&gt;&lt;/svg&gt;&amp;nbsp;[yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

&lt;svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;circle cx="12" cy="12" r="10"&gt;&lt;/circle&gt;  &lt;line x1="2" y1="12" x2="22" y2="12"&gt;&lt;/line&gt;  &lt;path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"&gt;&lt;/path&gt;&lt;/svg&gt;&amp;nbsp; https://sammo3182.github.io/

&lt;svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"&gt;&lt;/path&gt;&lt;/svg&gt;&amp;nbsp; [sammo3182](https://github.com/sammo3182)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
