---
title: "文本的数据分析进阶"
subtitle: "上海交通大学暑期社会科学方法班"
author: "胡悦"
institute: "清华大学社会科学学院" 
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  gridExtra,
  knitr, # dependency
  stringr, 
  tidytext, 
  tidyverse,
  lubridate,
  quanteda,
  quanteda.textstats,
  quanteda.textplots,
  quanteda.corpora,
  text2vec,
  LSX,
  seededlda,
  newsmap,
  keyATM,
  stm,
  tinytable,
  patchwork
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```



## 个人简介{.Small}

:::: {.columns}

::: {.column width="60%"}
*个人经历*

- 政治学博士[（University of Iowa)]{.small}
  - 信息学[（Graduated Certificate in Informatics)]{.small}
- 清华大学计算社会科学平台[(副主任)]{.small}
  - 清华数据与治理中心[(副主任)]{.small}
  - 计算社会科学编程语言证书项目[（负责人）]{.small}
  - Learning R with Dr. Hu & Friends 工作坊[（创始人）]{.small}

:::{.fragment}
*研究兴趣：认知、行为与现代性*

- **方法路径：计算政治学**
  - 实验室和调查实验
  - 潜变量分析、网络分析、空间分析
  - 文本大数据分析、数据可视化
:::

:::

::: {.column .fragment width="40%"}
*研究领域：比较政治、国家治理*

- **W. 心理学**
  - 政治[认知]{.red}治理
  - 行为公共[政策]{.red}
  - 政治传播

- **W. 经济学**
  - 经济不平等[感知]{.red}
  - 公共设施、服务均等化

- **W. 语言学**
  - 权力背书的[语言效果]{.red}与机制
  - 语言政策的治理功能

:::

::::

## 复习

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="50%"}
你应该已经知道……

- 你能用文本数据做什么
    - 你分析的是文字还是语言？
    - Close reading or distant reading？
    - 文本/音频/视频分析的理论基础是什么？
- 如何获取数据
    - 文本数据的获取渠道有哪些？
    - 网络爬取与正则表达式

:::

::: {.column .fragment width="50%"}

- 如何处理数据
    - 如何结构化文本数据？
    - 文本预处理的步骤有哪些
    - Tokenization的两种含义是什么？
- 如何分析数据
    - 词频能分析出什么？
    - 如何鉴别关键词？
    - 词的相似度？
    - 主题模型在干什么？
        - Bag of Words (BOW)?

:::

::::

## 提要

学完本课，你将了解:

:::{ style="text-align:center"}

- 如何捕捉“草蛇灰线”：*词汇层级*信息汇入
- 如何提升主题模型质量：*概念层级*信息汇入
- 词嵌入和LLM干了什么：*语义层级*信息汇入

[本讲的核心议题：[突破词基限制，纳入有用信息]{.red}]{.fragment .large}
:::

:::{.fragment .callout-warning .incremental}
- 学习本课内容，你不需要编程知识😱
- 应用本课内容，你需要一种编程知识😜
    - [如果你想学……](https://www.drhuyue.site/course/method-series/04-r-workshop/)
:::

# 问题源头

## 让计算机读懂人言的代价

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){fig-align="center" height=400}

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){.fragment fig-align="center" height=600}
:::


:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## 丢失了什么

:::{.fragment .large style="text-align:center; margin-top: 2em"}
- 词的重要性
- 语序/位置
- 虚词
- 语法
- Meta data
:::


# 词汇层级信息汇入

## “靶向”分析

- 关键性(Keyness)：识别在目标语料库中比在参照语料库中**统计上更频繁**出现的词语的度量方法 [@Gabrielatos2018]。

示例1：对比《卫报》新闻2016年与2012—2015年之间的新闻

```{r keyness, eval=FALSE}
dfmat_news <- tokens(corp_news, remove_punct = TRUE) |> 
  dfm()
 
tstat_key <- textstat_keyness(dfmat_news,
                              target = lubridate::year(dfmat_news$date) >= 2016)

saveRDS(tstat_key, file = here::here("slides", "courses", "governmentalBigData", "data", "text_keyness.rds"))
```

```{r keyness-out}
readRDS(url("https://drhuyue.site:10002/sammo3182/data/text_keyness.rds", open = "rb")) %>% textplot_keyness(n = 10)
```


## Keyness 示例2

在2012年到2016年的6,000篇《卫报》新闻文章中与欧盟（"EU", "europ*", "european union"）相关的词汇

```{r relevantKey,cache=TRUE}
#读入数据
corp_news <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/data_corpus_guardian.rds", open = "rb"))
#标记化
toks_news_guardian <- tokens(corp_news, remove_punct = TRUE)

eu <- c("EU", "europ*", "european union")

toks_inside <- tokens_keep(toks_news_guardian, pattern = eu, window = 10) |> 
  tokens_remove(pattern = eu) # remove the keywords
toks_outside <- tokens_remove(toks_news_guardian, pattern = eu, window = 10)

dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <-
  textstat_keyness(rbind(dfmat_inside, dfmat_outside),
                   target = seq_len(ndoc(dfmat_inside)))

textplot_keyness(tstat_key_inside, n = 10)
```

## 小结

:::{style="text-align:center; margin-top: 2em"}
- Weighing &larr; 从[词频]{.red}攫取信息
- N-gram &larr; 从[邻居]{.red}攫取信息
- Collocation &larr; 从[共现]{.red}攫取信息
- Keyness &larr; 从[关键词定位]{.red}攫取信息
- Functional words &larr; 从[社会心理]{.red}攫取信息
:::


# 概念层级信息汇入

## 主题模型能干什么

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
基于词频与共线的unsupervised降维，是一种frequency-based的降维
:::

## 主题模型缺什么

> “他的脸突然被魔杖的光照亮了。这是一张因痛苦、恐惧和愤怒而变得生动的脸。红色的眼睛向那个看不见的男孩站着的地方射去，他的隐形斗篷遮住了他。他的声音，当他发出声音时，就像一个冬天的夜晚一样冷。他说，“我回来了，比以前更强大了。”

:::{.notes}
哈利波特与火焰杯
:::

:::{.large .fragment style="text-align:center"}
- 主题之间的联系
- 篇章之间的联系
- 内容背景知识（外部信息）
:::

:::{.large .fragment style="text-align:center"}
&darr;    
STM/SeedLDA/keyATM
:::

## Keyword-Assisted Topic Models [keyATM, @EshimaEtAl2023]

:::{.r-stack}
- 针对概念测量而设计，而非探索主题
- 基于关键词（种子词） （类似于seedLDA）
- 允许没有关键词的主题 （不同于seedLDA）
- 对词频加权防止“词频主导”现象（类似于 weightedLDA）
- 允许文档向量、元信息的动态变化 （类似于STM）
- 贝叶斯方法
- 当种子词[质量高]{.red}时，性能优于加权LDA和STM

:::{.fragment}
```{r atm-input}

#数据
dfmat_inaug <- tokens(data_corpus_inaugural) |>
  dfm() |> 
  dfm_remove(stopwords("en"))

keyATM_docs <- keyATM_read(texts = dfmat_inaug)

# Keywords ####

keywords_inaug <- list(
  Government     = c("laws", "law", "executive"),
  Congress       = c("congress", "party"),
  Peace          = c("peace", "world", "freedom"),
  Constitution   = c("constitution", "rights"),
  ForeignAffairs = c("foreign", "war")
)

visualize_keywords(docs = keyATM_docs, keywords = keywords_inaug)

```
:::

:::


## Model Fit

```{r atm-fit, eval=FALSE}
# Fit ####

atm_inaug <- keyATM(
  docs = keyATM_docs,    # text input
  no_keyword_topics = 5,              # number of topics without keywords
  keywords = keywords_inaug,       # keywords
  model = "base",         # select the model
  options = list(seed = 313)
)

# Variable ####

vars_selected <- docvars(data_corpus_inaugural) |>
  dplyr::mutate(
    cent20 = ifelse(Year <= 1899, 0, 1),
    biparty = dplyr::case_when(
      Party == "Democratic" ~ "Democratic",
      Party == "Republican" ~ "Republican",
      TRUE ~ "Other"
    ) |> factor(levels = c("Other", "Democratic", "Republican"))
  )  |>
  dplyr::select(biparty, cent20)

# Model fit ####

atm_inaugX <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 5,
  keywords = keywords_inaug,
  model = "covariates",
  model_settings = list(
    covariates_data    = vars_selected,
    covariates_formula = ~ biparty + cent20
  ),
  options = list(seed = 313)
)

# Model covariate

vars_period <- docvars(data_corpus_inaugural) |>
  mutate(period = (Year - 1780) %/% 10 + 1)

max_time_index <- max(vars_period$period)

# Model dynamic

atm_inaugD <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 3,
  keywords = keywords_inaug,
  model = "dynamic",
  model_settings = list(
    time_index = vars_period$period,
    num_states = min(5, max_time_index)  # Ensure num_states doesn't exceed max_time_index
  ),
  options = list(seed = 313, 
                 store_theta = TRUE,
                 thinning = 5)
)

save(atm_inaug, atm_inaugX, atm_inaugD, file = here::here("slides", "courses", "governmentalBigData", "data", "text_atm.rda"))
```

```{r atm-fitness}
load(url("https://drhuyue.site:10002/sammo3182/data/text_atm.rda", open = "rb"))

# Model fit ####

plot_modelfit(atm_inaug)
```

## Base Model Result

```{r atm-result}
# Interpret ####

plot_topicprop(atm_inaug, show_topic = 1:10)
```

## Covariate Effect

```{r atmCov}
# Interpretation ####

## Binary
strata_period <- by_strata_DocTopic(atm_inaugX,
                                    by_var = "cent20",
                                    labels = c("18_19c", "20_21c"))

### Together view
plot(strata_period, var_name = "cent20", by = "covariate")
```

## Topic Dynamics

```{r atmDyn}
plot_timetrend(atm_inaugD, 
               time_index_label = docvars(data_corpus_inaugural)$Year, 
               xlab = "Year", 
               ci = 0.95)
```


## 小结

:::{style="text-align:center; margin-top: 1em"}
:::{.semi-fade-out}
- Keyness &larr; 从[关键词定位]{.red}攫取信息
:::

:::{.fragment}
- keyATM &larr; 纳入研究[意图]{.red}
:::

:::


# 语义层级信息汇入

:::{.notes}
- 语法：Grammar
- 语义: Semantic
- 语用: Pragmatic
:::

## 给词义建模：词嵌入(Word embedding)

> Words' meanings depend not just on immediate neighbors

![](https://drhuyue.site:10002/sammo3182/figure/theory_wordEmbedding.png){fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.

LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V × K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## 能做什么

- 纳入词语意义的表达 
    - 哪个最接近`paris - france + germany`（柏林）？
    - 哪个最接近`berlin - germany + uk + england`（伦敦）？

:::{.fragment}
- 与分类和聚类相比: Document-feature matrix &rarr; context-feature matrix (FCM)
    - 分类：不关注单个词语间的关系，而是关注词语在文档中的分布。
    - 聚类：提供对文档集合内容的更高级，而不是专注于单个词语的含义。
    
:::{.notes}
**context-feature matrix (FCM)** 是`quantada`中另一个重要的矩阵形式，它用来测量在用户定义的上下文语境中，一些特征的共现性，也就是利用上下文信息来捕捉词汇之间的语义关系。
具体来说，FCM 使用了一个上下文窗口，对每个词汇建模时考虑其周围的上下文。
这使得 FCM 能够更好地捕捉词汇之间的语义相似性，因为它不仅考虑了词汇本身的信息，还考虑了它们在语境中的使用情况。
:::
:::

:::{.fragment}
- 应用
    - 词条表征（term representation）
    - 情感分析（sentiment analysis）
:::


## 应用举例：Latent Semantic Scaling [@Watanabe2021]

- 专门用于区分[对立的立场]{.red}
- 基于词嵌入技术的，在构建模型之前将文档和特征转换为高维度向量空间
    - GloVe
    - Singular Value Decomposition (SVD)

![](https://drhuyue.site:10002/sammo3182/figure/text_svd.png){.fragment fig-align="center" height=400}

:::{.notes}
SVD is a math tool that helps us break down a big matrix (which you can think of like a giant table of numbers) into smaller parts. 
SVD, a matrix is decomposed into three other matrices: U, Σ, and V . The columns of U and V are called left and right singular vectors, respectively.

Comparison in Semantic Scaling:

- SVD: Better for general-purpose dimensionality reduction and identifying broad patterns in text. It's more of a tool for finding overall structure and relationships, but it may miss finer semantic distinctions.
- GloVe: Superior for scaling semantics in a more nuanced way, capturing both local (nearby words) and global (overall context) word meanings. This makes GloVe more effective for specific tasks like word similarity and analogy.
:::

## 应用：判断《卫报》新闻的情感走向

```{r lss, eval=FALSE}
# Preprocessing ####
# tokenize text corpus and remove various features
corp_sent <- corpus_reshape(corp_news, to =  "sentences")
toks_sent <- corp_sent |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE
  ) |>
  tokens_remove(stopwords("en", source = "marimo")) |>
  tokens_remove(c("*-time", "*-timeUpdated", "GMT", "BST", "*.com"))  



# create a document feature matrix from the tokens object
dfmat_sent <- toks_sent |> 
    dfm() |> 
    dfm_remove(pattern = "") |> 
    dfm_trim(min_termfreq = 5)

# Seed words ####

seed <- as.seedwords(data_dictionary_sentiment)


# Analyze ####

# identify context words
eco <- char_context(toks_sent, pattern = "econom*", p = 0.05)

# run LSS model (SVD)
tmod_lss_svd <- textmodel_lss(
  dfmat_sent,
  seeds = seed,
  terms = eco,
  k = 300,
  # the number of singular values requested to the SVD engine
  group_data = TRUE,
  # apply `dfm_group()` to `x` to group the sentences into the original documents, effectively reversing the segmentation by `corpus_reshape()`
  include_data = TRUE # save a grouped DFM in the LSS object as `lss$data`
)

fcm_sent <-fcm(dfmat_sent, tri = TRUE)

tmod_lss_glove <- textmodel_lss(
  fcm_sent,
  seeds = seed,
  terms = eco,
  engine = "rsparse" # using gloVe
)

save(tmod_lss_svd, tmod_lss_glove, dfmat_sent, file = here::here("slides", "courses", "governmentalBigData", "data", "text_embedding.rda"))

```

:::{.r-stack}
```{r lss-result-svd}
load(url("https://drhuyue.site:10002/sammo3182/data/text_embedding.rda", open = "rb"))

textplot_terms(tmod_lss_svd, highlighted = data_dictionary_LSD2015["negative"]) # many of the words (but not all of them) have negative meanings in the corpus.
```

:::{.fragment}
```{r lss-result-glove}
textplot_terms(tmod_lss_glove, highlighted = data_dictionary_LSD2015["negative"])
```
:::

:::

:::{.notes}
高亮的为词典中的词汇, 注意SVD结果，有一些词落在了0右面，而glove都落在了0左边
:::

## 趋势分析

```{r glovePrediction}
# Prediction ####

dat_svd <- docvars(tmod_lss_svd$data)
dat_svd$lss <- predict(tmod_lss_svd)

dfmat_doc <- dfm_group(dfmat_sent) # current glove version does not have the include_data argument
dat_glove <- docvars(dfmat_doc) 
dat_glove$lss <- predict(tmod_lss_glove, newdata = dfmat_doc)


# Draw the polarity score
smo_svd <- smooth_lss(dat_svd, lss_var = "lss", data_var = "date")

plot_svd <- ggplot(smo_svd, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (SVD)", x = "Year", y = "Sentiment")

smo_glove <- smooth_lss(dat_glove, lss_var = "lss", data_var = "date")

plot_glove <- ggplot(smo_glove, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (GloVe)", x = "Year", y = "Sentiment")
    
plot_svd / plot_glove
```

:::{.notes}
2016年以后波动比较大
:::

## 总结

> 本讲的核心议题：[突破词基限制，找回有用信息]{.red}

:::: {.columns}

::: {.column width="50%"}
**词汇层级的信息汇入**

:::{.fragment}
- Keyness &larr; 从[关键词定位]{.red}攫取信息
:::


**概念层级的信息汇入**

:::{.fragment}
- keyATM &larr; 纳入研究[意图]{.red}
:::

:::

::: {.column width="50%"}
**语义层级的信息汇入**

:::{.fragment}
- Word embedding &larr; 词汇之间的[语义]{.red}关联
:::

:::

::::



# 感谢倾听，欢迎交流 {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## 参考文献

::: {#refs}
:::
