---
title: "文本的数据分析进阶"
subtitle: "政务大数据应用与分析 (80700673)"
author: "胡悦"
institute: "清华大学社会科学学院" 
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  gridExtra,
  knitr, # dependency
  stringr, 
  tidytext, 
  tidyverse,
  lubridate,
  quanteda,
  quanteda.textstats,
  quanteda.textplots,
  quanteda.corpora,
  text2vec,
  LSX,
  seededlda,
  newsmap,
  keyATM,
  stm,
  tinytable,
  patchwork
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```



## 个人简介{.Small}

:::: {.columns}

::: {.column width="60%"}
*个人经历*

- 政治学博士[（University of Iowa)]{.small}
  - 信息学[（Graduated Certificate in Informatics)]{.small}
- 清华大学计算社会科学平台[(副主任)]{.small}
  - 清华数据与治理中心[(副主任)]{.small}
  - 计算社会科学编程语言证书项目[（负责人）]{.small}
  - Learning R with Dr. Hu & Friends 工作坊[（创始人）]{.small}

:::{.fragment}
*研究兴趣：认知、行为与现代性*

- **方法路径：计算政治学**
  - 实验室和调查实验
  - 潜变量分析、网络分析、空间分析
  - 文本大数据分析、数据可视化
:::

:::

::: {.column .fragment width="40%"}
*研究领域：比较政治、国家治理*

- **W. 心理学**
  - 政治[认知]{.red}治理
  - 行为公共[政策]{.red}
  - 政治传播

- **W. 经济学**
  - 经济不平等[感知]{.red}
  - 公共设施、服务均等化

- **W. 语言学**
  - 权力背书的[语言效果]{.red}与机制
  - 语言政策的治理功能

:::

::::

## 复习

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="50%"}
你应该已经知道……

- 你能用文本数据做什么
    - 你分析的是文字还是语言？
    - Close reading or distant reading？
    - 文本/音频/视频分析的理论基础是什么？
- 如何获取数据
    - 文本数据的获取渠道有哪些？
    - 网络爬取与正则表达式

:::

::: {.column .fragment width="50%"}

- 如何处理数据
    - 如何结构化文本数据？
    - 文本预处理的步骤有哪些
    - Tokenization的两种含义是什么？
- 如何分析数据
    - 词频能分析出什么？
    - 如何鉴别关键词？
    - 词的相似度？
    - 主题模型在干什么？
        - Bag of Words (BOW)?

:::

::::

## 提要

学完本课，你将了解:

:::{ style="text-align:center"}

- 如何捕捉“草蛇灰线”：*词汇层级*信息汇入
- 如何提升主题模型质量：*概念层级*信息汇入
- 词嵌入和LLM干了什么：*语义层级*信息汇入

[本讲的核心议题：[突破词基限制，纳入有用信息]{.red}]{.fragment .large}
:::

:::{.fragment .callout-warning .incremental}
- 学习本课内容，你不需要编程知识😱
- 应用本课内容，你需要一种编程知识😜
    - [如果你想学……](https://www.drhuyue.site/course/method-series/04-r-workshop/)
:::

# 问题源头

## 让计算机读懂人言的代价

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){fig-align="center" height=400}

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){.fragment fig-align="center" height=600}
:::


:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## 丢失了什么

:::{.fragment .large style="text-align:center; margin-top: 2em"}
- 词的重要性
- 语序/位置
- 虚词
- 语法
- Meta data
:::


# 词汇层级信息汇入

## 加权

DTM的问题：

1. 未将词的重要性纳入考量
2. 过度体现常见词
3. 轻视少见词

:::{.fragment}
举例：Term frequency-inverse document frequency (TF-IDF)

:::: {.columns}

::: {.column width="50%"}
$$\displaystyle \mathrm {tf} (t,d)={\frac {f_{t,d}}{\sum _{t'\in d}{f_{t',d}}}},$$ where $f_{t,d}$ is the number of times that term t occurs in document d. 
:::

::: {.column width="50%"}
$$\displaystyle \mathrm {idf} (t,D)=\log {\frac {N}{|\{d:d\in D{\text{ and }}t\in d\}|}},$$ 

- $N$: total number of documents; D.
- $|\{d\in D:t\in d\}|$ : number of documents where the term $t$ appears.

:::

::::

:::

:::{.notes}
TF： Importance of a term in a document
IDF： Frequency a term appear across documents
:::


## 加权带来什么

- 好处：
  1. 识别重要词汇
  2. 减少常见词汇的权重
  3. 提高机器学习性能 (why?)

:::{.fragment}
- 副作用：
  1. 假定词汇独立（与DTM相同）
  2. 对在语料库中非常罕见但在特定文档中出现过几次的罕见词汇给予高权重
  3. 对语料库的大小和多样性敏感
:::


## N-gram分析

- Markov Model of Order N
    + Unigram: 清华 大学 社会 科学 学院
    + Bigram: 清华大学 大学社会 社会科学 科学 学院
    + Trigram：清华大学社会 大学社会科学 社会科学学院

![](https://drhuyue.site:10002/sammo3182/figure/text_ngram.png){.fragment fig-align="center" height=400}

## 搭配分析

- 连续搭配（Contiguous collocations）：在文本中直接相邻出现。
- 揭示语言使用中的模式，这些模式从查看单个单词时并不立即明显。

示例数据：2012年到2016年的6,000篇《卫报》新闻文章

```{r guardianData, cache=TRUE}
# 2016年卫报新闻语料库（Corpus of Guardian news in 2016 from those from 2012--2015）

corp_news <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/data_corpus_guardian.rds", open = "rb"))

toks_news_guardian <- tokens(corp_news, remove_punct = TRUE)

corp_news
```

## Collocation 示例

- 最常见的词对（pairs of words）
- 最常见的三词模式（three-word patterns）

```{r collocation, message=FALSE}
toks_select <- tokens_select(
  toks_news_guardian,
  pattern = "^[A-Z]",
  valuetype = "regex",
  case_insensitive = FALSE,
  padding = TRUE
) 

tstat_col_caps <- textstat_collocations(toks_select, min_count = 100)
head(tstat_col_caps, 10)

tstat_col_caps3 <- textstat_collocations(toks_select, min_count = 80, size = 3)
head(tstat_col_caps3, 10)
```

## “靶向”分析

- 关键性(Keyness)：识别在目标语料库中比在参照语料库中**统计上更频繁**出现的词语的度量方法 [@Gabrielatos2018]。

示例1：对比《卫报》新闻2016年与2012—2015年之间的新闻

```{r keyness, eval=FALSE}
dfmat_news <- tokens(corp_news, remove_punct = TRUE) |> 
  dfm()
 
tstat_key <- textstat_keyness(dfmat_news,
                              target = lubridate::year(dfmat_news$date) >= 2016)

saveRDS(tstat_key, file = here::here("slides", "courses", "governmentalBigData", "data", "text_keyness.rds"))
```

```{r keyness-out}
readRDS(url("https://drhuyue.site:10002/sammo3182/data/text_keyness.rds", open = "rb")) %>% textplot_keyness(n = 10)
```


## Keyness 示例2

在2012年到2016年的6,000篇《卫报》新闻文章中与欧盟（"EU", "europ*", "european union"）相关的词汇

```{r relevantKey}
eu <- c("EU", "europ*", "european union")

toks_inside <- tokens_keep(toks_news_guardian, pattern = eu, window = 10) |> 
  tokens_remove(pattern = eu) # remove the keywords
toks_outside <- tokens_remove(toks_news_guardian, pattern = eu, window = 10)

dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <-
  textstat_keyness(rbind(dfmat_inside, dfmat_outside),
                   target = seq_len(ndoc(dfmat_inside)))

textplot_keyness(tstat_key_inside, n = 10)
```


## “入木三分”分析 [@Liu2022]

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_pronoun.png){fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/css_liwcTree.png){.fragment fig-align="center" height=600}
:::


## 小结

:::{style="text-align:center; margin-top: 2em"}
- Weighing &larr; 从[词频]{.red}攫取信息
- N-gram &larr; 从[邻居]{.red}攫取信息
- Collocation &larr; 从[共现]{.red}攫取信息
- Keyness &larr; 从[关键词定位]{.red}攫取信息
- Functional words &larr; 从[社会心理]{.red}攫取信息
:::


# 概念层级信息汇入

## 主题模型能干什么

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
基于词频与共线的unsupervised降维，是一种frequency-based的降维
:::

## 主题模型缺什么

> “他的脸突然被魔杖的光照亮了。这是一张因痛苦、恐惧和愤怒而变得生动的脸。红色的眼睛向那个看不见的男孩站着的地方射去，他的隐形斗篷遮住了他。他的声音，当他发出声音时，就像一个冬天的夜晚一样冷。他说，“我回来了，比以前更强大了。”

:::{.notes}
哈利波特与火焰杯
:::

:::{.large .fragment style="text-align:center"}
- 主题之间的联系
- 篇章之间的联系
- 内容背景知识（外部信息）
:::

:::{.large .fragment style="text-align:center"}
&darr;    
STM/SeedLDA/keyATM
:::

## Correlated Topic Model (CTM)

主题之间彼此关联被纳入考量

:::{.r-vstack}
![LDA](https://drhuyue.site:10002/sammo3182/figure/cluster_ldaDiagram.png){fig-align="center" height=150}

![CTM](https://drhuyue.site:10002/sammo3182/figure/cluster_ctmDiagram.png){fig-align="center" height=150}
:::

 $$\{\mu,\Sigma\}\sim N(\mu,\Sigma).$$

:::{.notes}
CTM模型（correlated topic model）的进步之处在于哪里呢？

最初人们做LDA模型的时候，人们认为不同的主题之间是不存在什么联系的，但这是不可能的。
想象一下，我们可以从《人民日报》提炼出关于经济发展、民生工程等主题，但是我们绝对不可能从中抽取出关于妇科广告的主题，因为这是这份报纸的性质决定的。
所以我们从语料中抽取出来的主题，彼此之间肯定是具有高度的相关性的。

CTM的名字中之所以有一个correlated，就是因为这个模型可以把所有的主题放在一起，放在一个结构（structure）里面去理解每一个主题。
因此CTM是一种层次化主题模型，它明确抓取了主题间的潜在相关性。

相比于LDA，CTM模型多了两个参数μ & Σ：一个K维的均值和协方差矩阵 $\{\mu,\Sigma\}\sim N(\mu,\Sigma)$。
:::

## Sparse Additive Generative Model (SAGE)

每个主题都被赋予一个模型，能够描述给予恒定背景分布对数频率的偏差。

![SAGE](https://drhuyue.site:10002/sammo3182/figure/cluster_sageDiagram.png){fig-align="center" height=400}


## Structure Topic Model [STM, @RobertsEtAl2013]

CTM + SAGE

![STM](https://drhuyue.site:10002/sammo3182/figure/cluster_stmDiagram.png){fig-align="center" height=500}


## 操作

示例：美国总统就职演说数据

1. 设定主题数目

```{r stm-searchKresult, cache=TRUE}
#对于上述每一个结果，计算连贯性和排他性
stm_searchK <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/stm_searchK.rds", open = "rb"))

fit_searchK <- map2(stm_searchK, names(stm_searchK), \(result, gName){
  tibble(group = gName,
         exclusivity = exclusivity(result),
         coherence = semanticCoherence(result, dfmat_inaug))
}) |> 
  list_rbind()

#求出每组的平均值
fit_searchK_agg <- group_by(fit_searchK, group) |> 
  summarise(coherence = mean(coherence),
            exclusivity = mean(exclusivity))

#作图
ggplot(fit_searchK_agg, aes(coherence, exclusivity, color = group, size = 3)) +
  geom_point() +
  scale_color_gb(palette = "full")

#选择主题数为8的stm模型，将其保存在stm_selected中
stm_selected <- stm_searchK$n8
```

## 2. 主题归类

$$Topic \sim Party + s(Year).$$

```{r topicDist, exercise = TRUE, exercise.setup = "stm-searchKresult"}
#查看词语在主题中的分布
labelTopics(stm_selected, topics = c(1:3), n = 5)
```

- Highest Prob：高频词
- FREX：主题高频词
- lift：通过词语在其他主题中的频率相除来加权词语
- score：将词语在主题中的对数频率除以词语在其他主题中的对数频率

:::{.notes}
这行代码为我们返回了每个主题下面的一些词语。

- “Highest Prob”指的是这个*语料库*中频率最高的词语，我们可以看到出现了逗号和句号，还有“america”等。

- FREX矩阵（FREX matrix）中的词语依然是高频词，但是它是仅在这个主题中的高频词，也就是能够区分这一主题和其他主题之间特殊性的高频词。
它的算法是通过词语的整体频率及其对主题的专有程度来加权词语。

- 提升（lift）：与FREX相同，但是算法不一样，它通过词语在其他主题中的频率相除来加权词语，因此给在其他主题中较少出现的词语赋予更高的权重。

- 赋分（score）：与FREX相同，但是算法是将词语在主题中的对数频率除以词语在其他主题中的对数频率。

所以我们通常建议大家，当你去对于每个主题的含义进行解释的时候，可以基于FREX，lift和score来解读，而不是只看高频词去解释它的含义，也就是说我们讲的这三个测量这是帮助我们去解读主题用的。
:::

## 3. 主题相关性

以余弦相似度来计算相关性，越接近于1表示两个主题越相关，越接近于0表示两个主题越不相关。

```{r topicCorrelation}
#计算STM模型中所有主题之间的相关性
corr_stm8 <- topicCorr(stm_selected)

library(GGally)
library(network)

#计算 STM 模型中主题之间的相关性矩阵，并取其绝对值
net_stm8 <- corr_stm8$cor |> abs() |> as.matrix()
#将相关性矩阵中的值乘以10，以增加差异的可视化效果。
net_stm8 <- net_stm8 * 10 
#将矩阵的对角线元素设置为1，因为每个主题与自身的相关性总是1。
diag(net_stm8) <- 1

#创建一个网络对象，表示主题之间的相关性
graph_stm8 <- network(net_stm8,
    matrix.type = "adjacency",
    ignore.eval = FALSE,
    names.eval = "weights",
    directed = FALSE
  )

#绘制网络图  
ggnet2(graph_stm8, label = TRUE, edge.size = "weights")
```


## 协变量的影响：党派

```{r covariateParty}
#| fig-height: 6.5

# 总统就职数据语料库（Corpus of presidential inaugural data）

dfmat_inaug <- tokens(data_corpus_inaugural,
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE,
                      remove_separators = TRUE) |>
  dfm() |> 
  dfm_remove(stopwords("en")) %>% 
  dfm_remove()

#模型建构
result_stm8 <- estimateEffect(formula = 1:8 ~ Party + s(Year), 
              #要分析的stm模型
               stmobj = stm_selected,
              #元数据来源
               metadata = docvars(dfmat_inaug))


#绘制党派（Party）对主题的影响的森林图
plot(result_stm8, 
     covariate = "Party",
     model = stm_selected,
     method = "difference",
     cov.value1 = "Democratic",
     cov.value2 = "Republican",
     xlim = c(-1, 1), 
     xlab = "Democratic <-> Republican")

```

## 协变量的影响：时间

```{r covariateTime}
#绘制主题3和主题7在年份上的连续效应图
plot(
  result_stm8,
  "Year",
  method = "continuous",
  model = stm_selected,
  topics = c(3, 7)
)
```


## Seed LDA

以种子词（seeds）引领主题分类。

示例数据：《卫报》2016

种子词：经济、政治、社会、外交和军事，基于对该类新闻的认知获取

```{r seeds}
# 预处理 ###
## 读取数据
corp_news_2016 <- corpus_subset(corp_news, year(date) == 2016)

#标记化与清洗
toks_news_guardian <-
  tokens(
    corp_news_2016,
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbol = TRUE
  ) |>
  tokens_remove(pattern = c(stopwords("en"), "*-time", "updated-*", "gmt", "bst"))

#矩阵化与清洗
dfmat_news_guardian <- dfm(toks_news_guardian) |>
  dfm_trim(
    min_termfreq = 0.8,
    termfreq_type = "quantile",
    max_docfreq = 0.1,
    docfreq_type = "prop"
  )

## 读取词典数据
dict_topic <- dictionary(file = system.file("extdata", "topics.yml", package = "drhurText"))
dict_topic
```

## 效果比较：LDA

```{r seedLda, eval=FALSE}
# 分析 ####

#使用lda模型
tmod_lda <- textmodel_lda(dfmat_news_guardian, k = 5)
#使用seeded lda模型
tmod_slda <- textmodel_seededlda(dfmat_news_guardian, dictionary = dict_topic)

save(tmod_lda, tmod_slda, file = here::here("slides", "courses", "governmentalBigData", "data", "text_seedLDA.rda"))
```

```{r lda-out}
load(url("https://drhuyue.site:10002/sammo3182/data/text_seedLDA.rda", open = "rb"))

#添加一个新的文档级别的变量以存储seededLDA 模型为每篇文章分配的主题。
dfmat_news_guardian$topic_seeded <- topics(tmod_slda) 

# 比较

seededlda::terms(tmod_lda, 8) %>% 
  as.data.frame() %>% 
  tt()
```

## 效果比较：Seed LDA

```{r seededlda-out}
seededlda::terms(tmod_slda, 8) %>% 
  as.data.frame() |> 
  tt()
```


:::{.notes}
而比较LDA与seeded LDA做出的主题分类结果，我们很容易发现，在LDA分成的主题3下面，不仅有“climate”和"water"，还有"apple"和"education"，它分类的效果很明显是不太好的。
但是在seeded LDA中，每个主题下面的词语大体上符合我们的认知。
:::


## Keyword-Assisted Topic Models [keyATM, @EshimaEtAl2023]

:::{.r-stack}
- 针对概念测量而设计，而非探索主题
- 基于关键词（种子词） （类似于seedLDA）
- 允许没有关键词的主题 （不同于seedLDA）
- 对词频加权防止“词频主导”现象（类似于 weightedLDA）
- 允许文档向量、元信息的动态变化 （类似于STM）
- 贝叶斯方法
- 当种子词[质量高]{.red}时，性能优于加权LDA和STM

:::{.fragment}
```{r atm-input}
keyATM_docs <- keyATM_read(texts = dfmat_inaug)

# Keywords ####

keywords_inaug <- list(
  Government     = c("laws", "law", "executive"),
  Congress       = c("congress", "party"),
  Peace          = c("peace", "world", "freedom"),
  Constitution   = c("constitution", "rights"),
  ForeignAffairs = c("foreign", "war")
)

visualize_keywords(docs = keyATM_docs, keywords = keywords_inaug)

```
:::

:::


## Model Fit

```{r atm-fit, eval=FALSE}
# Fit ####

atm_inaug <- keyATM(
  docs = keyATM_docs,    # text input
  no_keyword_topics = 5,              # number of topics without keywords
  keywords = keywords_inaug,       # keywords
  model = "base",         # select the model
  options = list(seed = 313)
)

# Variable ####

vars_selected <- docvars(data_corpus_inaugural) |>
  dplyr::mutate(
    cent20 = ifelse(Year <= 1899, 0, 1),
    biparty = dplyr::case_when(
      Party == "Democratic" ~ "Democratic",
      Party == "Republican" ~ "Republican",
      TRUE ~ "Other"
    ) |> factor(levels = c("Other", "Democratic", "Republican"))
  )  |>
  dplyr::select(biparty, cent20)

# Model fit ####

atm_inaugX <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 5,
  keywords = keywords_inaug,
  model = "covariates",
  model_settings = list(
    covariates_data    = vars_selected,
    covariates_formula = ~ biparty + cent20
  ),
  options = list(seed = 313)
)

# Model covariate

vars_period <- docvars(data_corpus_inaugural) |>
  mutate(period = (Year - 1780) %/% 10 + 1)

max_time_index <- max(vars_period$period)

# Model dynamic

atm_inaugD <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 3,
  keywords = keywords_inaug,
  model = "dynamic",
  model_settings = list(
    time_index = vars_period$period,
    num_states = min(5, max_time_index)  # Ensure num_states doesn't exceed max_time_index
  ),
  options = list(seed = 313, 
                 store_theta = TRUE,
                 thinning = 5)
)

save(atm_inaug, atm_inaugX, atm_inaugD, file = here::here("slides", "courses", "governmentalBigData", "data", "text_atm.rda"))
```

```{r atm-fitness}
load(url("https://drhuyue.site:10002/sammo3182/data/text_atm.rda", open = "rb"))

# Model fit ####

plot_modelfit(atm_inaug)
```

## Base Model Result

```{r atm-result}
# Interpret ####

plot_topicprop(atm_inaug, show_topic = 1:10)
```

## Covariate Effect

```{r atmCov}
# Interpretation ####

## Binary
strata_period <- by_strata_DocTopic(atm_inaugX,
                                    by_var = "cent20",
                                    labels = c("18_19c", "20_21c"))

### Together view
plot(strata_period, var_name = "cent20", by = "covariate")
```

## Topic Dynamics

```{r atmDyn}
plot_timetrend(atm_inaugD, 
               time_index_label = docvars(data_corpus_inaugural)$Year, 
               xlab = "Year", 
               ci = 0.95)
```


## 小结

:::{style="text-align:center; margin-top: 1em"}
:::{.fragment .semi-fade-out}
- Weighing &larr; 从[词频]{.red}攫取信息
- N-gram &larr; 从[邻居]{.red}攫取信息
- Collocation &larr; 从[共现]{.red}攫取信息
- Keyness &larr; 从[关键词定位]{.red}攫取信息
- Functional words &larr; 从[社会心理]{.red}攫取信息
:::


:::{.fragment}
- STM &larr; 纳入概念[关系]{.red}与[外部]{.red}信息
- SeedLDA &larr; 纳入[背景]{.red}知识
- keyATM &larr; 纳入研究[意图]{.red}
:::

:::


# 语义层级信息汇入

:::{.notes}
- 语法：Grammar
- 语义: Semantic
- 语用: Pragmatic
:::

## 给词义建模：词嵌入(Word embedding)

> Words' meanings depend not just on immediate neighbors

![](https://drhuyue.site:10002/sammo3182/figure/theory_wordEmbedding.png){fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.

LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V × K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## 能做什么

- 纳入词语意义的表达 
    - 哪个最接近`paris - france + germany`（柏林）？
    - 哪个最接近`berlin - germany + uk + england`（伦敦）？

:::{.fragment}
- 与分类和聚类相比: Document-feature matrix &rarr; context-feature matrix (FCM)
    - 分类：不关注单个词语间的关系，而是关注词语在文档中的分布。
    - 聚类：提供对文档集合内容的更高级，而不是专注于单个词语的含义。
    
:::{.notes}
**context-feature matrix (FCM)** 是`quantada`中另一个重要的矩阵形式，它用来测量在用户定义的上下文语境中，一些特征的共现性，也就是利用上下文信息来捕捉词汇之间的语义关系。
具体来说，FCM 使用了一个上下文窗口，对每个词汇建模时考虑其周围的上下文。
这使得 FCM 能够更好地捕捉词汇之间的语义相似性，因为它不仅考虑了词汇本身的信息，还考虑了它们在语境中的使用情况。
:::
:::

:::{.fragment}
- 应用
    - 词条表征（term representation）
    - 情感分析（sentiment analysis）
:::


## 应用举例：Latent Semantic Scaling [@Watanabe2021]

- 专门用于区分[对立的立场]{.red}
- 基于词嵌入技术的，在构建模型之前将文档和特征转换为高维度向量空间
    - GloVe
    - Singular Value Decomposition (SVD)

![](https://drhuyue.site:10002/sammo3182/figure/text_svd.png){.fragment fig-align="center" height=400}

:::{.notes}
SVD is a math tool that helps us break down a big matrix (which you can think of like a giant table of numbers) into smaller parts. 
SVD, a matrix is decomposed into three other matrices: U, Σ, and V . The columns of U and V are called left and right singular vectors, respectively.

Comparison in Semantic Scaling:

- SVD: Better for general-purpose dimensionality reduction and identifying broad patterns in text. It's more of a tool for finding overall structure and relationships, but it may miss finer semantic distinctions.
- GloVe: Superior for scaling semantics in a more nuanced way, capturing both local (nearby words) and global (overall context) word meanings. This makes GloVe more effective for specific tasks like word similarity and analogy.
:::

## 应用：判断《卫报》新闻的情感走向

```{r lss, eval=FALSE}
# Preprocessing ####
# tokenize text corpus and remove various features
corp_sent <- corpus_reshape(corp_news, to =  "sentences")
toks_sent <- corp_sent |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE
  ) |>
  tokens_remove(stopwords("en", source = "marimo")) |>
  tokens_remove(c("*-time", "*-timeUpdated", "GMT", "BST", "*.com"))  



# create a document feature matrix from the tokens object
dfmat_sent <- toks_sent |> 
    dfm() |> 
    dfm_remove(pattern = "") |> 
    dfm_trim(min_termfreq = 5)

# Seed words ####

seed <- as.seedwords(data_dictionary_sentiment)


# Analyze ####

# identify context words
eco <- char_context(toks_sent, pattern = "econom*", p = 0.05)

# run LSS model (SVD)
tmod_lss_svd <- textmodel_lss(
  dfmat_sent,
  seeds = seed,
  terms = eco,
  k = 300,
  # the number of singular values requested to the SVD engine
  group_data = TRUE,
  # apply `dfm_group()` to `x` to group the sentences into the original documents, effectively reversing the segmentation by `corpus_reshape()`
  include_data = TRUE # save a grouped DFM in the LSS object as `lss$data`
)

fcm_sent <-fcm(dfmat_sent, tri = TRUE)

tmod_lss_glove <- textmodel_lss(
  fcm_sent,
  seeds = seed,
  terms = eco,
  engine = "rsparse" # using gloVe
)

save(tmod_lss_svd, tmod_lss_glove, dfmat_sent, file = here::here("slides", "courses", "governmentalBigData", "data", "text_embedding.rda"))

```

:::{.r-stack}
```{r lss-result-svd}
load(url("https://drhuyue.site:10002/sammo3182/data/text_embedding.rda", open = "rb"))

textplot_terms(tmod_lss_svd, highlighted = data_dictionary_LSD2015["negative"]) # many of the words (but not all of them) have negative meanings in the corpus.
```

:::{.fragment}
```{r lss-result-glove}
textplot_terms(tmod_lss_glove, highlighted = data_dictionary_LSD2015["negative"])
```
:::

:::

:::{.notes}
高亮的为词典中的词汇, 注意SVD结果，有一些词落在了0右面，而glove都落在了0左边
:::

## 趋势分析

```{r glovePrediction}
# Prediction ####

dat_svd <- docvars(tmod_lss_svd$data)
dat_svd$lss <- predict(tmod_lss_svd)

dfmat_doc <- dfm_group(dfmat_sent) # current glove version does not have the include_data argument
dat_glove <- docvars(dfmat_doc) 
dat_glove$lss <- predict(tmod_lss_glove, newdata = dfmat_doc)


# Draw the polarity score
smo_svd <- smooth_lss(dat_svd, lss_var = "lss", data_var = "date")

plot_svd <- ggplot(smo_svd, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (SVD)", x = "Year", y = "Sentiment")

smo_glove <- smooth_lss(dat_glove, lss_var = "lss", data_var = "date")

plot_glove <- ggplot(smo_glove, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (GloVe)", x = "Year", y = "Sentiment")
    
plot_svd / plot_glove
```

:::{.notes}
2016年以后波动比较大
:::


## 还缺什么

> Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy [@IBM2021].

![](https://drhuyue.site:10002/sammo3182/figure/text_machineLearning.jpg){fig-align="center" height=400}

:::{.notes}
- 自动性：self-supervised learning

- 加强: reinforcement
:::

## 自监督学习

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_selfSupervised.png){fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/text_pretrain.webp){.fragment fig-align="center" height=600}
:::


:::{.notes}
Self-Supervised Learning: In self-supervised learning, the model generates its own labels from the input data, creating a supervised-like setup. For instance, a model might be given part of a sentence and asked to predict the missing word. The labels (the missing word) come from the data itself. This is often used in tasks like language modeling (e.g., predicting the next word in a sentence).
:::

## 强化学习

![](https://drhuyue.site:10002/sammo3182/figure/text_reinforcement.png){fig-align="center" height=600}


## 注意力机制 [Attention Mechanism, @VaswaniEtAl2017]

一般的word embedding认为所有词和词之间关系都同等重要[🤦‍♂️]{.large}

:::{.fragment .fade-in-then-semi-out}
"Attention is all you need" [@VaswaniEtAl2017]

以下是关于大型语言模型中注意力机制的示例翻译：

> 作为在_____领域的头部企业，我们雇佣了大量高水平的软件工程师。    
> 作为在_____领域的头部企业，我们雇佣了大量高水平的太阳能工程师。

应该在 "_____" 填入什么词？你是如何得出这个结论的？

:::


:::{.fragment}
作为在*信息技术*领域的头部企业，我们雇佣了大量高水平的[软件]{.red}工程师。    
作为在*绿色能源*领域的头部企业，我们雇佣了大量高水平的[太阳能]{.red}工程师。

:::

:::{.notes}
Self-Attention in Transformers:
Self-Attention: A specific type of attention called self-attention is key to Transformers. In self-attention, every word in a sentence can pay attention to every other word, not just the nearby words. This allows the model to capture complex relationships, even when words are far apart in the sentence.

For instance, in the sentence "The cat that was sitting on the mat is happy," self-attention allows the model to recognize that "cat" is related to "is happy," even though several words separate them.

Multi-Headed Attention: Transformers use multiple attention heads to learn different aspects of relationships between words. Each attention head focuses on a different aspect of the word relationships, which helps the model understand the text in a richer and more nuanced way.

3. During Pre-Training and Fine-Tuning:
Pre-Training Phase (Self-Supervised Learning): The attention mechanism helps the model learn language representations by focusing on relevant parts of the input when predicting the next word or filling in gaps in sentences.

Fine-Tuning Phase (Reinforcement Learning and Beyond): The attention mechanism continues to play a role when the model is fine-tuned using reinforcement learning, such as RLHF. The model uses the attention mechanism to generate responses during interactions, determining which parts of the input prompt to focus on when creating its output.
:::


:::{.fragment .callout-note}
## 自注意力机制

输入一个初始词元嵌入序列，并输出一个新的词元嵌入序列，[使初始嵌入能够相互作用]{.red}。
:::

## 组装起来

- 由堆叠的*注意力机制*和*前馈神经网络层*组成的大型神经网络可以使用专用处理器高效并行训练，即 **t**ransformer，通常通过预训练（**p**re-training)获得。
    - &rarr; 应用于基于学习成果的生成式（**g**enerative）任务


:::{.notes}
- BERT 使用transformer（encoder）来进行discriminative jobs，e.g, classification or predicting missing words in sentences
- GPT 使用transformer (decoder)来进行generative jobs, e.g., generating sentences
:::

:::{.fragment}
- 强化学习/微调过程

![](https://drhuyue.site:10002/sammo3182/figure/text_finetune.webp){fig-align="center" height=400}
:::


:::{.notes}
Fine-Tuning: After pretraining, the model undergoes fine-tuning on more specific data, often with human feedback or additional tasks. This process helps the model adjust its knowledge to be more useful for specific applications. For instance, fine-tuning might include adjusting the model's responses to be more helpful, accurate, or aligned with certain safety guidelines. This phase often involves techniques like Reinforcement Learning from Human Feedback (RLHF), where the model learns from human-provided rankings of response quality.

Interaction with ChatGPT: not fine tune

Interaction with ChatGPT (Ask-Answer Rounds):
When you engage in ask-answer rounds with ChatGPT, the model uses the knowledge it gained during training and fine-tuning to respond to your queries. However:

- No Learning Happens: The model is not updating its parameters or "learning" from your individual interactions. The model's behavior is static, and its responses are generated based on the preexisting knowledge and patterns it has learned.
- Inference: The process you’re engaging in is called inference, where the model is simply generating responses based on what it already knows.


The reason you often get better answers after interacting with ChatGPT is not because the model is learning from your specific questions in real-time (since it doesn't update itself during these interactions). Instead, it's due to the way the model processes context and maintains a conversation. Here's why:

1. **Context Awareness**:
ChatGPT can remember the context of the ongoing conversation within the session. As the interaction progresses, the model uses previous information from the conversation to better understand your current question. This allows it to generate more accurate, relevant, and coherent responses.

For example:
- If you ask a series of related questions, ChatGPT can "build on" the information exchanged earlier in the conversation, leading to more refined answers.
- It can keep track of clarifications you’ve provided and adapt its responses to align with your preferences or focus on specific details you’re interested in.

2. **Clarification and Refinement**:
In a conversation, you often provide more details, clarifications, or corrections as you interact with the model. This helps ChatGPT refine its understanding of your needs, enabling it to give more relevant answers.

For instance:
- If an initial response is too vague or off-target, you might ask follow-up questions or provide additional information. With this new input, ChatGPT can refine its responses to better match what you're looking for.

3. **Pattern Matching**:
ChatGPT has been trained on a vast amount of conversational data, so it’s very good at recognizing patterns in dialogue. As the conversation continues, the model can "hone in" on the patterns that seem most relevant to your questions based on prior exchanges. This can give the appearance of improvement in the quality of responses.

4. **Elaboration and Deeper Explanation**:
With more interaction, you might be guiding ChatGPT to provide deeper or more nuanced explanations. The initial responses might be general, but as you ask for further detail or clarification, the model responds with more in-depth information, which can feel like an improvement.

5. **Conversational Flow**:
ChatGPT is designed to simulate a conversational flow, so as you continue to interact, the model attempts to align more closely with your communication style, the specific topic, and your expressed needs. This ongoing refinement can feel like it's "learning," but it’s really just adapting to the conversation in progress.

Conclusion:
The improvement in the quality of answers you receive during the conversation is due to **contextual understanding** and **clarification through interaction**, rather than real-time learning or fine-tuning. The model is designed to keep track of the dialogue and adjust its responses based on the context of your ongoing discussion.
:::


## 总结

> 本讲的核心议题：[突破词基限制，找回有用信息]{.red}

:::: {.columns}

::: {.column width="50%"}
**词汇层级的信息汇入**

:::{.fragment}
- Weighing &larr; 从[词频]{.red}攫取信息
- N-gram & Collocation &larr; 从[邻居/共现]{.red}攫取信息
- Functional words &larr; 从[心理]{.red}攫取信息
:::


**概念层级的信息汇入**

:::{.fragment}
- STM &larr; 纳入概念[关系]{.red}与[外部]{.red}信息
- SeedLDA &larr; 纳入[背景]{.red}知识
- keyATM &larr; 纳入研究[意图]{.red}
:::

:::

::: {.column width="50%"}
**语义层级的信息汇入**

:::{.fragment}
- Word embedding &larr; 词汇之间的[语义]{.red}关联
- Self-supervised learning & Attention model &larr; [自动]{.red}化和自我[提升]{.red}
- Reinforcement learning &larr; 具体任务[情境]{.red}
:::

:::

::::



# 感谢倾听，欢迎交流 {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## 参考文献

::: {#refs}
:::
