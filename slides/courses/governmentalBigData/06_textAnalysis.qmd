---
title: "文本的数据分析原则"
subtitle: "政务大数据应用与分析 (80700673)"
author: "胡悦"
institute: "清华大学社会科学学院" 
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  flextable, 
  gridExtra,
  knitr, # dependency
  rvest, 
  stringr, 
  broom, jiebaR, 
  tidytext, 
  tidyverse
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

## 个人简介{.Small}

:::: {.columns}

::: {.column width="60%"}
*个人经历*

- 政治学博士[（University of Iowa)]{.small}
  - 信息学[（Graduated Certificate in Informatics)]{.small}
- 清华大学计算社会科学平台[(副主任)]{.small}
  - 清华数据与治理中心[(副主任)]{.small}
  - 计算社会科学编程语言证书项目[（负责人）]{.small}
  - Learning R with Dr. Hu & Friends 工作坊[（创始人）]{.small}

:::{.fragment}
*研究兴趣：认知、行为与现代性*

- **方法路径：计算政治学**
  - 实验室和调查实验
  - 潜变量分析、网络分析、空间分析
  - 文本大数据分析、数据可视化
:::

:::

::: {.column .fragment width="40%"}
*研究领域：比较政治、国家治理*

- **W. 心理学**
  - 政治[认知]{.red}治理
  - 行为公共[政策]{.red}
  - 政治传播

- **W. 经济学**
  - 经济不平等[感知]{.red}
  - 公共设施、服务均等化

- **W. 语言学**
  - 权力背书的[语言效果]{.red}与机制
  - 语言政策的治理功能

:::

::::

## 提要

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="30%"}
### 认知

[见字如数]{.red}：    
文本与文本数据

- 文本&rarr;数据
- 文本分析
- 应用案例
:::

::: {.column .fragment width="30%"}
### 原则

[以数知文]{.red}:     
文本数据化原理规范

* 数据获取
* 数据整理
* 基本分析

:::

::: {.column .fragment width="40%"}
### 操作

[望数生义]{.red}:     
文本数据分析过程示范（R）

- 网络信息爬取（[`rvest`](https://rvest.tidyverse.org/)）
- 中文处理（[`jiebaR`](https://qinwenfeng.com/jiebaR/)/[`TopWORDS`](http://www.stat.tsinghua.edu.cn/kdeng/r-package/)）
- 文本关系分析（[`tidytext`](https://www.tidytextmining.com/)）

:::

::::



# 见字如数：建立认知

## 文本数据

[每年： 2024年初社交媒体用户50.4亿，2023年新增2.66亿首次使用用户 [@Kemp2024]]{.fragment}

:::{.fragment}
每日：

+ 百度日用户搜索请求，需[1.7天]{.red}才能扫描一遍；
+ 微信日增数据[500TB]{.red}——比人类所有书籍存量还多。
:::

[每秒：全世界每秒发送290万封email，一人需要[5.5年]{.red}日以继夜才能读完。]{.fragment}

:::{.fragment .callout-tip}
2023年，全国数据生产总量达32.85ZB,，同比增长22.44%，即人均 `r round(32.85 * (1024^3) / (1.6 * 10^9))` TB [@YanZhiHongYanFuJing2024]^[1ZB = 1,024EB = 1,024<sup>2</sup>PB = 1,024<sup>3</sup>TB  = 1024<sup>4</sup>GB; 1080p HD 2hrs：6GB.]


预计2027年，全球[非结构化数据]{.red}将占到数据总量的86.8% [@IDCFutureScape2024]
:::


:::{.notes}
Giga Byte < Tera Byte < Peta Byte < Exa Byte < Zetta Byte < Yotta Byte
:::


## 文本研究

历史悠久而非主流 &larr; 资料难获取；花时间；难推广；难管理；难分析

- 法律的外部性问题 [@Coase1960]
- 通过历史文件研究“policy surprises” [@FriedmanSchwartz2008]

:::{.fragment style="text-align:center; margin-top: 2em"}
新兴工具的繁荣：

+ 文本[资料]{.red}指数级增长；
+ 大规模文本数据[采集]{.red}；
+ 存储和管理[能力]{.red}增强；
+ 可推广、系统化和[廉价]{.red}化；
+ 文本分析[方法]{.red}蓬勃发展
:::



## (计算机辅助）文本分析

:::: {.columns}

::: {.column width="20%"}
**对象**

~~文字~~ 语言
:::

::: {.column .fragment width="80%"}
**类型**

* Text analysis vs. content analysis
* Representational analysis vs. Instrumental analysis
* Thematic analysis vs. semantic analysis

:::{.notes}
Representational 精确解码，关注外显（manifest）；
Instrumental 探索意图，关注隐含（latent）
Thematic 概念是否出现及何种关系，基于词频和向量；
Semantic 识别主题间的具体关系；考虑语法、逻辑等

语法学(syntax, how to say it)
语义学(semantic, what is said)
语用学(pragmatic, what is implicated)
:::

:::

::::


## 🌰 I

@Grimmer2010

- Objective
    - The priorities political actors emphasize in statements
- Data
    - An original collection of over 24,000 Senate press releases in 2007
- Method
    - Bayesian Hierarchical Topic Model

## 发现

::: {.panel-tabset}
### Focus

![Committee leaders focus on their committee's issues](https://drhuyue.site:10002/sammo3182/figure/theory_grimmer1.png){fig-align="center" height=500}

:::{.notes}
Fig. 4 Chairman and ranking members of committees allocate more attention to issues under their committees' jurisdiction than other senators. **This figure compares the attention that Senate committee leaders---chairs or ranking members---dedicate to topics under their committee jurisdictions to the attention allocated by the rest of the Senate.** The solid dots represent the expected difference, the thick lines are 80% credible intervals, and the thin lines are 95% intervals. Along the left-hand vertical axis, the topics are listed, and on the right-hand side, the corresponding committee names are listed. In all but seven cases, the dot is to the right of the zero line, indicating that leaders of committees discuss issues that highlight their power in the institution more often than other senators.
:::


### Cluster

![Expressed agendas cluster geographically](https://drhuyue.site:10002/sammo3182/figure/theory_grimmer2.png){fig-align="center" height=500}


:::{.notes}
Fig. 5 Attention to issues follows expected geographic patterns. **This figure demonstrates that senators' expressed agendas are grouped geographically.** The left-hand plot shows that senators from western states allocate substantial attention to public-land issues. Darker shades indicate that the average expected attention from the state's delegation to public-land issues is larger. The center plot carries out a comparison of three different regional issues: public-land and western states (top estimate), hurricanes and gulf coast states (middle estimate), and border-security and states that share an international border (bottom estimate). The point in each plot represents the expected difference between the attention to senators in a geographic area allocate to an issue and the attention senators from other areas of the country dedicate to the same issue. The thick and thin lines are 80% and 95% HPD intervals for this difference. Each point is to the right of the zero, indicating that the issues receive more attention in the geographic areas we would expect. The right-hand plot shows that senators from states with a large number of farms per person also tend to allocate more attention to agriculture issues. The horizontal axis represents the number of farms per resident of the state (one measure of agriculture's importance to a state), and the vertical axis indicates the proportion of press releases allocated to agricultural issues. The gray lines are lowess curves indicates the relationship between the number of farms per capita and the attention to agriculture, whereas the black line is the average relationship.
:::

### Predict

![Attention to appropriations predicts opposition to earmark reform](https://drhuyue.site:10002/sammo3182/figure/theory_grimmer3.png){fig-align="center" height=500}

:::{.notes}
An earmark is a provision inserted into a discretionary spending appropriations bill that directs funds to a specific recipient while bypassing the merit-based or competitive funds allocation process. These provisions are often associated with legislation specifying certain congressional spending priorities or in revenue bills that apply to a limited number of individuals or entities. 

The DeMint-McCain amendment was a significant proposal related to earmark reform. Senator Jim DeMint introduced a one-year earmark moratorium amendment, which was co-sponsored by both Republicans and Democrats. The amendment aimed to end the era of earmarks and was seen as a step towards increased transparency, accountability, and fiscal responsibility in Congress. Despite its support from some senators and advocacy groups, the amendment was defeated in a late-night Senate vote, with a result of 29-71.

Fig. 6 Senators who dedicate more attention to appropriations were more likely to oppose Demint-McCain. This figure shows that **senators who dedicate more attention to appropriations in their press releases are more likely to oppose the Demint-McCain amendment.** The vertical axis plots the vote on the amendment, and along the horizontal axis is the average proportion of press releases dedicate to discussing appropriations secured for fire departments. To generate the light gray lines, I took 100 draws from each senator's posterior expressed agenda and then regressed the earmark vote on the draw from the posterior. The gray lines represent the expected probability of supporting the DemintMcCain amendment, and the solid black line is the expected value of the relationship, averaged over the draws from the posterior distribution on the expressed agenda. 

The left-hand figure shows that senators who discuss fire department grants more often were more likely to oppose the DemintMcCain amendment, and the center plot shows that this relationship was even stronger for an aggregate appropriations category. The right-hand plot shows that the relationship remains even after conditioning upon estimated ideal points of senators, suggesting that consistency explains components of voting behavior beyond ideal point estimates.
:::
:::

## 🌰 II

@BenoitEtAl2016

- Objective
    - Professionals vs. Masses
- Method
    - Crowd-sourced identification
- Data
    - 18,263 natural sentences from *British Conservative, Labour and Liberal Democrat* manifestos
    

## 操作

::: {.panel-tabset}
### 编码

![](https://drhuyue.site:10002/sammo3182/figure/theory_crowd-sourced.jpg){fig-align="center" height=500}

:::{.notes}
Figure 1: Hierarchical coding scheme for two policy domains with ordinal positioning.
:::

### 结果

![](https://drhuyue.site:10002/sammo3182/figure/theory_expert-crowd.png){fig-align="center" height=500}

:::{.notes}
Figure 2. British party positions on economic and social policy 1987 – 2010; sequential expert text processing (vertical axis) and independent expert surveys (horizontal).

(Labour red, Conservatives blue, Liberal Democrats yellow, labeled by last two digits of year)
:::

:::



## 🌰 III

@DietrichEtAl2019

- Objective
    - Speakers' emotional state
- Method
    - Analyses of vocal pitch
- Data
    - 74,158 Congressional floor speeches

## 操作

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_vocalPitch_hcy.png){fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/theory_vocalPitch.png){.fragment fig-align="center" height=600}

:::


## 🌰 IV

@ZhangPan2019

- Objective
    - Group activities from social media
- Method
    - CNN for images; CNN-RNN for texts
- Data
    - A random sample of 20,000 geocoded posts from Weibo, 2010--2017

:::{.notes}
CNN: Convolutional Neural Network

RNN: Recurrent Neural Network
:::

## 操作

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_casm.png){fig-align="center" width=1000}

![](https://drhuyue.site:10002/sammo3182/figure/theory_runningMan.png){.fragment fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/theory_banner.png){.fragment fig-align="center" height=500}

:::



## 挑战


*理论*: Single causal mechanism 

[*前提*: Bag of words]{.fragment}

:::{.fragment}
*数据*

* DGP
    + 随机抽取    
    + Only posted   
    + One-time censor
* 非结构化
* 海量潜在维度
* 内容复杂且微妙
:::



:::{.notes}

审查已经使用 machine learning

Su, Y.-S., Y. Ruan, S. Sun, and Y.-T. Chang. 2020. “A Pattern Recognition Framework for Detecting Changes in Chinese Internet Management System.” Journal of Social Computing 1(1): 28–39.
:::



# 以数知文: 理解原则


## 本节要回答之问题

:::{.large style="text-align:center; margin-top: 2em"}
1. 资源哪里找？
1. 信息如何用？
1. 数据能干啥？
1. **[红线]{.red}在哪里**？
:::


## 原生数据

:::: {.columns}

::: {.column width="20%"}
* Email/短信
* 网站HTML
* RSS feeds
* 网络社交媒体
* 网络论坛
* 网络问答平台
* 媒体数据库
* 网络交易行为    
……
:::

::: {.column width="80%"}

![社交媒体](https://drhuyue.site:10002/sammo3182/figure/text_social-media.gif){fig-align="center" height=500}

:::{.notes}
新浪微博1.4亿，微信用户5.5亿;
微信日增数据500TB，QQ日增数据200TB。
:::
:::

::::


## 公共开放平台

![](https://drhuyue.site:10002/sammo3182/figure/text_zhongsheng_index.png){fig-align="center" height=600}

## 网络问政平台

![](https://drhuyue.site:10002/sammo3182/figure/text_henan-Egovernment.png){fig-align="center" height=600}


## 社会化问答网站

![](https://drhuyue.site:10002/sammo3182/figure/text_zhihu.png){fig-align="center" height=600}

:::{.notes}
Quora, Stack Overflow
:::


## 媒体数据库

![](https://drhuyue.site:10002/sammo3182/figure/text_renminribao.png){fig-align="center" height=600}


## 问卷开放性问题

![](https://drhuyue.site:10002/sammo3182/figure/text_survey_openQuestion.png){fig-align="center" height=600}


## 二手数据

::: {.panel-tabset}
### 档案数据

* 中国知网等数据库（期刊、报刊、年鉴等）
* Google Books、百度学术
* Google Trend、百度指数
* JSTOR Data for Research……

### 百度指数

![](https://drhuyue.site:10002/sammo3182/figure/text_baidu_zhishu.png){fig-align="center" height=500}

### 文本数字化

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_hongkong_lib.png){fig-align="center" height=500}

![](https://drhuyue.site:10002/sammo3182/figure/theory_cbdb.png){.fragment fig-align="center" height=500}
:::

:::


## 文本获取

* 原生数据：Spider/crawler/scraper
* 二手数据：档案数据和数字化数据

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_bazhuayu.png){fig-align="center" height=450}

![](https://drhuyue.site:10002/sammo3182/figure/theory_huochetou.png){.fragment fig-align="center" height=450}

![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseeker.png){.fragment fig-align="center" height=450}
:::

## 操作演示

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseekerI.png){.fragment fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseekerII.png){.fragment fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseekerIII.png){.fragment fig-align="center" height=600}
:::

## 编程抓取

`SelectorGadget` (Chrome add-in)

![](https://drhuyue.site:10002/sammo3182/figure/theory_zhongsheng_page.png){fig-align="center" height=500}

## Scrapping with `rvest`

```{r zhongsheng-eg}
#| eval: false
#| echo: true

ls_zhongsheng <-
  read_html("http://politics.people.com.cn/GB/8198/426918/index.html") |> # index page
  html_nodes("h5 a") |> # the nodes of the links
  html_attr("href") |> # just the links
  str_replace("^/n1", "http://politics.people.com.cn/n1")

df_zhongsheng <- map_df(ls_zhongsheng, function(link) {
  zs_article <- read_html(link, encoding = "GB18030") # read the html
  
  zs_title <- html_nodes(zs_article, "h1") |>
    html_text
  
  zs_time <- html_nodes(zs_article, ".box01 .fl") |>
    html_text |>
    str_extract("\\d{4}年\\d{2}月\\d{2}日")
  
  zs_content <- html_nodes(zs_article, "#rwb_zw p") |>
    html_text |>
    str_c(collapse = "") |> # combined the paragraphs
    str_remove_all("\\s|\\n|\\t") # remove the horizontal spaces
  
  zs_data <- data.frame(title = zs_title,
                        time = zs_time,
                        content = zs_content)
})
```

## 正则表达式

![](https://drhuyue.site:10002/sammo3182/figure/theory_regular_expression.png){fig-align="center" height=600}

## 文本数据结构化

![](https://drhuyue.site:10002/sammo3182/figure/theory_zhongsheng_output.png){fig-align="center" width=1000}


## 文本分析的基础原则 [@GrimmerStewart2013]

:::{.incremental .large style="text-align:center; margin-top: 1em"}
1. All quantitative models of language are [wrong]{.red}---but some are useful. 
1. Quantitative methods for text amplify resources and [augment humans]{.red}. 
1. There is [no globally best]{.red} method for automated text analysis. 
1. [Validate, Validate, Validate.]{.fragment .highlight-blue}
:::

# 望数生义

## 文本分析（传统）方法概览

![@GrimmerStewart2013](https://drhuyue.site:10002/sammo3182/figure/text_analysis-method.png){fig-align="center" height=550}

## 研究层次

:::: {.columns}

::: {.column width="50%"}

**数据层次**

- Corpus
    - Document (volumn, chapter, section)
        - Paragraph [&check;]{.orange}
            - Sentence [&check;]{.green}
                - Clause [&check;]{.green}
                    - **Word (Unigram)** [&check;]{.green}
                        - **Token** [&check;]{.green}
                        

:::{.fragment .callout-tip}
## "Token"：语言特征单元

- Token in a document: term
- Token in a group: N-gram, e.g., a word = a Unigram
:::

:::

::: {.column .fragment width="50%"}

**分析层次**

### 描述

词频、词云(👎)、网络

### 聚类

知类分文、知文分类

### 语义

情感分析(sentiment analysis)

:::

::::


## A Bag of Words

> All quantitative models of language are [wrong]{.red}---but some are useful [@GrimmerStewart2013].

:::{.fragment .callout-warning}
## Bag of words (BoW)

A text is represented as the bag (multiset) of its words.
:::

![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){.fragment fig-align="center" height=300}


## 从自然语言到计算机语言

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){fig-align="center" height=600}

:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## 预处理

:::{style="text-align:center; margin-top: 2em"}
- Segmentation
- Tokenization
- Stopwords (停词）/function words removing   
- 其他根据研究目的的删减
:::


## Segmentation

:::: {.columns}

::: {.column width="30%"}
*Scriptio discreta* (e.g., English)

Document &rarr; paragraphs/sentences

:::{.notes}
discrete script
:::

:::

::: {.column .fragment width="70%"}
*Scriptio continua* (e.g, CJK) 

```{r segment-eg}
#| code-fold: false

library(jiebaR)

zhongsheng <- "近年来，美国一些政客被“美国优先”遮住了双眼，大搞贸易保护主义、单边主义，肆意挥舞关税大棒，全然不顾中美两国人民和全世界人民的强烈反对。"
cutter <- worker() # segment engine

segment(zhongsheng, cutter)
```
:::

:::{.notes}
continuous script

《人民日报》（2019年05月30日02版）
:::


::::

## Tokenization

:::: {.columns}

::: {.column width="50%"}
- 目标：去除Syntax
    + 大小写
    + 标点
    + 非字符（@#￥%……&*）
    - 停词
:::

::: {.column .fragment width="50%"}
```{r stop-words}
#| echo: false

read_lines(STOPPATH)[883:903]
read_lines(STOPPATH)[157:177]
```
:::

::::


:::{.fragment .callout-tip}
## 中文停词表

* 百度停词表
* 哈工大停词表
* 网络搜集停词表，如["最全中文停用词表整理（1893个）"](https://blog.csdn.net/shijiebei2009/article/details/39696571)
:::

## Scriptio discreta special

- Lemmatization: (happy, happier, happiness) &rarr; happy
- Stemming: (happy, happier, happiness) &rarr; happi

:::{.fragment .callout-tip}
## More Examples

- Lemmization: (went, leaves, geese, unhappy) &rarr; (go, leaf, goose, unhappy)
- Stemming: (went, leaves, geese, unhappy) &rarr; (went, leav, geese , unhappi)
:::


:::{.fragment style="text-align:center; margin-top: 1em"}
*为什么要这么做？*

:::{.fragment}
还能做什么

- Labeling
    - Content vs. function
    - Linguistic features: n., v., adj., adv., prep., conj.......
:::


:::


:::{.notes}
Issue: Sparse matrix
:::


## 综合的🌰

![2019-05-14 ~ 05-22: 中美贸易战](https://drhuyue.site:10002/sammo3182/figure/text_zhongsheng.png){fig-align="center" height=550}

## 数据清理

*原始数据*

```{r zhongsheng-clean}
df_zhongsheng <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/zhongsheng.RDS"))

df_zhongsheng$segmented <- map_chr(df_zhongsheng$content, function(content){
  segment(content, cutter) |> paste(collapse = " ")
})

df_zhongsheng$phase <- "US_fail"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-22"] <- "theory_test"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-11"] <- "reassessment"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-08"] <- "optimism"


df_token <- df_zhongsheng |> 
  select(-content) |> 
  unnest_tokens(word, segmented) # tokenization

# Show the word counts
select(df_token, -phase) |> head()
```

:::{.fragment}
*去掉停词*

```{r zhongsheng-stop}
#| code-fold: true

# removing the stop words
df_stopWords <- tibble(word = read_lines(STOPPATH))

df_token <- df_token |> 
  anti_join(df_stopWords) 

select(df_token, -phase) |> head()
```

:::

## 词频分析

```{r zhongsheng-frequency, fig.align='center'}
df_plot <- df_token |> count(word, sort = TRUE) |> 
  filter(n > 151) |>
  mutate(word = reorder(word, n))

ggplot(df_plot) +
  geom_col(aes(word, n)) +
  xlab("高频词") +
  ylab("词频") + 
  theme(axis.text = element_text(size = 20)) +
  coord_flip()
```

## 词频异质性

```{r zhongsheng-phase-freq}
#| fig-height: 6
#| fig-pos: center

df_plot <- df_token |> 
  group_by(phase) |> 
  count(word, sort = TRUE) |> 
  top_n(10) |>
  ungroup() |> 
  mutate(word = reorder(word, n),
         word = factor(word, levels = rev(unique(word))),
         phase = factor(phase, labels = c("乐观观望", "重新评估", "理论检验", "美国必败")))

ggplot(df_plot, aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("词频") + 
  theme(axis.text = element_text(size = 12)) +
  facet_wrap(~ phase, scales = "free")
```

:::{.notes}
The Economist journalist Simon Rabinovitch
:::

## 相关性分析

```{r zhongsheng-fre-compare}
#| fig-height: 7
#| fig-pos: center

frequency <- df_token |> 
  count(phase, word) |>
  group_by(phase) |>
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  spread(phase, proportion) |> 
  gather(phase, proportion, optimism:theory_test)

library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `US_fail`, color = abs(`US_fail` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, family = "WenQuanYi Micro Hei") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~phase, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "美国必败", x = NULL) +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

:::{.notes}
x = proportion, y = `US_fail`

proportion = n / sum(n)
:::

## 关键词识别与分析

2019-05-07 关键词

```{r keyword1}
extractor_keyword <- worker("simhash", topn = 5)

simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-07"], extractor_keyword)
```


:::{.fragment}
2019-05-30 关键词

```{r keword2}
simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"], extractor_keyword)
```
:::



## 相似度分析

通过“距离”测量相似度

> Hamming distance: the distance between two strings of equal length is the number of [positions]{.blue} at which the [corresponding symbols are different]{.red}. 

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/theory_hammingDis_org.png){fig-align="center" height=num}
![](https://drhuyue.site:10002/sammo3182/figure/theory_hammingDis_comp.png){fig-align="center" height=num}
:::

:::{style="text-align:center; margin-top: 1em"}
e.g., H(100→011) = 3; H(010→111) = 2.
:::


:::{.notes}
It measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.
:::

## 相似度能告诉你什么

:::: {.columns}

::: {.column width="50%"}
*05-30 vs. 05-[22]{.red}*

```{r keyword-distnace}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-22"],
         extractor_keyword)
```
:::

::: {.column width="50%"}
*05-30 vs. 05-[23]{.red}*

```{r keyword-distance2}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-23"][1],
         extractor_keyword)
```
:::

::::

## 主题模型(Topic modeling)

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V × K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::


## 总结

:::: {.columns}

::: {.column width="50%"}
1. 认知
    + 丰富资源
    + 技术门槛
1. 原则
    + 在“错误”的前提下寻找价值
:::

::: {.column width="50%"}
3. 操作
    - 打散：预处理与结构化
    - 聚合：
        - 词频
        - 相关性/相似度
        - 主题模型
:::
::::

![Distant reading](https://drhuyue.site:10002/sammo3182/figure/text_indirect_phone.gif){.fragment fig-align="center" height=300}



# 感谢倾听，欢迎交流 {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## 参考文献

::: {#refs}
:::
