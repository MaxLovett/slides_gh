@article{Arthur2013,
  title = {Tech Giants May Be Huge, but Nothing Matches Big Data},
  author = {Arthur, Charles},
  year = {2013},
  month = aug,
  journal = {The Guardian},
  issn = {0261-3077},
  urldate = {2023-07-08},
  abstract = {When Nasdaq stopped trading this week, it again showed how global firms are at the mercy of a power that created them},
  chapter = {Technology},
  langid = {british},
  language = {en-GB},
  keywords = {Amazon,Business,Google,Internet,Nasdaq,Technology,Technology sector}
}

@inproceedings{BengioEtAl2000,
  title = {{A neural probabilistic language model}},
  booktitle = {{Advances in Neural Information Processing Systems}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  year = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  urldate = {2023-05-27},
  abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
  langid = {catalan},
  language = {ca},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Bengio et al2000_A Neural Probabilistic Language Model.pdf}
}

@article{CironeHobbs2022,
  title = {Asymmetric Flooding as a Tool for Foreign Influence on Social Media},
  author = {Cirone, Alexandra and Hobbs, William},
  year = {2022},
  month = mar,
  journal = {Political Science Research and Methods},
  pages = {1--12},
  publisher = {{Cambridge University Press}},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2022.9},
  urldate = {2022-04-24},
  abstract = {Research on Russian troll activity during the 2016 US presidential campaign largely focused on divisive partisan messaging. Here, we document the use of apolitical content\textemdash content that could counteract mobilization efforts and escape detection in future campaigns. We argue this resembled techniques used by autocratic regimes domestically, in ``flooding'' social media with entertainment content to distract from and displace mobilizing messaging. Using automated text analysis and hand coding to construct a timeline of IRA messaging on Twitter, we find left-leaning trolls posted large volumes of entertainment content in their artificial liberal community and shifted away from political content late in the campaign. Simultaneously, conservative trolls were targeting their community with increases in political content. This suggests the use of apolitical content might be an overlooked strategy to selectively manipulate levels of attention to politics.},
  langid = {english},
  language = {en},
  keywords = {2016 US Election,authoritarian propaganda,disinformation,Russia,Social media,text and content analysis,Twitter},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Cirone_Hobbs2022_Asymmetric flooding as a tool for foreign influence on social media.pdf;D\:\\zotero_system\\storage\\CBH33EAG\\1D007EBDEF5D511812B615B9F6D61B02.html}
}

@article{DietrichEtAl2019,
  title = {Pitch Perfect: Vocal Pitch and the Emotional Intensity of Congressional Speech},
  author = {Dietrich, Bryce J. and Hayes, Matthew and O'Brien, Diana Z.},
  year = {2019},
  journal = {American Political Science Review},
  pages = {Forthcoming},
  urldate = {2019-05-30},
  abstract = {Though audio archives are available for a number of political institutions, the data they provide receive scant attention from researchers. Yet, audio data offer important insights, including information about speakers' emotional states. Using one of the largest collections of natural audio ever compiled\textemdash 74,158 Congressional floor speeches\textemdash we introduce a novel measure of legislators' emotional intensity: small changes in vocal pitch that are difficult for speakers to control. Applying our measure to MCs' floor speeches about women, we show that female MCs speak with greater emotional intensity when talking about women as compared to both their male colleagues and their speech on other topics. Our two supplementary analyses suggest that increased vocal pitch is consistent with legislators' broader issue commitments, and that emotionally intense speech may affect other lawmakers' behavior. More generally, by demonstrating the utility of audio-as-data approaches, our work highlights a new way of studying political speech.},
  timestamp = {2019-05-30T02:50:44Z},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Dietrichet al2019_Pitch Perfect - Vocal Pitch and the Emotional Intensity of Congressional Speech.pdf}
}

@incollection{GartnerHiebl2017,
  title = {Issues with Big Data},
  booktitle = {The {{Routledge Companion}} to {{Accounting Information Systems}}},
  author = {G{\"a}rtner, Bernhard and Hiebl, Martin R. W.},
  year = {2017},
  publisher = {{Routledge}},
  abstract = {This chapter aims to discuss opportunities and challenges which arise through the use of Big Data for management accounting. It outlines understanding of Big},
  isbn = {978-1-315-64721-0},
  langid = {english},
  language = {en}
}

@article{GebruEtAl2017,
  title = {Using Deep Learning and Google Street View to Estimate the Demographic Makeup of Neighborhoods across the United States},
  author = {Gebru, Timnit and Krause, Jonathan and Wang, Yilun and Chen, Duyun and Deng, Jia and Aiden, Erez Lieberman and {Fei-Fei}, Li},
  year = {2017},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {50},
  pages = {13108--13113},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1700035114},
  urldate = {2023-06-29},
  abstract = {The United States spends more than \$250 million each year on the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed several years. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may become an increasingly practical supplement to the ACS. Here, we present a method that estimates socioeconomic characteristics of regions spanning 200 US cities by using 50 million images of street scenes gathered with Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22 million automobiles in total (8\% of all automobiles in the United States), were used to accurately estimate income, race, education, and voting patterns at the zip code and precinct level. (The average US precinct contains {$\sim$}1,000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next presidential election (88\% chance); otherwise, it is likely to vote Republican (82\%). Our results suggest that automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time.},
  langid = {english},
  language = {en},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Gebru et al2017_Using deep learning and Google Street View to estimate the demographic makeup.pdf}
}

@article{Goldston2008,
  title = {{Big data: data wrangling}},
  shorttitle = {{Big data}},
  author = {Goldston, David},
  year = {2008},
  month = sep,
  journal = {Nature},
  volume = {455},
  number = {7209},
  pages = {15--15},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/455015a},
  urldate = {2023-06-28},
  abstract = {Collecting and releasing environmental data have stirred up controversy in Washington, says David Goldston, and will continue to do so.},
  copyright = {2008 Springer Nature Limited},
  langid = {jv},
  language = {jv},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Goldston2008_Big data.pdf;D\:\\zotero_system\\storage\\SG96EBEN\\455015a.html}
}

@book{Hey2009,
  title = {The Fourth Paradigm: Data-Intensive Scientific Discovery},
  shorttitle = {The Fourth Paradigm},
  author = {Hey, Tony},
  editor = {Tansley, Stewart and Tolle, Kristin},
  year = {2009},
  month = oct,
  edition = {1st edition},
  publisher = {{Microsoft Research}},
  address = {{Redmond, Washington}},
  abstract = {This book presents the first broad look at the rapidly emerging field of data-intensive science, with the goal of influencing the worldwide scientific and computing research communities and inspiring the next generation of scientists. Increasingly, scientific breakthroughs will be powered by advanced computing capabilities that help researchers manipulate and explore massive datasets. The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud-computing technologies. This collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized.},
  isbn = {978-0-9825442-0-4},
  langid = {english},
  language = {en},
  keywords = {big data}
}

@incollection{King2016,
  title = {Preface: Big Data Is Not about the Data!},
  booktitle = {Computational {{Social Science}}: {{Discovery}} and {{Prediction}}},
  author = {King, Gary},
  editor = {Alvarez, R. Michael},
  year = {2016},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  langid = {english},
  language = {en},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\KingPreface.pdf}
}

@article{KingEtAl2017,
  ids = {kingₕow₂017},
  title = {How the Chinese Government Fabricates Social Media Posts for Strategic Distraction, Not Engaged Argument},
  author = {King, Gary and Pan, Jennifer and Roberts, Margaret E.},
  year = {2017},
  month = aug,
  journal = {American Political Science Review},
  volume = {111},
  number = {3},
  pages = {484--501},
  issn = {0003-0554, 1537-5943},
  abstract = {The Chinese government has long been suspected of hiring as many as 2 million people to surreptitiously insert huge numbers of pseudonymous and other deceptive writings into the stream of real social media posts, as if they were the genuine opinions of ordinary people. Many academics, and most journalists and activists, claim that these so-called 50c party posts vociferously argue for the government's side in political and policy debates. As we show, this is also true of most posts openly accused on social media of being 50c. Yet almost no systematic empirical evidence exists for this claim or, more importantly, for the Chinese regime's strategic objective in pursuing this activity. In the first large-scale empirical analysis of this operation, we show how to identify the secretive authors of these posts, the posts written by them, and their content. We estimate that the government fabricates and posts about 448 million social media comments a year. In contrast to prior claims, we show that the Chinese regime's strategy is to avoid arguing with skeptics of the party and the government, and to not even discuss controversial issues. We show that the goal of this massive secretive operation is instead to distract the public and change the subject, as most of these posts involve cheerleading for China, the revolutionary history of the Communist Party, or other symbols of the regime. We discuss how these results fit with what is known about the Chinese censorship program and suggest how they may change our broader theoretical understanding of ``common knowledge'' and information control in authoritarian regimes.},
  langid = {english},
  language = {English},
  timestamp = {2019-10-03T06:14:11Z},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\King et al2017_How the Chinese Government Fabricates Social Media Posts for Strategic2.pdf;D\:\\MEGAsync\\Nutstore\\01_Literature\\Kinget al2017_How the Chinese Government Fabricates Social Media Posts for Strategic.pdf}
}

@article{LazerEtAl2009,
  title = {Computational Social Science},
  author = {Lazer, David and Pentland, Alex and Adamic, Lada and Aral, Sinan and Barab{\'a}si, Albert-L{\'a}szl{\'o} and Brewer, Devon and Christakis, Nicholas and Contractor, Noshir and Fowler, James and Gutmann, Myron and Jebara, Tony and King, Gary and Macy, Michael and Roy, Deb and Van Alstyne, Marshall},
  year = {2009},
  month = feb,
  journal = {Science},
  volume = {323},
  number = {5915},
  pages = {721--723},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1167742},
  urldate = {2023-06-28},
  langid = {english},
  language = {en},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Lazer et al2009_Computational Social Science.pdf;D\:\\zotero_system\\storage\\QI647PGA\\Lazer et al. - 2009 - Computational Social Science.pdf}
}

@article{Liu2021a,
  title = {The {{Rise}} of {{Data Politics}}: {{Digital China}} and the {{World}}},
  shorttitle = {The {{Rise}} of {{Data Politics}}},
  author = {Liu, Lizhi},
  year = {2021},
  month = mar,
  journal = {Studies in Comparative International Development},
  volume = {56},
  number = {1},
  pages = {45--67},
  issn = {1936-6167},
  doi = {10.1007/s12116-021-09319-8},
  urldate = {2023-05-17},
  abstract = {Data has become one of the most valuable assets for governments and firms. Yet, we still have a limited understanding of how data reshapes international economic relations. This paper explores various aspects of data politics through the lens of China's digital rise and the country's global engagement. I start with the theoretical premise that data differs from traditional strategic assets (e.g., land, oil, and labor), in that it is nonrival and partially excludable. These characteristics have generated externality, commitment, and valuation problems, triggering three fundamental changes in China's external economic relations. First, data's externality problem makes it necessary for states to regulate data or even to pursue data sovereignty. However, clashes over data sovereignty can ignite conflicts between China and other countries. Second, the commitment problem in data use raises global concerns about foreign government surveillance. As data is easier to transfer across borders than physical commodities, Chinese tech companies' investments abroad are vulnerable to national security investigations by foreign regulators. Chinese tech companies, therefore, confront a ``deep versus broad'' dilemma: deep ties with the Chinese government help promote their domestic business but jeopardize their international expansion. Lastly, data's valuation problem makes traditional measures (e.g., GDP) ill-suited to measure the relative strengths of the world's economies, which may distort perceptions of China and other states.},
  langid = {english},
  language = {en},
  keywords = {China,Data,Politics of Technology,Privacy,Surveillance},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Liu2021_The Rise of Data Politics.pdf}
}

@article{LuPan2022,
  title = {The Pervasive Presence of Chinese Government Content on Douyin Trending Videos},
  author = {Lu, Yingdan and Pan, Jennifer},
  year = {2022},
  month = feb,
  journal = {Computational Communication Research},
  volume = {4},
  number = {1},
  publisher = {{Amsterdam University Press}},
  issn = {2665-9085},
  doi = {10.5117/CCR2022.2.002.LU},
  urldate = {2023-06-29},
  abstract = {Abstract As audiences have moved to digital media, so too have governments around the world. While previous research has focused on how authoritarian regimes employ strategies such as the use of fabricated accounts and content to boost their reach, this paper reveals two different tactics the Chinese government uses on Douyin, the Chinese version of the video-sharing platform TikTok, to compete for audience attention. We use a multi-modal approach that combines analysis of video, text, and meta-data to examine a novel dataset of Douyin videos. We find that a large share of trending videos are produced by accounts affiliated with the Chinese government. These videos contain visual characteristics designed to maximize attention such as high levels of brightness and entropy and very short duration, and are more visually similar to content produced by celebrities and ordinary users than to content from non-official media accounts. We also find that the majority of videos produced by regime-affiliated accounts do not fit traditional definitions of propaganda but rather contain stories and topics unrelated to any aspect of the government, the Chinese Communist Party, policies, or politics.},
  langid = {english},
  language = {en},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Lu_Pan2022_The Pervasive Presence of Chinese Government Content on Douyin Trending Videos.pdf;D\:\\zotero_system\\storage\\Z7E3WL6M\\CCR2022.2.002.html}
}

@article{MichelEtAl2011,
  title = {Quantitative Analysis of Culture Using Millions of Digitized Books},
  author = {Michel, Jean-Baptiste and Shen, Yuan Kui and Aiden, Aviva Presser and Veres, Adrian and Gray, Matthew K. and Team, The Google Books and Pickett, Joseph P. and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and Orwant, Jon and Pinker, Steven and Nowak, Martin A. and Aiden, Erez Lieberman},
  year = {2011},
  month = jan,
  journal = {Science},
  volume = {331},
  number = {6014},
  pages = {176--182},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1199644},
  abstract = {We constructed a corpus of digitized texts containing about 4\% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of `culturomics,' focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities. Linguistic and cultural changes are revealed through the analyses of words appearing in books. Linguistic and cultural changes are revealed through the analyses of words appearing in books.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  langid = {english},
  language = {English},
  pmid = {21163965},
  timestamp = {2021-01-28T12:21:35Z},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Michel et al2011_Quantitative Analysis of Culture Using Millions of Digitized Books3.pdf;D\:\\zotero_system\\storage\\U7EULTTI\\Michel et al2011_Quantitative Analysis of Culture Using Millions of Digitized Books.pdf}
}

@misc{Palmer2006,
  title = {Data Is the {{New Oil}}},
  author = {Palmer, Michael},
  year = {2006},
  month = nov,
  journal = {ANA Marketing Maestros},
  urldate = {2023-07-08},
  abstract = {By Michael Palmer "Data is the new oil!" Clive Humby, ANA Senior marketer's summit, Kellogg School. Data is just like crude. It's valuable, but if unrefined it cannot really be used. It has to be changed into gas, plastic, chemicals,...}
}

@inproceedings{PenningtonEtAl2014,
  title = {{{GloVe}}: Global Vectors for Word Representation},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  urldate = {2023-05-27},
  langid = {english},
  language = {en},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Pennington et al2014_GloVe.pdf}
}

@article{Samuel2000,
  title = {Some Studies in Machine Learning Using the Game of Checkers},
  author = {Samuel, A. L.},
  year = {2000},
  month = jan,
  journal = {IBM Journal of Research and Development},
  volume = {44},
  number = {1.2},
  pages = {206--226},
  issn = {0018-8646},
  doi = {10.1147/rd.441.0206},
  abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
  langid = {english},
  language = {en}
}

@article{ScienceStaff2011,
  title = {Challenges and Opportunities},
  author = {{Science Staff}},
  year = {2011},
  month = feb,
  journal = {Science},
  volume = {331},
  number = {6018},
  pages = {692--693},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.331.6018.692},
  urldate = {2023-06-28},
  langid = {english},
  language = {en},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\SCIENCE STAFF2011_Challenges and Opportunities.pdf}
}

@misc{Singh2023,
  title = {Is Data Still the New Oil?},
  author = {Singh, Malvika},
  year = {2023},
  month = apr,
  journal = {Medium},
  urldate = {2023-07-08},
  abstract = {``Information is the oil of the 21st century, and analytics is the combustion engine.''{$\mkern1mu$}\textemdash{$\mkern1mu$}Peter Sondergaard, 2011},
  langid = {english},
  language = {en}
}

@article{TheEconomist2017,
  title = {The World's Most Valuable Resource Is No Longer Oil, but Data},
  author = {{The Economist}},
  year = {2017},
  month = may,
  journal = {The Economist},
  issn = {0013-0613},
  urldate = {2023-07-08}
}

@article{YeeEtAl2021,
  title = {Image Cropping on Twitter: Fairness Metrics, Their Limitations, and the Importance of Representation, Design, and Agency},
  shorttitle = {Image Cropping on Twitter},
  author = {Yee, Kyra and Tantipongpipat, Uthaipon and Mishra, Shubhanshu},
  year = {2021},
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  number = {CSCW2},
  pages = {450:1--450:24},
  doi = {10.1145/3479594},
  urldate = {2023-06-29},
  abstract = {Twitter uses machine learning to crop images, where crops are centered around the part predicted to be the most salient. In fall 2020, Twitter users raised concerns that the automated image cropping system on Twitter favored light-skinned over dark-skinned individuals, as well as concerns that the system favored cropping woman's bodies instead of their heads. In order to address these concerns, we conduct an extensive analysis using formalized group fairness metrics. We find systematic disparities in cropping and identify contributing factors, including the fact that the cropping based on the single most salient point can amplify the disparities because of an effect we term argmax bias. However, we demonstrate that formalized fairness metrics and quantitative analysis on their own are insufficient for capturing the risk of representational harm in automatic cropping. We suggest the removal of saliency-based cropping in favor of a solution that better preserves user agency. For developing a new solution that sufficiently address concerns related to representational harm, our critique motivates a combination of quantitative and qualitative methods that include human-centered design.},
  langid = {english},
  language = {en},
  keywords = {demographic parity,ethical HCI,fairness in machine learning,image cropping,representational harm},
  file = {D\:\\MEGAsync\\Nutstore\\01_Literature\\Yee et al2021_Image Cropping on Twitter.pdf}
}
