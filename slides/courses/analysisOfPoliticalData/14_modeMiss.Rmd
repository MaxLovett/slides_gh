---
title: "Moderation & Missing"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - ../../../css/zh-CN_custom.css
      - ../../../css/styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    chakra: ../../../libs/remark-latest.min.js # to show slides offline
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable, gridExtra,
  knitr, # dependency
  descr, stringr, broom, dotwhisker, interplot, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(114)
```

class: inverse, bottom

# Moderation in Theory

---

class: small

## What's Moderation

Moderation effect aims to test conditional effects of one variable on the contribution of another variable to the dependent variable and has been extensively applied in the empirical research of social science since the 1970s.

--

Unfortunately, the nonlinear nature determines that the statistical estimate of an interactive effect cannot be interpreted as straightforward as the coefficient of a regular regression parameter.

--

Formally

$$Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3X\times Z + u.$$

Testing the conditional effect of Z on X's contribution (or the conditional effect of X on Z's contribution) to the variance of Y.

---

## Hypothesis

H<sub>0</sub>: If effects of X depends on Z (&beta;<sub>3</sub> = 0).

H<sub>0</sub>: If X has an effect (&beta;<sub>1</sub> + &beta;<sub>3</sub>Z = 0). .magenta[(More useful)]

--

Level set: &alpha; = 0.05

---

## Special Case: Dichotomous Z

* When Z = 0, $\beta_1 = 0;$
* When Z = 1, $\beta_1 + \beta_3 = 0.$
* Let X = X<sup>0</sup> when Z = 0; X = X<sup>1</sup> when Z = 1, then 

\begin{align}
\hat Y =& \hat\beta_0 + \hat\beta_1X + \hat\beta_2Z + \hat\beta_3X\times Z\\
\Leftrightarrow \hat{\tilde{Y}} =& \hat{\tilde\beta_0} + \hat{\tilde\beta_1}X^0 + \hat{\tilde\beta_2}Z + \hat{\tilde\beta_3}X^1
\end{align}

--

* When Z = 0, $H_0: \tilde\beta_1 = 0;$
* When Z = 1, $H_0: \tilde\beta_3 = 0.$

---

## General Case: Z Is Continuous

Effect of X: 

$$\frac{\partial Y}{\partial X} = \beta_1 + \beta_3Z.$$

Statistics:

$$\frac{\hat\beta_1 + \hat\beta_3Z}{SE(\hat\beta_1 + \hat\beta_3Z)}\sim t_{n - 4}$$

---

## How to Interpret It Wrong?

### Isolated interpretation of &beta;<sub>1</sub>

Let Z increases by c: 

\begin{align}
Y =& \beta_0' + \beta_1'X + \beta_2'(Z + c) + \beta_3'X(Z + c) + u.\\
  =& (\beta_0' + \beta_2'c) + (\beta_1' + \beta_3'c)X + \beta_2'Z + \beta_3'XZ \\
  &+ u
\end{align}

--

The coefficient of X can change by changing Z.

---

### Interpreting &beta;<sub>3</sub> Omitting X

$$Y = \beta_0 + \beta_2Z + \beta_3X\times Z + u', u' = \beta_1X + u$$

--

Then 

\begin{align}
E(u'|X) \neq& 0\\
E[u'(X,Z)] \neq& 0.
\end{align}

Unless &beta;<sub>1</sub>X or XZ is zero.

---

### Statistical Significance Overstated

When testing the X's effect $\frac{\partial Y}{\partial X} = \beta_1 + \beta_3Z.$

The standard error:

$$SE_{\frac{\partial Y}{\partial X}} = \sqrt{var(\hat{\beta_1}) + Z^2var(\hat{\beta_3}) + 2Zcov(\hat{\beta_1}, \hat{\beta_3})}.$$

It's perfectly possible for the contribution of X on Y to be statistically significant for certain values of Z, even if all of the model parameters are insignificant. 

???

 Brambor, Clark and Golder 2006

---

$$\frac{\partial Y}{\partial X} = \beta_1 + \beta_3Z.$$

In other words, one cannot infer whether X has a meaningful conditional effect on Y .magenta[simply from the magnitude and significance of either &beta;<sub>1</sub> or &beta;<sub>3</sub>].

--

Instead, the conditional effect should be examined based on the .magenta[marginal effect] at every observed value of Z.

---

## More Risky Points

* Point estimation may not be reliable
    + Link function (e.g., logit and probit) is involved
    
???

Berry, DeMeritt, and Esarey (2016)

--

* Substantive significance of the conditional effect highly relates to the distribution of the conditioning variable (viz., Z in the above example). 

???

Berry, Golder, and Milton (2012)

---

## Recommendation

1. Plotting the marginal effects.
1. Showing the confidence intervals
1. Presenting the frequency distribution of Z
    + esp., when the effect trend goes across the zero point.

---

## Marginal Effects Isn't a Panacea

The estimation of the regular marginal effects might cause a "multiple comparison problem"

Leading to over- or under-confidence of the confidential intervals.

--

.magenta[Solution]: Adjust the CIs with a critical t-statistics by Benjamini 1995

???

Esarey and Sumner (2017)

---

class: inverse, bottom

# Interaction in Practice

---

## Operating in `R`

.left-column[
Package

* `effects`
* `sjplot`


* `interplot`
]

--

.right-column[
<img src="images/yueHu.png" height = 200 width = 265 />
<img src="images/solt.jpg" height = 200 />
]


---

## Installation

To install:

* the latest released version: `install.packages("interplot")`.
* the latest developing version: `devtools::install_github("sammo3182/interplot")`.

---

## Run a Model

This example is based on the `mtcars` dataset, which is drawn from the 1974 volume of the US magazine *Motor Trend*. 

* Y: Mileage in miles per (US) gallon (`mpg`)
* Conditioned/moderated X: The number of engine cylinders (`cyl`)
* Conditioning/moderator X: Automobile weight in thousands of pounds (`wt`)

---

```{r data}
select(mtcars, mpg, cyl, wt) %>% kable(format = "html") #load the data
```

---

\begin{align}
Mileage =& \beta_0 + \beta_1 Cylinder + \beta_2 Weight + \\
         & \beta_3 Cylinder\times Weight + u
\end{align}

* `cylinder`, `weight`: Base terms
* `cylinder * Weight`: Two-way (multiplicative) interaction term

---

```{r interaction, echo = TRUE}
m_cyl <- lm(mpg ~ wt * cyl, data = mtcars)
summary(m_cyl)
```

---

## Basic Graph

.left-column[
* `m`: Moderation model
* `var1`: Moderated variable
* `var2`: Moderator variable

Hint: CI(Max- Min)
]


.right-column[
```{r fig.width = 6, fig.height=5, message=FALSE}
interplot(m = m_cyl, var1 = "cyl", var2 = "wt")
```
]


---

* `ci`: CI levels; default 95% CI

```{r fig.show='hold', fig.width=4.5}
interplot(m = m_cyl, var1 = "cyl", var2 = "wt")
interplot(m = m_cyl, var1 = "cyl", var2 = "wt", ci = .8)
```

(Changing to 80% CI)

---

## Dot-Whisker vs. Line-Rainbow

```{r fig.show='hold', fig.width=4.5}
interplot(m = m_cyl, var1 = "cyl", var2 = "wt")
interplot(m = m_cyl, var2 = "cyl", var1 = "wt")
```

---

## Better Appearance

```{r fig.width = 10}
interplot(m = m_cyl, var1 = "cyl", var2 = "wt") + 
  # Add labels for X and Y axes
    xlab("Automobile Weight (thousands lbs)") +
    ylab("Estimated Coefficient for\nNumber of Cylinders") +
  # Change the background
    theme_bw() +
  # Add the title
    ggtitle("Estimated Coefficient of Engine Cylinders \non Mileage by Automobile Weight") +
    theme(plot.title = element_text(face="bold")) +
  # Add a horizontal line at y = 0
    geom_hline(yintercept = 0, linetype = "dashed")
```

---

```{r eval = FALSE, echo=TRUE}
interplot(m = m_cyl, 
          var1 = "cyl", 
          var2 = "wt") +
  # Add labels for X and Y axes
  xlab("Automobile Weight (thousands lbs)") +
  ylab("Estimated Coefficient for\nNumber of Cylinders") +
  # Change the background
  theme_bw() +
  # Add the title
  ggtitle("Estimated Coefficient of Engine Cylinders \non Mileage by Automobile Weight") +
  theme(plot.title = 
          element_text(
            face = "bold")) +
  # Add a horizontal line at y = 0
  geom_hline(yintercept = 0, 
             linetype = "dashed")
```

---

## Special Case

### Quadratic Model

```{r fig.width = 5, eval = FALSE}
m_wt <- lm(mpg ~ wt + I(wt^2), data = mtcars)

interplot(m = m_wt, var1 = "wt", var2 = "wt")
```

---

## Dichotomous Z

```{r fig.width = 10}
mtcars$gear <- factor(mtcars$gear)
m_gear <- lm(mpg ~ gear * wt, data = mtcars)

interplot(m = m_gear, var1 = "wt", var2 = "gear")
```

---

## Include the Distribution of the Conditioning Variable

@Berry2012 points out that, when a variable's conditional effect reaches statistical significance over only part of the range of the conditioning variable, it can be helpful to the evaluation of the substantive significance of the conditional effect to know the distribution of the conditioning variable. For this purpose, `interplot` has the `hist` argument for users to choose to superimpose a histogram at the bottom of the conditional effect plot.

```{r fig.width = 5}
interplot(m = m_cyl, var1 = "cyl", var2 = "wt", hist = TRUE) +
    geom_hline(yintercept = 0, linetype = "dashed")
```

Our implementation of this option was inspired by the excellent work of @Hainmueller2016.
A tip is that when presenting the histogram, some default setting would not be directly modified by the build-in arguments or the `geom` functions. 
Instead, one can change these settings by the `aes` function---as illustrated by the following example.

```{r fig.width = 5}
interplot(m = m_cyl, var1 = "cyl", var2 = "wt", hist = TRUE) +
  aes(color = "pink") + theme(legend.position="none") +  # geom_line(color = "pink") + 
  geom_hline(yintercept = 0, linetype = "dashed")
```


## Advanced Use: Adjust the Overconfidence in Confidence Interval Estimations

@Berry2016 emphasized the importance to estimate the uncertainty in studying conditional effects.
The most common way to do that is to follow the straightforward method of @Brambor2006.
Nevertheless, @Esarey2017 pointed out that this method might cause a "multiple comparison problem" and result over- or underconfidence of the confidential intervals.
For the overconfidence cases, they recommended to adjust the CIs with a critical t-statistics following the Benjamini1995 procedure.

`interplot` incorporates this recommendation with an argument `adjCI`.
If it is set `TRUE`, the function will calculate the critical t-statistics to limit the false discovery rate and adjust the estimation of confidence intervals. 
In the following example, the left panel presents the conditional effect plot with CIs estimated based on the @Brambor2006 method, and the right panel presents results with adjusted CIs.
Although the adjustment does not change the substantial conclusion, we can see the adjusted CIs cover more area both in the middle and at the extreme values of x. 

```{r fig.width = 7}
stdCI_plot <- interplot(m = m_wt, var1 = "wt", var2 = "wt", adjCI = FALSE) +
ggtitle("Marginal Effects with Standard CIs")
adjCI_plot <- interplot(m = m_wt, var1 = "wt", var2 = "wt", adjCI = TRUE) +
ggtitle("Marginal Effects with Adjusted CIs")

grid.arrange(stdCI_plot, adjCI_plot, ncol = 2)
```

## Adnvaced Use: Plot Conditional Predicted Probability

@Hanmer2013 point out (also in the associated [post](https://ajps.org/2014/11/13/translating-statistical-results-into-meaningful-quantities-methods/) in the AJPS website) that it is important to translate statistical results into meaningful quantities. 
The task not only pushes researchers to interpret the results in a real-life manner---which may lead to substantively different conclusions---but also provides convenience for a far broader scope of readers (e.g., policy makers, governmental officials, stakeholders, etc.) to understand the implications of the study and their importance.

To accomplish this task requires researchers to go beyond estimating the effect for the "average case," but focuses more on the values or intervals that are illustratively important and meaningful.
This could be a time-consuming job, though, especially when researchers work on models with limited dependent variables.
`interplot` provides a convenient way to achieve the task.
When researchers are dealing with general linear flat or multilevel models, they have the choice to set the argument `predPro` to `TRUE` and give the critical values they are interested in the argument `var2_vals`.
Then, `interplot` will automatically estimated the conditional effects of predicted probabilities at these given values of the conditioned variable.
The following example illustrates how it works.

In this example, we are interested how the economic inequality affect the impact of income on the U.S. citizens' belief in meritocracy, a critical ideology of the "American Dream."
We estimated this conditional effect based on an interaction model with three years of Pew surveys (2006, 2007, and 2009), in which the income is the conditioned variable and economic inequality (county-level Gini coefficients).^[For the purpose of illustration, we omitted the county level variance in this example. One can see more comprehensive analyses in @Solt2016.]
We first estimate the average conditional effect for the entire sample and then estimate the conditional predicted probabilities for citizens with the lowest and highest levels of income separately.
As shown in the following plot, the average case in the left panel only shows a decreasing conditional effect, while the predicted probability plot in the right uncovers conditional effects in opposite directions for the high and low income individuals.

```{r fig.width = 7, warning = FALSE, message = FALSE}
pew1.w <- read.csv("pew1_w.csv")

m <- glm(formula=meritocracy~ginicnty+income_i+ginicnty:income_i+income_cnty+black_cnty+
                perc_bush04+pop_cnty+educ_i+age_i+gender_i+unemp_i+union_i+partyid_i+
                ideo_i+attend_i+survid2006+survid2007+survid2009,
              data=pew1.w,family=binomial(link="logit"))
              
plot_avg <- interplot(m, var1 = "ginicnty",var2 = "income_i", predPro = FALSE) + 
ggtitle("Average Conditional Effects")

plot_3val <- interplot(m, var1 = "ginicnty",var2 = "income_i", predPro = TRUE, var2_vals = c(min(pew1.w$income_i), max(pew1.w$income_i))) +
ggtitle("Conditional Predicted Probabilities for \nCitizens with Low and High Incomes") +
scale_colour_discrete(guide = guide_legend(title = "Income"), labels = c("Low", "High")) + 
scale_fill_discrete(guide = guide_legend(title = "Income"), labels = c("Low", "High")) +
theme(legend.position = c(0, .8), legend.justification = c(0, .5))

grid.arrange(plot_avg, plot_3val, ncol = 2)
```



## Advanced Use: Plot Conditional Effects Without a Model

In some cases, one may analyze some complicated or self-made regression functions which are not supported by the current version of `interplot`. For such models, as long as the user has a dataset loading the simulated results of the interaction effects, she can still use `interplot` to visualize it. The dataset needs four columns the scale of the conditioning variable (`fake`), the simulated interactive effect at each break of the conditioning variable (`coef1`), and the simulated lower bound and upper bound of the confidence interval (`lb`, and `ub`). The column names should be exactly the ones shown in the above parentheses. Here is an example with some arbitrary artificial data:

```{r fig.width = 5.5}
# Create a fake dataset of conditional effects
fake <- rnorm(100, 0, 1)
coef1 <- fake * sample(.5:2.5, 100, replace = T)
lb <- coef1 - .5
ub <- coef1 + .5

df_fake <- data.frame(cbind(fake, coef1, lb, ub))

# Use interplot directly with the dataset
interplot(df_fake)
```

If one also has the data of the `var2`, she can also draw a histogram under it by the argument `var_dt`. 

```{r fig.width = 5.5}
var2_fake <- fake
# Set `hist` to TRUE is required to superimpose a histogram.
interplot(df_fake, hist = TRUE, var2_dt = var2_fake)
## The ribbon and histogram do not fit. This is just an illustration
```


## Conclusion

The `interplot` package provides a flexible and convenient way to visualize conditional coefficients of variables in multiplicative interaction terms. This vignette offers an overview of its use and features. We encourage users to consult the help files for more details. 

The development of the package is ongoing, and future research promises a compatible tool for more types of regressions and more functions. Please contact us with any questions, bug reports, and comments.