---
title: "Probability Theory"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    number-sections: true
    css: https://sammo3182.github.io/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%

revealjs-plugins:
  - spotlight

# lightbox: auto
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---

```{r setup}
#| include = FALSE

library(pacman)

p_load(
  "knitr",
  "tidyverse",
  "DiagrammeR"
)

# Functions preload
set.seed(313)
```



## Overview {.unnumbered}

* Probability as a Concept
* Probabilities as Estimand
* Probabilities as Statistics

# Probability: Concept

## Determinism vs. Probabilism

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/prob_zhuyingfusheng.jpg){fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/prob_smokingCancer.png){.fragment fig-align="center" height=500}
:::

:::{.notes}
- Determinism
    - 说岳全传
        - 赵匡胤赤须龙转世， 烛影斧声被赵光义所害，转世金兀术扰乱大宋江山，为岳飞所阻，赵构杀岳飞，苗刘兵变后阳痿，传位养子孝宗赵昚（shen4）
        - 女土蝠听经放屁，被大鹏金翅明王啄死，转世王氏。大鹏鸟犯杀戒下凡，见铁背虬龙操练虾兵蟹将，啄瞎一眼，后转世秦桧，乌龟王八螃蟹转世张俊、罗汝辑、万俟卨(qi xie)
- Probabilism

:::

## Perspectives of Probability

- **Frequentist ("objective")**
    - "Coin has two sides, balance" &rArr; Pr(head) = 0.5.
- **Relative frequency**
    - You are observing a random sample.
    - $$\lim_{n\to \infty}\frac{f(x)}{n}$$
- **Evolutionary ("subjective")**: Bayesian, based on beliefs (prior), game theory


# Probability: Estimand

## How to Know the Probability in Statistics

> Sample Space: Space of all possible outcomes

1. &sum; Pr = 1
1. All of the probabilities constitute the probability distribution
1. For all event in the space, $P(E) + P(\tilde E) = 1$


:::{.fragment}
*Methods to "discover" the probability in a sample space*

1. Sample Point Method
1. Event Composition Method
1. Bayesian Method
:::


## Sample Point Method

"Count out"

1. Define experiment event;
1. Define sample space;
1. Assign probabilities, P(E<sub>i</sub>) &GreaterEqual; 0, &sum; P(E<sub>i</sub>) = 1;
1. Define event of interest, A;
1. Find Pr(A) by summing probability of A.
    - With or without [replacement]{.red}? 


## Replacement

W/o replacement means once being picked out, never put it back.

:::: {.columns}

::: {.column width="30%"}
![](https://drhuyue.site:10002/sammo3182/figure/prob_drawBalls.jpg){fig-align="center" height=num}
:::

::: {.column width="70%"}
Five soccer balls and five basketballs (in the same size and surface).

What's the chance (a.k.a., probability) to get 5 soccer balls in five spilling out?
:::

::::


- W. replacement: $0.5^5$; 
- W/o replacement: $\frac{5}{10}\times\frac{4}{9}\times\frac{3}{8}\times\frac{2}{7}\times\frac{1}{6} = {10 \choose 5}^{-1} = \frac{5!(10 - 5)!}{10!}.$ 


:::{.notes}
n! factorial

- w. independent, no memory
- w.o no repetition, global results
:::

## Think deeper

Is the cross-sectional survey sampling a w. or w.o. replacement process? 

![CFPS](https://drhuyue.site:10002/sammo3182/figure/prob_cfps.png){fig-align="center" height=500}

:::{.notes}
CFPS is w.o. replacement
:::

## Event Composition Method (ECM)

"Calculate" 

Applying [multiplicative and additive]{.red} rules.

::::{.columns}
:::{.column width="30%"}
- E<sub>1</sub>: m outcomes
- E<sub>2</sub>: n outcomes
:::

:::{.column .fragment width="70%"}
&rArr; $m\times n$ possible outcomes for E<sub>1</sub>E<sub>2</sub>. 
:::
::::

::::{.columns}
:::{.column width="50%" .fragment}
![](https://drhuyue.site:10002/sammo3182/figure/prob_toys.jpeg)
:::

:::{.column width="50%" .fragment}
Six heads, 15 bodys, 25 colors, how many distinctive types of toys?

$$6 \times 15 \times 25 = 2,250.$$
:::
::::


## Rules of ECM

::::{.columns}
:::{.column width="50%" .fragment}
Permutation: Ordered

$$P(n, r) = \frac{n!}{(n - r)!}.$$
:::

:::{.column width="50%" .fragment}
Combination: ~~Order~~

$${n \choose r} = \frac{n!}{r!(n - r)!}.$$
:::
::::


:::{.fragment style="text-align: center"}

* Intersection: Event G and H, G&cap;H = $\{S_i: S_i\in G\ and\ S_i\in H\}$
* Union: G&cup;H = $\{S_i: S_i\in G\ or\ S_i\in H\}$
    - P(G&cup;H) = P(G) + P(H) - P(G)&cap;P(H).
* Exclusive: G&cap;H = &empty;
* Complement: J &sub; -G &cap; H
* [Conditional: $P(H|G) = \frac{P(G\cap H)}{P(G)}$]{.red}
    - Independent: P(H|G) = P(H); P(G&cap;H) = P(G)P(H)

:::


## Baysian Method

"Figure out"

::::{.columns}
:::{.column width="50%" .fragment}
Bayes' Theorem

$$
\begin{aligned}
P(A|B)P(B) =& P(B|A)P(A) = P(A\cap B);\\
P(A|B)=& \frac{P(B|A)P(A)}{P(B)},\\
=& \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\tilde A)P(\tilde A)}
\end{aligned}
$$

:::

:::{.column width="50%" style="text-align: right"}
![](https://drhuyue.site:10002/sammo3182/figure/prob_Bayes.gif)

Thomas Bayes (1702--1761) 
:::
::::


:::{.notes}
1. Conjugate prior: posterior distribution = prior distribution
1. Cromwell's rule: if a region of the parameter space has 0 prior probability, then it also has 0 posterior probability. 
:::

---

![](https://drhuyue.site:10002/sammo3182/figure/prob_bayesComponents.png){height="600"}
 

:::{.notes}

![](https://drhuyue.site:10002/sammo3182/figure/prob_ar.gif)

A new season of Antiques Roadshow verifies antiques for buyers based on machine-learning techniques.
Based on experience of previous seasons, .red[**30%**] of the products were knockoff in the platform. 
When people bring a product to verify, AR can tell .red[**90%**] knockoff knockoff, and .red[**80%**] authentic products authentic. 
Then, what's the probability that a product is knockoff, if AR said so?

$$\begin{align}
& P(K|VK) \\
=& \frac{P(VK|K)P(K)}{P(VK|K)P(K) + P(VK|\tilde K)P(\tilde K)}, \\
=& \frac{.9 * .3}{.9 * .3 + .2*.7} = .659.
\end{align}$$


```{dot bayesDiagram}
digraph usedCar {
  # a 'graph' statement
  graph [overlap=true fontsize=10]

  # several 'node' statements
  node [shape=circle fixedsize=true width=0.9] // sets as circles
  
  N[label = "Nature"];
  MF[label = "Verify"];
  MS[label = "Verify"];
  RFF[label = "Result"];
  RFS[label = "Result"];
  RSF[label = "Result"];
  RSS[label = "Result"]

  # several 'edge' statements
  N -> MF[label = "K: .3"]
  N -> MS[label = "A: .7"]
  MF -> RFF[label = "K: .9"]
  MF -> RFS[label = "A: .1"]
  MS -> RSF[label = "K: .2"]
  MS -> RSS[label = "A: .8"]

}
```


:::

## Bayesian probability of your roommate is a pervert

{{< video https://drhuyue.site:10002/sammo3182/video/prob_bayesianApplication.mp4 title="Bi dao" height=600 preload="auto" controls allowfullscreen>}}


## Diss

Frequentist 🔫 Bayesian

1. *Repeated sample*: When the population data are collected, the repeat-sample makes no sense.
1. *Future event*: It is impossible to have repeat samples for future.

:::{.fragment}
Bayesian 🔫 Frequentist

![](https://drhuyue.site:10002/sammo3182/figure/prob_freqBayesian.png){.lightbox fig-align="center" height=400}
:::

# Probability: Statistics

## Four Common Ways to Presenting Probability

1. Percentage &check;
1. Relative risk
1. Odds Ratio.



## Relative Risk

* $\hat{\pi}_{1|1}/\hat{\pi}_{1|2} = \frac{n_{11}/n_{\bullet 1}}{n_{12}/n_{\bullet 2}}$

:::{.fragment}
E.g., A hospital investigated whether alcoholism relates to fatal illness. 

| Alcoholism 	| Yes 	| No 	|
|----------	|-------	|----------	|
| Fatal      	| 1,601  	| 510   	|
| Nonfatal       	| 162,527   	| 412,368   	|

$$Risk_{fatal} = \frac{1601/(1601 + 162527)}{510/(510 + 412368)} = 7.897$$
:::

[When people drink, they are *7.9 more times* to die in a fatal accident than those w/o.]{.fragment}


:::{.fragment .callout-important appearance="simple"}
**The above line distinguishes a scholar from a student.**
:::



## Odds Ratio

::::{.columns}
:::{.column width="50%" .fragment}
\begin{align}
O_{12} =& \frac{\pi_{1|1}/\pi_{1|2}}{\pi_{2|1}/\pi_{2|2}}, \\
 =& \frac{n_{11}/n_{12}}{n_{21}/n_{22}}, \\
 =& n_{11}n_{22}/n_{21}n_{12}.
\end{align}
:::

:::{.column width="50%" .fragment}
The previous e.g., 

| Alcoholism 	| Yes 	| No 	|
|----------	|-------	|----------	|
| Fatal      	| 1,601  	| 510   	|
| Nonfatal       	| 162,527   	| 412,368   	|

$O_{fatal} = \frac{1601/510}{162527/412368}$ = 7.965
:::
::::

[The fatal risk of alcoholism is *8 times larger* than not.]{.fragment}

:::{.fragment}
::: {.callout-tip appearance="minimal"}
![](https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png){.lightbox fig-align="center" height=30} *Log odds*: mapping the range (0,1) to (-&infin;, +&infin;) using log of odds, 

- A.k.a., "logistic unit" (logit) 
- If independent, then log odds = 0.
:::
:::



## RR vs. OR

:::{style="text-align: center"}
![](https://drhuyue.site:10002/sammo3182/figure/prob_rrOr.png){height="450"}
:::

* For common cases, RR is better.
* For rare cases, both ok.

## Puting Probability Together

> (Probability) distribution: The mathematical [function]{.red} that gives the probabilities of occurrence of different possible [outcomes]{.red} for an [experiment]{.red}.


::::{.columns}
:::{.column width="50%" .fragment}

**Marginal and joint totals**

\begin{align} 
n_{i\bullet} =& \sum_j n_{ij}; n_{\bullet j} = \sum_i n_{ij}; \\
n_{\bullet\bullet} =& \sum\sum n_{ij}.
\end{align}

**Conditional distribution**

\begin{align} 
\pi_{i|j} = \pi_{ij} / \pi_{\bullet j}; \\
\hat{\pi_{i|j}} = n_{ij}/n_{\bullet j}.
\end{align}

:::

:::{.column width="50%" .fragment}
**Marginal and joint distributions**

- Marginal \begin{align}
\pi_{i\bullet} =& \sum_j \pi_{ij}, \hat{\pi_{i\bullet}} =  n_{i\bullet}/n_{\bullet\bullet}; \\
\pi_{\bullet j} =& \sum_i \pi_{ij}, \hat{\pi_{\bullet j}} =  n_{\bullet j}/n_{\bullet\bullet}.
\end{align}

- Joint: $\pi_{\bullet\bullet}$ = &sum;&sum; &pi;<sub>ij</sub> = 1.
:::
::::


## Calculate the Distributions

:::: {.columns}

::: {.column width="60%"}
E.g., Into *Young Woman and the Sea*?

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|
:::

::: {.column .fragment width="40%"}
![](https://drhuyue.site:10002/sammo3182/figure/prob_youngWomanSea.webp){fig-align="center" height=300}
:::

::::

- What's the probability for a female that don't like?
    - $Pr(FN) = 5/28.$ (event probability)
- What's the probability for finding a disliker? 
    - $Pr(N) = 17/28.$ (marginal distribution)
- What's the probability of disliking given the respondent is a female?
    - $P(N|F) = 5/11.$ (conditional distribution)



## Independence

$$P(X\cap Y) = P(X)P(Y); P(Y|X) = P(Y).$$

::::{.columns}
:::{.column width="40%" .fragment}
Such event's probability is: 

- Population: $\pi_i = n_i/N$;
- Sample: $\hat{\pi_i} = n_i/n_{\bullet\bullet}.$
:::

:::{.column width="60%" .fragment}
E.g., *Young Woman and the Sea*?

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|

- Are gender and approval independent? 
    - Pr(MY) = [11/28 &times; 17/28]{.red} &ne; 5/28
:::
::::

:::{.notes}
11/28 &times; 17/28: [Expected value for independence]{.red}

$\chi^2 = \sum(\frac{Observation - Expected}{Expected})^2.$
:::


## Take-home point

::: {style="text-align: center"}
[![](https://drhuyue.site:10002/sammo3182/figure/prob_mindmap.png){ height="600"}](http://103.238.162.29:8787/auth-sign-in?appUri=%2F)
:::
