---
title: "Multiple Regression"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
institution: "Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse, flexable, icons, xaringanExtra
) 


use_xaringan_extra(c("tile_view", # O
                     "broadcast",
                     "panelset",
                     "tachyons",
                     "fit_screen"))
use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = FALSE  #<<
)


# Functions preload
set.seed(313)

theme_set(theme_minimal())
```

## Overview

+ Multiple regressions and their coefficients
+ R<sup>2</sup>
+ Post-estimation inferences

---

class: inverse, bottom

# Regression with Two (or More) Xs

---

class: center, middle

## Terminology

Multivariate regression/analysis

Multiple regression 

--

### Expression

Population Regression Function (PRF): $$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i$$
According to CLRM: $E[u_i|\boldsymbol{X}] = 0.$

Sample Regression Function (SRF): $$Y_i = \hat\beta_0 + \hat\beta_1X_{1i} + \hat\beta_2X_{2i} + \hat u_i$$

---

class: small

## Goal

.left-column[
$min_{\hat\beta_0,\hat\beta_1,\hat\beta_2}\sum \hat u_i^2\Rightarrow$

\begin{align}
\frac{\partial \sum \hat u_i^2}{\partial\hat\beta_0}\to& 0\\
\frac{\partial \sum \hat u_i^2}{\partial\hat\beta_1}\to& 0\\
\frac{\partial \sum \hat u_i^2}{\partial\hat\beta_2}\to& 0
\end{align}
]

--

.right-column[
When Achieved:

\begin{align}
\hat\beta_0 =& \bar Y - (\bar\beta_1X_{1i} + \bar\beta_2X_{2i}),\\
\hat\beta_1 =& \frac{[\sum(Y_i - \bar Y)(X_{1i} - \bar X_1)][\sum(X_{2i} - \bar X_2)^2-\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]}{\sum(X_{1i} - \bar X_1)^2\sum(X_{2i} - \bar X_2)^2 - [\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]^2}\\
            =& \frac{\sum\hat r_{1i}(Y_i - \bar Y)}{\hat r_{1i}^2},
\end{align}

where $\hat r_{1i}$ are the errors from the regression of $X_{1i}$ on $X_{2i}$ (i.e., $X_{1i} = \hat\delta_0 + \hat\delta_1X_{2i} + \hat r_{1i}$), the proportion that $X_2$cannot explain.


$\hat\sigma^2 = \frac{\sum\hat u_i^2}{n - 3}$
]

---

## What does &beta;<sub>1</sub> mean?

Every unit change in X<sub>1</sub> leads to &beta;<sub>1</sub> changes in Y .red[on average], .red[holding everything else constant].

--

* Change the same amount?

???

On average

--

* Constant how?

???

hold on mean or median

---

class: inverse, bottom

# R<sup>2</sup>

---

class: small

## Multiple Coefficient of Determination

.left-column[
$$R^2 = \frac{\sum(\hat{Y} - \bar Y)^2}{\sum(Y - \bar Y)^2}$$
]

--

.right-column[
From the SRF, 

\begin{align}
Y_i =& \hat Y_i + \hat u_i\\
Y_i - \bar Y =& \hat Y_i  - \bar Y + \hat u_i\\
\Rightarrow (Y_i - \bar Y)^2 =& (\hat Y_i  - \bar Y + \hat u_i)^2\\
                         =& (\hat Y_i  - \bar Y)^2 + \hat u_i^2 + 2u_i(\hat Y_i  - \bar Y)\\
\text{Sum up, } \Rightarrow \sum(Y_i - \bar Y)^2 =& \sum(\hat Y_i  - \bar Y)^2 + \sum\hat u_i^2\\
SST =& SSR + SSE\\
1 =& \frac{SSR}{SST} + \frac{SSE}{SST}\\
\text{In which, } R^2 =& \frac{SSR}{SST} = \frac{\sum(\hat Y_i  - \bar Y)^2}{SST}\\
                      =& \frac{\sum[\hat\beta_0 + (\hat\beta_1X_1 +\cdots +\hat\beta_nX_n)  - \bar Y]^2}{SST}
\end{align}

p.s., SST: Sum of Squares Total, SSR: Sum of Squares Regression, SSE: Sum of Squares Error.
]
---

background-image: url("images/mr_bad4ya.jpg")
background-position: center
background-size: contain

---

## Why Is R<sup>2</sup> Bad?

.bg-black.golden.ba.shadow-5.ph4.mt3[
.center[Can be very low for a correct model]
]

--

\begin{align}
R^2 =& \frac{SSR}{SST} \\
    =& \frac{\sum(\hat Y_i  - \bar Y)^2}{SST}\\
    =& \frac{\sum[(Y_i - u_i) - \bar Y]^2}{SST}
\end{align}

When the residual (&sigma;, estimated by u<sub>i</sub> in sample estimations) is large enough, R<sup>2</sup> could approach a very low score towards zero.

---

```{r rsmall, fig.width=15, fig.height=9, fig.align='center'}
r2 <- function(sig){
  x <- seq(1,10,length.out = 1000)        # our predictor
  y <- 2 + 1.2*x + rnorm(1000,0,sd = sig) # our response; a function of x plus some random noise
  summary(lm(y ~ x))$r.squared           # print the R-squared value
}

df_r <- tibble(sigma = seq(0.5,20,length.out = 20), 
               rout = map_dbl(sigma, r2))

ggplot(data = df_r, aes(x = sigma, y = rout)) +
  geom_point() + 
  ylab(expression(R^2)) + 
  xlab(expression(sigma)) +
  ggtitle("The R squared is calculated based on the CORRECT model\n with only the residual varying.") + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold")
)
```

---

.bg-black.golden.ba.shadow-5.ph4.mt3[
.center[Can be very high for a misspecified model]
]

--

```{r rlarge, fig.width=15, fig.height=7, fig.align='center'}
df_rLarge <- tibble(x = rexp(50,rate=0.005), # from an exponential distribution
                    y = (x-1)^2 * runif(50, min=0.8, max=1.2))# non-linear data generation

rout <- summary(lm(y ~ x, data = df_rLarge))$r.squared %>% round(digits = 2) 

ggplot(data = df_rLarge, aes(x, y)) +
  geom_point() +
  labs(title = expression("True model:"~ Y == (X - 1)^2),
       subtitle = expression("Estimated model:"~ Y == beta[0] + beta[1]*X + sigma ),
       caption = bquote(R^2 == ~ .(rout) )) + 
  theme(
  plot.title = element_text(size = 28),
  plot.subtitle = element_text(size = 20),
  plot.caption = element_text(size = 20),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"))
```

---

.bg-black.golden.ba.shadow-5.ph4.mt3[
.center[Can be very high for a redundant model]
]

--

\begin{align}
R^2 =& \frac{SSR}{SST} = \frac{\sum(\hat Y_i - \bar Y)^2}{SST}\\
    =& \frac{\sum[\hat\beta_0 + (\hat\beta_1X_1 +\cdots +\hat\beta_nX_n) - \bar Y]^2}{SST}
\end{align}

Therefore, the more Xs are added, the larger SSR (and thus R<sup>2</sup>) is, a.k.a., the "trash-can" model.

???

$\hat Y_i  - \bar Y = 0$

---

## Adjusted R<sup>2</sup>?

$$\text{Adj.} R^2 = 1 - (1 - R^2)\frac{n - 1}{n - k - 1}.$$

--

.pull-left[

### Issue adjusted

X booming

]

--

.pull-right[

### Issues not adjusted

* goodness of fit;
* predictive error;
* model comparison;
* X's explanatory power.

]

---

class: center, middle

.Large[When can R<sup>2</sup> be useful?]

???

Model comparison

---

class: inverse, bottom

# Post-Estimation Inferences

---

## Predicted Value 

### Goal

1. Forecast
1. Interpretation: 
    + How the model is close to the reality?
    + What substantive changes can Xs make?

--

### Approach

1. The expected value (average) of $\hat Y$
1. A one-time draw of $\hat Y$

---

## Expected Value

Let X<sub>0</sub> be the values at which we want to predict, the the expected value of Y is:

.pull-left[
\begin{align}
E(\hat Y_0|X_0) =& E(\hat Y_0|X = X_0) = \boldsymbol{X_0\beta}\\
var(\hat Y_0|X_0) =& var(\hat\beta_0) + var(\hat\beta_1)X_0^2 \\
                   &+ 2cov(\hat\beta_0, \hat\beta_1)X_0\\
                  =& \sigma^2[\frac{1}{n} + \frac{(X_0 - \bar X)^2}{\sum(X_i - \bar X)^2}].
\end{align}
]

--

.pull-right[

```{r predicted, fig.width=10}
ggplot(mtcars, aes(x=mpg, y=wt)) +
  geom_smooth(method=lm, se=TRUE) +
  ylab(bquote(hat(Y[0]))) + 
  xlab(bquote(hat(X[0]))) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold")
)
```

Why wider at the two terminals?
]

???

Fewer observations at two terminals, thus wider rainbow.

---

## Single-Point Forecast

\begin{align}
\hat Y_0 =& \hat\beta_0 + \hat\beta_1X_0 + \hat u\\
var(\hat Y_0|X_0) =& \sigma^2[1 + \frac{1}{n} + \frac{(X_0 - \bar X)^2}{\sum(X_i - \bar X)^2}].
\end{align}

--

There is an error term to account for.

In other words, single prediction is .red[more uncertain] than the average prediction.

---

## (Review) Hypothesis Testing

1. For some parameter &theta;, typically, 
    + H<sub>0</sub>: &theta;<sub>0</sub> &equiv; &theta; = 0;
    + H<sub>1</sub>: &theta; &ne; 0.
    + Another type: 
        + H<sub>0</sub>: &pi; = 1/2;
        + H<sub>1</sub>: &pi; &ne; 1/2.
1. Pick a level of evidence, typically, &alpha; = 0.05.
1. Define the test statistics, e.g., $\frac{\hat\theta - \theta}{SE(\theta)}\sim t_{n - k}$.
1. Calculate $\hat\theta_0$.
1. Calculate the test statistics.
1. Evaluate H<sub>0</sub> (reject or fail to reject): p-value is the level of marginal significance within a statistical hypothesis test representing the probability of the occurrence of a given event.
    + Significance level (&alpha;) is a pre-set p-value.

---

## NB:

1. The correct description is, AGAIN, that CI has 95% chance including the true &theta;, rather than  &theta; has 95% chance in CI.
1. Because the level of evidence is pre-set, there is no difference between p = 0.049 and 0.001, as long as they are under 0.05.

???

Nota bene (NB): note well

---

## Hypothesis Test for the Coefficient

Let's set &alpha; = 0.05.

--

Hypothesis:

\begin{align}
H_0: \beta =& \beta^*;\\
H_1: \beta \neq& \beta^*.
\end{align}

--

Statistics:

$$\frac{\hat\beta - \beta^*}{\sqrt{\frac{\hat\sigma^2}{\sum(X_i - \bar X)^2}}}\sim t_{n-k}.$$

--

Interpretation: 

For every unit change of X<sub>1</sub>, Y changes by &beta;<sub>1</sub>, .red[holding everything else constant].

---

## Hypothesis Test for the Variance

Let's set &alpha; = 0.05.

--

Hypothesis:

\begin{align}
H_0: \sigma =& \sigma^*;\\
H_1: \sigma \neq& \sigma^*.
\end{align}

--

Statistics:

$$(n - k)\frac{\hat\sigma^2}{\sigma^2}\sim\chi^2.$$

---

## Hypothesis Test for the Restricted Model

Let's set &alpha; = 0.05.

--

\begin{align}
H_0:& \beta_1 + 2\beta_2 = 3\Rightarrow \beta_1 = 3 - 2\beta_2;\\
H_1:& \beta_1 + 2\beta_2 \neq 3.
\end{align}

Then, 

\begin{align}
Y =& \beta_0 + \beta_1X_1 + \beta_2X_2 + u, \text{(unrestricted)}\\
  =& \beta_0 + 3X_1 + \beta_2(X_2 - 2X_1) + u\\
\Leftrightarrow Y - 3X_1 =& \beta_0 + \beta_2(X_2 - 2X_1) + u\\
Y^* =& \beta_0' + \beta_2'Z + u, \text{(restricted)}
\end{align}
where $Y^*=Y - 3X_1; Z = X_2 - 2X_1.$

The test is thus transformed to $H_0: \beta_2' = \beta_2; \beta_0' = \beta_0$

---

Statistics:

$$\frac{\frac{SSR_R - SSR_U}{\Delta k}}{\frac{SSR_U}{n - k_U - 1}} = \frac{\frac{R_U^2 - R_R^2}{\Delta k}}{\frac{1 - R_U^2}{n - k_U - 1}}\sim F_{\Delta k, n - k - 1}$$

--

If the hypothesis is rejected, the unrestricted model is better.

--

Hint: When testing general linear restrictions, that is, comparing the pair of nested models with different Ys, one should not use R<sup>2</sup> to deduct the F test, because $SST_U \neq SST_R$ in this case.

---

## Wrap Up

+ Coefficients and interpretations
+ R<sup>2</sup> sucks
+ Post-estimation inferences
    + Predicted value
    + Hypothesis testing

---

class: center, middle

## <img src = "images/ci_fsmrof.png" height = 60> Latin Terms

*a fortiori*: Not even saying.    
*ad hoc*: To this, immediate purpose.   
*ibid.*: "ibidem", in the same place.   
.red[*ceteris paribus*]: with other conditions remaining the same.    
*c.f.*: "confer", compare   
.red[*e.g.*]: "exempli gratia," for the sake of example.    
*etc.*: "et cetera," and the rest.    
*i.e.*: "id est," that is.    
.red[*n.b./NB*]: "nota bene", note well.    
*per se*: By/of/for/in itself.    
.red[*QED*]: "quod erat demonstrandum", that which was to have been shown.    


* Bonus: Good.
* Vise versa: In a turned position.
