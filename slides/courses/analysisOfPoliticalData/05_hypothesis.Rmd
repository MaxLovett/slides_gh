---
title: "Hypothesis Testing"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - ../../../css/zh-CN_custom.css
      - ../../../css/styles.css
      # - "https://use.fontawesome.com/releases/v5.6.0/css/all.css"
      # - "https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css"
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    chakra: ../../../libs/remark-latest.min.js # to show slides offline
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

## Joint Distribution Revisit 

What's joint distribution in the probability theory? 

--

$$n_{\bullet\bullet} = \sum\sum n_{ij}.$$

--

In statistics, joint distribution refers to the joint patterns of two or more variables. 

---

## Covariance: 

$$\sigma_{X, Y} = \sum(X - \mu_x)(Y - \mu_y)p(x,y)$$.

---

class: small

e.g., n: population 10,000,000, X: education, Y: age, 

| Education 	| 20~35 (mean = 30) 	| 35~55 (mean = 45) 	| 55~100 (mean = 70) 	|
|-----------	|----------------	|----------------	|-----------------	|
| 0         	| 1000000        	| 2000000        	| 5000000         	|
| 1         	| 3000000        	| 6000000        	| 10000000        	|
| 2         	| 18000000       	| 21000000       	| 15000000        	|
| 3         	| 7000000        	| 8000000        	| 4000000         	|

--

\begin{align}
\mu_X =& \sum Xp(x) \\
      =& 0 * 0.8 + 1 * 0.19 + 2 * 0.54 + 3 * 0.19 = 1.84;\\
\mu_Y =& 30 * 0.29 + 45 * 0.57 + 70 * 0.34 = 49.15.\\
\sigma_{x, y} =& (0 - 1.84)(30 - 49.15)* 0.01 \\
               & + (0 - 1.84)(45 - 49.15)*0.02\dots = -3.636
\end{align}

---

## Correlation

For a population:

$$\rho_{X,Y} = \frac{\sum(X - \mu_X)(Y - \mu_Y)p(x,y)}{\sqrt{\sum(X - \mu_X)^2p(X)}\sqrt{\sum(Y - \mu_Y)^2p(Y)}}$$

--

For a sample: 

$r_{XY}=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.$

--

Relations with covariance: $\rho_{X,Y} = \frac{\sigma_{X, Y}}{\sigma_X\sigma_Y}$

---

## Terminology

### Random variable

A variable that can take some array of values, with the probability that it takes any particular value defined according tot some random/stochastic process.

### Probability

The chance of occurrence of $X_i$ given the PDF of X

### Likelihood

How probable a given set of observations is for certain values of the parameters of a distribution.

---

## Probability vs. Likelihood

About probability: the integral, the area, the results

```{r probability, echo=FALSE, fig.height=5}
library(ggplot2)

ggplot(data.frame(x = 0:10, 
                  pr = dbinom(x = 0:10, size = 10, prob = 0.5)), 
       aes(x = x, y = pr)) +
  geom_bar(stat = "identity") + 
  ylab("Probability") +
  xlab("Number of successes")
  
```

---

About likelihood: the parameter, the product, the hypothesis

```{r likelihood,  echo=FALSE, fig.height=7}
ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(x) dbinom(8, 10, x)) +
  ylab("Likelihood") + 
  xlab(expression(paste("Binomial ", rho)))
```

---

Formally, let $O$ be the set of observed outcomes and $\theta$ be the set of parameters that describe the stochastic process. 

Probability function: $P(O|\theta)$

Likelihood function: $\mathcal{L}(\theta|O)$; 
$\mathcal{L} = \prod_{i=1}^n y_i$

Hint: $\mathcal{L}$ can above 1.

???

https://acarril.github.io/posts/probability-likelihood
https://www.youtube.com/watch?v=pYxNSUDSFH4
https://khakieconomics.github.io/2018/07/14/What-is-a-likelihood-anyway.html

---

## Maximum Likelihood Estimation:

* Goal: Model (parameter) selection
    + What's the most appropriate estimation?
        + Maximizing the probability that $\mathcal{L}(\theta \vert O) = P(O \vert \theta)$
    + Point estimation: unbiasdness
    + Interval estimation: range of plausible values
    + Goodness of fit: explained variances
    + Diagnostic estimation: what if the assumptions are violated
    
---

background-image: url("images/mle_illustration.png")
background-position: center
background-size: contain

---

## Hypothesis test:

+ Null model: $Likelihood(null): Y = \beta_0 + \epsilon$
+ Theoretical model: $Likelihood(theory): Y = \beta_0 + \beta_1X + \epsilon$
+ Comparing $\mathcal{L}(null)$ and $\mathcal{L}(theory)$

---

E.g., Given binomial distribution $L(\pi) = {n \choose s}\pi^s(1 - \pi)^{n - s}$, what $\pi$ reaches the maximum likelihood?

$log[L(\pi)] = log{n\choose s} + s\cdot log(\pi) + (n - s)\cdot log(1 - \pi).$

To get the peak value, we use the derivative:

$$\frac{dlog[L(\pi)]}{d\pi} = \frac{s}{\pi} - \frac{n - s}{1 - \pi}.$$

To minimize this, let it equal 0, then $\pi = \frac{s}{n}$

---

## Model Comparison

The famous(notorious)

$$R^2 = \frac{\sum(\hat{Y} - \bar Y)^2}{\sum(Y - \bar Y)^2}$$

--

We'll come back to it in a few weeks. Spoil alert: 

---

background-image: url("images/bad4ya.jpg")
background-position: center
background-size: contain

---

## Better Option

Given k is the number of parameter estimates,

+ Akaike information criterion: $-log(L) + k$, therefore finding the .red[minimum].
+ Bayesian information criterion: $-log(L) + k\times \frac{log(n)}{2}$, n is number of observations

---

## Sample Distribution

* Population: Total collection of individuals
* Random sample: every individual has an equal chance of being selected

---

## Random Sample

+ Sample size: n
+ Value: {$x_1, x_2,\dots, x_n$} of random variable {$X_1, X_2, \dots, X_n$}.
+ IID: identically and independently distributed
    + Identical: X and Y are from the same distribution, ${\displaystyle F_{X}(x)=F_{Y}(x)\,\forall x\in I}$.
    + Independent: ${\displaystyle F_{X,Y}(x,y)=F_{X}(x)\cdot F_{Y}(y)\,\forall x,y\in I}$

---

## Reliable sample:

+ $\bar X \rightarrow \mu; p\rightarrow \pi.$

Unbiased Mean: 

\begin{align}
\bar X =& \frac{\sum X}{n}\\
E(\bar X) =& \frac{1}{n}[E(X_1) + E(X_2) + \dots + E(X_n)]\\
  =& \frac{1}{n}[\mu + \mu + \dots + \mu] = \mu
\end{align}

---

class: small

Unbiased variance (standard error): 

\begin{align}
s(\bar X)^2 =& \frac{[E(s(X_1)^2) + E(s(X_2)^2) + \dots + E(s(X_n)^2)]}{n^2}\\
  =& \frac{1}{n}[\sigma^2 + \sigma^2 + \dots + \sigma^2] = \frac{\sigma^2}{n}\\
\therefore\ SE =& \frac{\sigma}{\sqrt{n}}.
\end{align}

--

+ When n gets larger, $\bar X$ is more concentrated around $\mu$.
+ Central limit theorem: For a random sample of n, $\bar X$ fluctuate around $\mu$ with a SE

---

Q. Given the population mean as 69 and standard deviation as 3.2, how would the mean of a random sample of four observations fluctuate? 

--

\begin{align}
E(\bar X) =& \mu = 69\\
\therefore SE(\bar X) =& \frac{3.2}{\sqrt 4} = 1.6.
\end{align}

---

Q. Given $\mu$ = 72 and $\sigma$ = 9, and a random sample of 10. Calculate the probabilities: (1) P(X > 80); (2) P($\bar X$ > 80)

1. $Z = \frac{80 - 72}{9} = .89\Rightarrow P(Z > .89) =$ `r 1 - pnorm(.89)`;
2. $Z = \frac{80 - 72}{9/\sqrt{10}} = 2.81\Rightarrow P(Z > 2.81) =$ `r 1 - pnorm(2.81)`

---

## Comparison with the mean

$Z = \frac{\bar X - \mu}{SE}= \frac{\bar X - \mu}{\sigma/\sqrt n}$

Hint: A rule of thumb, N > 30, large sample; for time series, N > 80

---

## Proportion (average)

| $x$ 	| $f(x)$   	| $xf(x)$   	| $x^2f(x)$ 	|
|---	|--------	|---------	|--------	|
| 0 	| 1 - $\pi$ 	| 0       	| 0      	|
| 1 	| $\pi$     	| $\pi = \mu$ 	| $\pi$     	|

$\sigma^2 = E(X^2) - \mu^2 = \pi - \pi^2 = \pi(1 - \pi)$
Therefore, $\sigma = \sqrt{\pi(1 - \pi)}.$

---

Normal Approximation rule for proportion: In a sample of N, the sample proportion P fluctuates around the population proportion $\pi$ with SE ( $\sqrt{\frac{\pi(1 - \pi)}{n}}$ ).

The confidence interval: $\pi = P \pm Z\sqrt{\frac{P(1 - P)}{n}}, Z = \frac{P - \mu}{\sqrt{\frac{\pi(1 - \pi)}{n}}}.$

---

E.g., Given the Republican are 60% of the population, while the Democrats are 40. What's the probability that the Republican are the minority in a random sample of 100 people from the population?

--

Minority means $P(\pi < 0.5)$. Then, $Z = \frac{5 - 6}{\sqrt{\frac{6(1 - 6)}{100}}} = -2$, therefore, $P(Z < -2) = $ `r pnorm(-2)`

---

## Small population sampling

Without replacement, SE of $\bar X$ is reduced by $\sqrt{\frac{N - n}{N - 1}}$ (finite population correction, FPC). 

--

Leading to more uncertainty

--

E.g., If only sampling one people, n = 1. Then $SE = \frac{\sigma}{\sqrt{n}}\sqrt{\frac{N - n}{N - 1}} = \frac{\sigma}{\sqrt{n}}$, no changes; if sampling the entire population, then n = N, SE = 0. For a large sample of a large population (n = 1000, N = 100,000,000), the reductive factor is $\sqrt{\frac{100000000 - 1000}{100000000 - 1}}\approx .999$, little changes.

---

* Unbiased estimators: produce the right answer on coverage.
    + How to reduce bias? (1) Randomization; (2) Weight.
    
--

* Efficiency: smaller variance (Hint: mean is 1.57 times more efficient than median for a standard normal)
    + How to increase efficiency? $SE = \frac{\sigma}{\sqrt{n}}$
    
---

+ Finite-sample properties properties (unbiasedness + efficiency) holds irrespective of the size of the sample
+ "Large-sample" properties asymptotic. 

---

## Convergence

When a sequence of random variables stabilizes to a certain probabilistic behavior as n &rarr; &infin; , the sequence is said to show stochastic convergence

$p\lim_{n \to \infty}X_n = a, a\in R.$

--

### Two types of meanings

1. Convergence in .magenta[probability]: values in the sequence eventually take a constant value (i.e. the limiting distribution is a point mass).

1. Convergence in .magenta[distribution]: values in the sequence continue to vary, but the variation eventually comes to follow an unchanging distribution (i.e. the limiting distribution is a well characterized distribution)

---

## Consistency: 

An estimator $\theta_n$ is consistent if the sequence $\theta_1\cdots\theta_n$ converges in probability to the true parameter value $\theta$ as sample size n grows to infinity:

$p\lim_{n \to \infty}\theta_n = \theta.$

--

* Often seen as a minimal requirement for estimators
* A consistent estimator may still perform badly in small samples

---

## Does Unbiasedness Imply Consistency?

--

No, e.g., An estimator can be unbiased but not consistent. For example, for an IID sample {x
1,..., xn} one can use $T_n(X) = x_n$ as the estimator of the mean E[x]. 
$E[T_n(X)] = E[x]$ and it is unbiased, but it does not converge to any value.

--

Only if a sequence of estimators is unbiased and converges to a value, then it is consistent, as it must converge to the correct value.

---

## Does Consistency Imply Unbiasedness?

--

No, e.g., if the mean is estimated by ${1 \over n}\sum x_{i}+{1 \over n}$ it is biased, but as $n\rightarrow \infty$, it approaches the correct value, and so it is consistent.

--

Another example?


???

w.o., Bressel's correction

\begin{align}
s^2 =& \sum(X - \bar X)^2p(x) \\
=& \frac{\sum(X - \bar{X})^2}{n - 1}.
\end{align}