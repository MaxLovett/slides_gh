---
title: "Model Specification"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    css: https://sammo3182.github.io/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    callout-appearance: simple
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
      
filters:
  - lightbox

revealjs-plugins:
  - spotlight

# lightbox: auto
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools,
  ggeffects,
  dotwhisker
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

## Overview

:::{style="text-align:center"}
How can you get things wrong in *model specificiation*?
:::

:::{.fragment style="text-align:center; margin-top: 2em; margin-bottom: 1em"}

Like...many ways!

:::


:::: {.columns}

::: {.column .fragment width="40%"}
**[Measurement]{.red} Issues**

- Proxy variable
- Measurement error
- Variable type
:::

::: {.column .fragment width="30%"}
**[Specification]{.red} Issues**

- Functional form
- Omitted variable
:::

::: {.column .fragment width="30%"}
**[Interpretation]{.red} Issues**

- Nonlinear model
:::

::::


# Measurement Issues

## Proxy Variable

True model: Y = &beta;<sub>0</sub>  + &beta;<sub>1</sub>[X<sub>1</sub>]{.red} + &beta;<sub>2</sub>[X<sub>2</sub>]{.red} + &beta;<sub>3</sub>X<sub>3</sub><sup>*</sup> + [&epsilon;]{.red}, &epsilon; ~ &Phi;(0, &sigma;<sub>&epsilon;</sub><sup>2</sup>).

- Unobservable: X<sub>3</sub><sup>*</sup>.
    - Observable X<sub>3</sub><sup>*</sup> = &delta;<sub>0</sub> + &delta;<sub>1</sub>[X<sub>3</sub>]{.blue} + [&nu;]{.blue}, &nu; ~ &Phi;(0, &sigma;<sub>&nu;</sub><sup>2</sup>).
- &rArr; Y = (&beta;<sub>0</sub> + &beta;<sub>3</sub>&delta;<sub>0</sub>) + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + &beta;<sub>3</sub>&delta;<sub>1</sub>X<sub>3</sub> + (&epsilon; + &beta;<sub>3</sub>&nu;).
    - *Assuming* cov(&epsilon;, &nu;) = 0, then
        1. E(&epsilon; + &beta;<sub>3</sub>&nu;|X) = 0;
        1. var(&epsilon; + &beta;<sub>3</sub>&nu;|X) = &beta;<sub>3</sub><sup>2</sup>&sigma;<sub>&nu;</sub><sup>2</sup> + &sigma;<sub>&epsilon;</sub><sup>2</sup>.

:::{.notes}
cov(u,v) = 0: u, v independent
:::

:::{.fragment}
:::{.callout-important}
Only when cov([&epsilon;]{.red}, [X<sub>3</sub>]{.blue}) = cov([&nu;]{.blue}, [X<sub>1</sub>]{.red}) = cov([&nu;]{.blue}, [X<sub>2</sub>]{.red}) = 0, can the model produce *unbiased* estimates.
:::

:::

## Measurement Error

- Nonrandom error &rarr; bias
- Random error in Y: Y = Y<sup>*</sup> + &epsilon;.
    - \begin{align}
&Y^* = \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + \epsilon;\\
&Y = \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + (\nu + \epsilon)
\end{align}
- *Consequence*
    + &beta; Unbiased 
    + var(&beta;) &uarr;

## Measurement Error

- Random error in X: X = X<sup>*</sup> + &nu;. If E(&nu;X) = 0, then \begin{align}
Y =& \beta_0 + \beta_1(X_1 - \nu) + \epsilon,\\
  =& \beta_0 + \beta_1X_1 + (\epsilon - \beta_1\nu).
\end{align}
- \begin{align}
cov(\epsilon, X_1) =& E(\epsilon X_1) - E(\epsilon)E(X_1) = E(\epsilon X_1), \\
          =& E[\epsilon (X_1^* + \epsilon)] = E(\epsilon X_1^*) + E(\epsilon^2) = \sigma_\epsilon^2\neq0.\\
cov(\epsilon - \beta_1\nu, X_1) =& cov(-\beta_1\nu, X_1) = -\beta_1cov(\nu, X_1) = -\beta_1\sigma_\nu^2.\\
\Rightarrow\ plim(\hat\beta_1) =& \beta_1 + \frac{cov(\epsilon - \beta_1\nu, X_1)}{var(X_1)},\\
                               =& \beta_1 + \frac{-\beta_1\sigma_\nu^2}{\sigma_{X_1}^2}
                               = \beta_1 + \frac{-\beta_1\sigma_\nu^2}{\sigma_{X_1}^{*2} + \sigma^2_\epsilon} = \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_\nu}\beta_1.
\end{align}

:::{.notes}
plim: probability limit operation, convergence in probability
:::

## Measurement Error

$$plim(\hat\beta_1) = \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_\nu}\beta_1.$$

- Unbiased [only]{.red} when var(X<sub>1</sub>) = var(X<sub>1</sub><sup>\*</sup> + &nu;), i.e., &sigma;<sup>2</sup><sub>X<sub>1</sub></sub> = &sigma;X<sub>1</sub><sup>\*2</sup>+ &sigma;<sub>&nu;</sub><sup>2</sup>
- Consequence: $|\hat\beta_1| < \beta_1$, a.k.a., [attenuation]{.red} bias. 
    + &beta;<sub>1</sub>: underestimated;
    + Affecting the estimations of others in unknown ways .

## Concerns caused by variable type

:::{.callout-tip}
In OLS, there is no requirement (assumption) for Xs to be linear. 
:::

* Indicator variable (binary)
    + $Y = \beta_0 + \beta_1X_i + u_i,$ where X is either male (0) or female(1)
        + &beta;<sub>0</sub>: E(Y|X = male);
        + &beta;<sub>1</sub>: E(Y|X = female) - E(Y|X = male).

* Nominal: 
    + Can't regress unless being broken up into indicators
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.

- How about variable transition, e.g., log(x)?

:::{.notes}
In a regression, the information of the baseline is captured by the intercept
:::


## Logarithm Transformation

What does a [logarithm transformation]{.red} on some variables for? 

:::{.notes}
Some may say making X more linear? However, no assumption on X
:::


:::{.r-stack}

:::{.fragment}
```{r hist}
mammals <- MASS::mammals
mammals_l <- pivot_longer(mammals,
                          cols = everything(),
                          names_to = "variable",
                          values_to = "value")



ggplot(data = mammals_l, aes(x = value, fill = variable)) + 
    geom_histogram(position = "dodge") +
    scale_fill_gb(reverse = TRUE) + 
    theme(legend.position = "top") +
    ggtitle("Brain and Body Weights for 62 Species of Land Mammals")
```
:::

:::{.fragment}
```{r bodyBrain}
ggplot(mammals, aes(body, brain)) + geom_point()
```

:::

:::{.fragment}
```{r bodyLBrain}
ggplot(data = mammals, aes(x = body, y = brain)) +
  geom_point() +
  scale_x_log10() +
  xlab("Log(body)")
```
:::

:::{.fragment}
```{r bodyBrainL}
ggplot(data = mammals, aes(x = body, y = brain)) +
  geom_point() +
  scale_y_log10() +
  ylab("Log(brain)")
```
:::

:::{.fragment}
```{r bodyLBrainL}
ggplot(data = mammals, aes(x = body, y = brain)) +
  geom_point() +
  scale_x_log10() + 
    scale_y_log10() +
  xlab("Log(body)") + ylab("Log(brain)")
```
:::


:::

## Model Fit with Logarithm Transformation

:::{.r-vstack}

:::{.fragment}
```{r fit}
#| fig-height: 3

fit <- vector(mode = "list")
fit$original <- lm(brain ~ body, data = mammals)
fit$log <- lm(log(brain) ~ log(body), data = mammals)

dwplot(fit) +
    scale_color_gb(reverse = TRUE) + 
    theme(legend.position = "top") + 
    ggtitle("Coefficient Estimation")
```
:::

:::{.fragment}
```{r residual}
#| fig-height: 3

df_residual <- map_dfc(fit, ~.$residuals) %>% 
    pivot_longer(cols = everything(), names_to = "model", values_to = "residual")

ggplot(df_residual, aes(x = residual)) +
    geom_density() + 
    facet_wrap(~ model, scales = "free") + 
    ggtitle("Model Residuals")
```
:::

:::


## When Using Logrithm Transformation

Problem to solve: Assumption violation (linearity)

:::{.fragment}
Critical criterion: Skewness
:::


:::{.fragment}
Rule of thumb:

- |Skewness| < 0.5, the distribution is approximately symmetric (0 &hArr; perfect symmetric)
- |Skewness| < 1, moderately skewed
- [|Skewness| > 1, highly skewed]{.red}
:::


:::{.fragment}
Example data:

```{r skew}
map(mammals, e1071::skewness)
```
:::


# Misspecification

## Problem Types

1. Functional form
1. Omitted variable

## Functional Form

Q: When there are two ways to specify the model:

\begin{align}
Y =& f_1(X_1, X_2, X_3) + u =\beta_1X_1 + \beta_2X_2 + \beta_3X_3 + u \\
Y =& f_2(X_1, X_2, X_3) + u =\beta_4X_1^2 + \beta_5X_2^3 + \beta_6X_3^4 + u
\end{align}

Which specification is [correct]{.red}?

:::: {.columns}

::: {.column width="50%"}

:::{.fragment}
**How to figure out?**

Test &beta;<sub>1</sub> in Y = f<sub>1</sub>(X) + f<sub>2</sub>(X) + u?

Probably not. [Hint: How many [true]{.red} models are there?]{.small}
:::

:::{.notes}
Only one true model 
:::

:::{.fragment}
Let's assume f<sub>1</sub> is the true model:

$$var(\hat\beta_1) = \frac{\sigma^2}{(X_1 - \bar X_1)^2(1 - \rho_{12})}.$$

&rho;<sub>12</sub>: Correlations between X<sub>1</sub> and X<sub>2</sub>, such as between X and X<sup>2</sup>. So, when &rho;<sub>12</sub> &uarr;......
:::

:::{.notes}
Variance is inflated when &rho;<sub>12</sub> &uarr;
:::
:::

::: {.column .fragment width="50%"}
**Joint test**

H<sub>0</sub>: &beta;<sub>1</sub> = &beta;<sub>2</sub> = &beta;<sub>3</sub> = 0 or 

&beta;<sub>4</sub> = &beta;<sub>5</sub> = &beta;<sub>6</sub> = 0

Statistics: Davidson-Mackinnon test

1. Run $Y = f_2(X) + u$ and calculate the expected value $\hat Y = E[Y|f_2(X)]$
1. Run $Y = f_1(X) + \theta\hat Y + u$, and test if &theta; = 0.
:::

::::


## Omitted Variable{.smaller}

\begin{align}
True: Y_i =& \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i\\
Specified: Y_i =& \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde u_i
\end{align}

How does $\tilde\beta_1$ compare to $\beta_1$?

:::{.fragment}
\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + u_i,\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (u_i - \bar u),\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (u_i - \bar u)(X_1 - \bar X_1),\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}.\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},\\
\Rightarrow E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1],\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(u_i|X_1),\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1.
\end{align}

:::


## Biased or Not

$E(\tilde\beta_1|X_1) = \hat\beta_1 + \hat\beta_2\delta_1,$ in which $\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$ ( $X_1 = \delta_0 + \delta_1X_2 + r$).

- $\tilde\beta_1$: [Biased]{.red}, unless X<sub>2</sub> is an irrelevant variable.
    - Even if &delta;<sub>1</sub> = 0, $X_1 = \delta_0 + r$, the model may increase the risk of Type I error

:::{.notes}
False positive
:::


:::{.notes}
How About the Residual?

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon,$$

$$\epsilon = \beta_2X_{2} + u_i.$$


$E(\epsilon|X_1) = E(\beta_2X_{2} + u_i) = \beta\mu_2\neq0$

+ Against the assumption.
  

$var(\epsilon|X_1) = var(\beta_2X_{2} + u_i|X_1)= \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2$

+ All values are fixed, that is, still homoscedasitic.  

$E(X_1\epsilon) = \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}],$ 

+ When cov(X<sub>1</sub>, X<sub>2</sub>)&ne;0 &rArr; Increase the difficulty to isolate X from u.
    
$E(\epsilon_i\epsilon_j|X_i)\neq 0$

\begin{align}
E(\epsilon_i\epsilon_j) - E(\epsilon_i)E(\epsilon_j) =& E[(\beta_2X_{2i} + u_i)(\beta_2X_{2j} + u_j)] - \beta_2^2\mu^2_{X_2},\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + u_iu_j) - \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2E(X_{2i}X_{2j}) - \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}


\begin{align}
E(X_1\epsilon) =& E[X_1(\beta_2X_2 + u)], \\
=& E(X_1\beta_2X_2) + E(X_1u),\\
=& \beta_2E(X_1X_2),\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}].
\end{align}

+ When cov(X<sub>1</sub>, X<sub>2</sub>) &ne; 0, E(X<sub>1</sub>&epsilon;) &ne; 0.

:::

# Misinterpretation

## Nonlinear Modeling

![](http://103.238.162.29:10002/webdav/sammo3182/figure/nonlinearity.jpg){fig-align="center" height=300}

:::{.fragment}
```{r nonlinear}
#| layout-ncol: 2
#| layout-align: center

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u)) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )
```
:::


:::{notes}

[Unbiased & efficient]{.red} linear estimates, 

but what does &beta;<sub>1</sub> mean? 
:::

## Marginal Effects

Discrete:

$$Pr(Y|x = X_{n + 1}) - Pr(Y|x = X_n)$$

Continuous: 

$$\lim_{\Delta x\to0} \frac{ f(x + \Delta x) - f(x)}{\Delta x}$$

:::{.fragment}
That's to say, every value in X (n > 1) has a marginal effect (&Delta;<sub>x</sub>). Which one should we use?
:::

## Marginal Effects in Types

:::: {.columns}

::: {.column .fragment width="30%"}
*Average Marginal Effect (AME)*

1. Calculate the marginal effect of each variable x for [each observation]{.red}.
1. Calculate the average.
:::

::: {.column .fragment width="30%"}
*Marginal Effect at the Mean (MEM)*

Calculate the marginal effect of each variable x for [each's mean value]{.red}.

:::

::: {.column .fragment width="40%"}

*Marginal Effect at Representative Values (MER)*

Calculate the marginal effect of each variable x for [value(s) of interest]{.red}.

![](http://103.238.162.29:10002/webdav/sammo3182/figure/spe_interaction2.jpg){fig-align="center" height=350}

:::

::::


## Hypothesis Testing of A Nonlinear Model {auto-animate=true}

$$e.g., Y = \beta_0 + \beta_1X + \beta_2X^2 + u.$$

- Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$
- Let &alpha; = 0.05, H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
    - The average acceleration is zero

:::{.fragment}
Statistics: 

\begin{align}
\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim& t_{n - 3}.\\
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)}, \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}.
\end{align}
:::



## Interpretation {auto-animate=true}

Statistics: 

\begin{align}
\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim& t_{n - 3}.\\
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)}, \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}.
\end{align}

:::{.fragment}

:::{.callout-warning}
Can't simply say whether the null hypothesis is rejected, because it may [not be a coherent conclusion]{.red} in the entire domain of X, due to the nonlinearity. 
:::

:::

- "In the range from a to b, the hypothesis can be rejected."
    * Max - min/First difference
    * Marginal effects across values
    * Just plot it. (Do this, cool kids!)

## Take-home point

![](http://103.238.162.29:10002/webdav/sammo3182/figure/spe_mindmap.png){.r-stretch}

