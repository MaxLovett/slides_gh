<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Simple OLS and Properties of Estimators</title>
    <meta charset="utf-8" />
    <meta name="author" content="Yue Hu" />
    <script src="08_simpleOLS_files/header-attrs-2.11/header-attrs.js"></script>
    <link href="08_simpleOLS_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="08_simpleOLS_files/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="08_simpleOLS_files/tile-view-0.2.6/tile-view.js"></script>
    <link href="08_simpleOLS_files/tachyons-4.12.0/tachyons.min.css" rel="stylesheet" />
    <script src="08_simpleOLS_files/xaringanExtra_fit-screen-0.2.6/fit-screen.js"></script>
    <link href="08_simpleOLS_files/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="08_simpleOLS_files/panelset-0.2.6/panelset.js"></script>
    <script src="08_simpleOLS_files/js-cookie-3.0.0/js.cookie.js"></script>
    <script src="08_simpleOLS_files/peerjs-1.3.1/peerjs.min.js"></script>
    <script src="08_simpleOLS_files/tiny.toast-1.0.0/toast.min.js"></script>
    <link href="08_simpleOLS_files/xaringanExtra-broadcast-0.2.6/broadcast.css" rel="stylesheet" />
    <script src="08_simpleOLS_files/xaringanExtra-broadcast-0.2.6/broadcast.js"></script>
    <link href="08_simpleOLS_files/xaringanExtra-extra-styles-0.2.6/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="zh-CN_custom.css" type="text/css" />
    <link rel="stylesheet" href="style_ui.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Simple OLS and Properties of Estimators
## Large N &amp; Leeuwenhoek (70700173)
### Yue Hu

---




class: inverse, bottom

A Methodologist View
# Understanding OLS

---

## Overview

1. Road to OLS (Old stuff)
1. OLS Principles
    + Goal
    + Model
    + Uncertainty
1. OLS Components
    + Coefficients
    + Variances/SEs


---

## The Path

Genre: Hypothesis testing
+ Uncertainty: Confidence intervals
+ Statistical significance

.pull-left[
*Path We've Passed*

One variable
+ Probability theory
+ Distribution

Variable comparison
+ Difference of means (t score)
+ Cov &amp; cor (r/&amp;tau;/&amp;rho;, &amp;chi;&lt;sup&gt;2&lt;/sup&gt;/&amp;Phi;/V)
+ ANOVA (F)
]


.pull-right[
*Path Foward*

+ Single explanatory variable (.red[OLS])
+ Multiple explanatory variable (.red[Multiple OLS])
+ Nonlinear effect (.red[Med/Mod])
+ Generalized linear (.red[MLE])
]


---

## Review: Expectation and Variance

.pull-left[
`\begin{align}
E(X)_{discrete} =&amp; \sum^n_{i = 1} X_if(X_i)\\
E(X)_{continuous} =&amp; \int^{+\infty}_{-\infty}X_if(X_i)dX.\\
\sigma^2 =&amp; E[X - E(X)]^2 
\end{align}`
]

.pull-right[
`\begin{align}
\sigma^2 =&amp; E[X - E(X)]^2\\
=&amp; E[(X - \mu)^2]\\
              =&amp; E(X^2 - 2X\mu + \mu^2)\\
              =&amp; E(X^2) - 2\mu E(X) + \mu^2\\
              =&amp; E(X^2) - \mu^2
\end{align}`
]

---

class: small

## Review: Rule of Variance

1. var(aX + b) = a&lt;sup&gt;2&lt;/sup&gt;var(X);

`\begin{align}
var(aX + b) =&amp; E[(aX + b) - E(aX + b)]^2\\
            =&amp; E[aX + b - aE(X) - b]^2\\
            =&amp; E[aX - aE(X)]^2 = a^2E[X - E(X)]^2 = a^2var(X)
\end{align}`

--

1. var(aX&lt;sub&gt;1&lt;/sub&gt; + bX&lt;sub&gt;2&lt;/sub&gt; + c) = a&lt;sup&gt;2&lt;/sup&gt;var(X&lt;sub&gt;1&lt;/sub&gt;) + b&lt;sup&gt;2&lt;/sup&gt;var(X&lt;sub&gt;2&lt;/sub&gt;) + 2ab&amp;middot;cov(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;).

--

1. When X&lt;sub&gt;1&lt;/sub&gt; is i.i.d., 
var(a&lt;sub&gt;1&lt;/sub&gt;X&lt;sub&gt;1&lt;/sub&gt; + a&lt;sub&gt;2&lt;/sub&gt;X&lt;sub&gt;2&lt;/sub&gt; + ... + a&lt;sub&gt;n&lt;/sub&gt;X&lt;sub&gt;n&lt;/sub&gt;) = var(&amp;sum;a&lt;sub&gt;i&lt;/sub&gt;X&lt;sub&gt;i&lt;/sub&gt;) = &amp;sum;a&lt;sub&gt;i&lt;/sub&gt;^2var(X&lt;sub&gt;i&lt;/sub&gt;)

--

1. `\(var(\bar X) = var(\frac{\sum X_i}{n}) = \sum^n_{i = 1}\frac{var(X_i)}{n^2} = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}\)`

--

1. cov(X, Y) = E(XY) - E(X)E(Y)
    + When X = Y, cov(X, X) = E(X&lt;sup&gt;2&lt;/sup&gt;) - [E(X)]&lt;sup&gt;2&lt;/sup&gt; = var(X)

`\begin{align}
cov(X, Y) =&amp; E[X - E(X)][Y - E(Y)]\\
          =&amp; E[XY - XE(Y) - YE(X) + E(X)E(Y)]\\
          =&amp; E(XY) - E(Y)E(X) - E(X)E(Y) + E(X)E(Y)\\
          =&amp; E(XY) - E(X)E(Y)
\end{align}`

---

## Review: Property of Estimator `\(\hat \theta\)`

* Unbiased: On average, the estimator gives the right answer, formally, `\(E(\hat\sigma) = \sigma.\)`

--

* Consistent: As the sample size increases, the variance decreases.

&lt;img src="08_simpleOLS_files/figure-html/consistency-1.png" style="display: block; margin: auto;" /&gt;

???

finite sample property    
large sample property

Consistency: n1 &lt; n2 &lt; n3

---

* Efficiency: Smallest variance among unbiased estimators

&lt;img src="08_simpleOLS_files/figure-html/efficiency-1.png" style="display: block; margin: auto;" /&gt;

--

Sometimes, one may want to trade off a little bias against variance.

+ RMES: Root mean square error, `\(\sqrt{bias^2 + var}\)`


---

## Ordinary Least Squares (OLS)

One type of .red[simulation] of the reality among many others, unnecessarily the best one.


.center[General Guide of Usage]

.bg-black.golden.ba.shadow-5.ph4.mt3[
Don't let the data drive you, you .red[drive the data] with appropriate methods.
]

.center[
.large[&amp;dArr;]

Always getting attention to the .red[detailed] data.
]



---

## Regression in Social Science

How does some outcome variable Y change .navy[when] some explanatory variable X changes? 

--

E.g., are the political ideology of different partisan voters different? 

.center[H&lt;sub&gt;0&lt;/sub&gt;: &amp;mu;&lt;sub&gt;D&lt;/sub&gt; = &amp;mu;&lt;sub&gt;R&lt;/sub&gt;]

`$$\text{diff} = \frac{\bar X_R - \bar X_D}{SE}.$$`

???

&lt;img src="images/srf_illustrateSimpleOLS.png" height = 200 /&gt;

---

## Linearity

A crucial yet strong assumption

Y changes .red[linearly] with X. In terms of the population regression function, formally,
.center[Y&lt;sub&gt;i&lt;/sub&gt; = &amp;beta;&lt;sub&gt;0&lt;/sub&gt; + &amp;beta;&lt;sub&gt;1&lt;/sub&gt;X&lt;sub&gt;1&lt;/sub&gt; + u&lt;sub&gt;i&lt;/sub&gt;]


&amp;zwj;Q: Which one shows a linear relationship between X and Y?

`\begin{align}
Y_i =&amp; \beta_0 + \beta_1X_i + ui;\\
Y_i =&amp; \beta_0 + \frac{\beta_1}{X_i} + ui;\\
Y_i =&amp; \beta_0 + \beta_1ln(X_i) + ui;\\
Y_i =&amp; \beta_0 + X_i^{\beta_1} + ui;\\
Y_i =&amp; \frac{1}{\beta_0} + \frac{X_i}{\beta_1} + ui.
\end{align}`

???

First three are linear.

The last one isn't linear until redefining.


---

## Uncertainty

.center[
The Signal of Being Scientific    
u&lt;sub&gt;i&lt;/sub&gt;: Things we don't know (or human errors).
]



`\begin{align}
Y_i =&amp; \beta_0 + \beta_1X_1 + u_i.\\
E(Y_i|X_i) =&amp; E(\beta_0 + \beta_1X_1 + u_i|X_i),\\
           =&amp; E(\beta_0|X_i) + E(\beta_1X_1|X_i) + E(u_i|X_i),\\
           =&amp; \beta_0 + \beta_1X_i + E(u_i|X_i).
\end{align}`

???

`\(\beta_1X_i\)` consistent

E(u_i|X_i) assumed 0

--

### .center[Sample Regression Function (SRF)]

`\begin{align}
Y_i =&amp; \hat\beta_0 + \hat\beta_1X_i + \hat u_i,\\
    =&amp; \hat Y_i + \hat u_i.\\
Y_i =&amp; E(Y_i|X_i) + u_i,\\
\Leftrightarrow u_i =&amp; Y_i - E(Y_i|X_i).
\end{align}`



---

## Estimating Goal

.bg-black.golden.ba.shadow-5.ph4.mt3[
Getting the mean right is a good thing.

.tr[
---Frederick J. Boehmke
]
]

.center[
`\(u_i = Y_i - E(Y_i|X_i).\)`

`\(\sum|\hat u_i| = 0\)`    
&amp;dArr;    
`\(\hat u^2 = 0\)`  
&amp;dArr;    
`\(\sum(Y_i - \hat Y_i)^2 = 0,\)`    
(or `\(\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2 = 0.\)`)  
&amp;dArr;    
`\(\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2 = 0.\)` How ?
]

???

Minimizing expected ui

---

class: center, middle

## Minimizing u&lt;sub&gt;i&lt;/sub&gt;

 &amp;hArr; &amp;beta;&lt;sub&gt;0&lt;/sub&gt; and &amp;beta;&lt;sub&gt;1&lt;/sub&gt; that make `\(\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2 = 0\)`


.center[&lt;img src="images/srf_gaoNeng.gif" height="300"/&gt;]


???

1. &amp;beta;&lt;sub&gt;1&lt;/sub&gt;
1. &amp;beta;&lt;sub&gt;0&lt;/sub&gt;


---

class: small

## &amp;beta;&lt;sub&gt;1&lt;/sub&gt;


`\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_1} =&amp; -\sum 2X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0\\
\Leftrightarrow \sum X_iY_i - \sum X_i\hat\beta_0 - \sum\hat\beta_1 X_1^2 =&amp; 0\\
\sum X_iY_i =&amp; \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2\\
\sum X_iY_i - \sum X_i(\bar Y - \hat\beta_1\bar X) - \sum\hat\beta_1 X_1^2 =&amp; 0\\
\sum X_iY_i - \sum X_i\bar Y + \sum X_i\hat\beta_1\bar X - \sum\hat\beta_1 X_1^2 =&amp; 0\\
\sum X_i(Y_i - \bar Y) + \hat\beta_1\sum X_i(\bar X - X_i) =&amp; 0\\
\sum X_i(Y_i - \bar Y) =&amp; \hat\beta_1\sum X_i(X_i - \bar X)
\end{align}`

--

.pull-left[
`\begin{align}
\sum X_i(Y_i - \bar Y) =&amp; \sum X_i(Y_i - \bar Y) - \bar X\sum (Y_i - \bar Y)\\
                       =&amp; \sum X_i(Y_i - \bar Y) - \bar X(\sum Y_i - \sum\bar Y)\\
                       =&amp; \sum X_i(Y_i - \bar Y) - \bar X(n\bar Y - n\bar Y)\\
                       =&amp; (\sum X_i - \bar X)(Y_i - \bar Y) \blacksquare
\end{align}`
]

???

 "Quod Erat Demonstrandum" which loosely translated means "that which was to be demonstrated".

--

.pull-right[
`\begin{align}
Given \sum(X_i - \bar X)(Y_i - \bar Y) =&amp; \sum X_i(Y_i - \bar Y)\\
\sum X_i(Y_i - \bar Y) =&amp; \hat\beta_1\sum X_i(X_i - \bar X)\\
\sum (X_i - \bar X)(Y_i - \bar Y) =&amp; \hat\beta_1\sum (X_i - \bar X)(X_i - \bar X)\\
\hat\beta_1 =&amp; \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}.
\end{align}`
]

---

class: small

## &amp;beta;&lt;sub&gt;0&lt;/sub&gt;

`\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =&amp; -\sum 2(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0\\
\Leftrightarrow\sum Y_i - \sum\hat\beta_0 - \sum\hat\beta_1X_i =&amp; 0\\
\sum Y_i =&amp; \sum\hat\beta_0 + \sum\hat\beta_1X_i\\
                         =&amp; n\hat\beta_0 + \hat\beta_1\sum X_i\\
\hat\beta_0 =&amp; \frac{\sum Y_i}{n} - \hat\beta_1\frac{\sum X_i}{n}\\
                        =&amp; \bar Y - \hat\beta_1\bar X
\end{align}`

--

 Hint: `\(\hat\beta_0, \hat\beta_1\)` are called .red[co]-efficients;

.center["Normal equations"]

`\begin{align}
\sum Y_i =&amp; n\hat\beta_0 + \hat\beta_1\sum X_i\\
\sum X_iY_i =&amp; \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2
\end{align}`

???

&amp;beta;&lt;sub&gt;0&lt;/sub&gt; is efficient (minimizng the u) depending on &amp;beta;&lt;sub&gt;1&lt;/sub&gt;

---


## Coefficient

`\begin{align}
\hat\beta_1 =&amp; \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}\\
        =&amp; \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum (X_i - \bar X)^2}\sqrt{\sum (Y_i - \bar Y)^2}}\cdot\frac{\sqrt{\sum (Y_i - \bar Y)^2}}{\sqrt{\sum (X_i - \bar X)^2}}\\
        =&amp; r_{X, Y}\frac{s_Y}{s_X}
\end{align}`

--

So when the variance of Y(s&lt;sub&gt;Y&lt;/sub&gt;) increases, &amp;beta;&lt;sub&gt;1&lt;/sub&gt; increases.

--

Special case: Standardized X and Y, i.e., `\(s_Y, s_X\)` are 1s, then,

`$$\beta_1 = r_{X, Y}\frac{s_Y}{s_X} = r_{X,Y}.$$`

---

## Linearity (Revisit)

Remember `\(\sum X_i(Y_i - \bar Y) = \sum(X_i - \bar X)(Y_i - \bar Y).\)`
.pull-left[
`\begin{align}
\hat\beta_1 =&amp; \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2},\\
            =&amp; \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)Y_i,\\
            =&amp; \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)(\beta_0 + \beta_1X_i + u_i),\\
            =&amp; \frac{1}{\sum (X_i - \bar X)^2}[\sum (X_i - \bar X)(\beta_0 + \beta_1X_i) + \sum (X_i - \bar X)u_i],\\
            =&amp; \frac{\sum (X_i - \bar X)(\beta_0 + \beta_1X_i)}{\sum (X_i - \bar X)^2} + \frac{\sum (X_i - \bar X)u_i}{\sum (X_i - \bar X)^2}.
\end{align}`
]

--

.pull-right[
Let `\(k_i=\frac{X_i - \bar X}{\sum (X_i - \bar X)^2},\)` then `\(\hat\beta_1 = \beta_1\sum k_iX_i + \sum k_iu_i\)`. 

+ A .red[linear] combination of errors
+ Min/max(X) influences a lot.
]


---

## Characteristics of Regression Coefficients

1. Calculated using .red[observed] data
1. Unique point estimates
1. SRF passes through `\((\bar X, \bar Y)\)`
1. `\(\bar{\hat Y} (\text{predicted}) = \hat Y (\text{observed})\)`, `\(\frac{\sum\hat Y_i}{n} = \frac{\sum Y_i}{n}\)`
1. `\(\bar{\hat u_i} = \frac{\sum{\hat u_i}}{n} = 0\)`
1. `\(\sum X_i\hat u_i = 0\)`

---

.pull-left[
##  `\(\bar{\hat u_i} = \frac{\sum{\hat u_i}}{n} = 0\)`

`\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =&amp; 0\\ -2\sum[Y_i - (\hat{\beta_0} + \hat{\beta_1}X_i)] =&amp; 0\\
\sum (Y_i-\hat Y_i) =&amp; 0\\
\sum{\hat u_i} =&amp; 0\\
\Rightarrow \frac{\sum{\hat u_i}}{n} = \bar{\hat u_i} =&amp; 0 \blacksquare
\end{align}`
]

--

.pull-right[
## `\(\sum X_i\hat u_i = 0\)`

`\begin{align}
cov(X_i, u_i) =&amp; 0,\\
\frac{\sum(X_i - \bar X)(\hat u_i - \bar{\hat u_i})}{n-1} =&amp; 0,\\
\frac{\sum X_i\hat u_i}{n-1} =&amp; 0. \blacksquare
\end{align}`
]

---

## Uncertainty of OLS Estimators: &amp;beta;&lt;sub&gt;1&lt;/sub&gt;

.pull-left[
`\begin{align}
\sigma^2 =&amp; var(u_i|X),\\
         =&amp; var(Y_i - \hat\beta_0 - \hat\beta_1X_i|X),\\
         =&amp; \frac{\sum(\hat u_i^2)}{n - 2},\\
         =&amp; \hat\sigma^2.
\end{align}`
]


???

n - 2: `\(\hat\beta_0, \hat\beta_1\)`

--

.pull-right[
.small[
`\begin{align}
var(\hat \beta_1|X) =&amp; var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X)\\
                  =&amp; var(\beta_1\sum k_iX_i + \sum k_iu_i|X)\\
                  =&amp; var(\sum k_iu_i|X), \text{given}\ \beta_1\sum k_iX_i \text{constant}\\
                  =&amp; \sum var(k_iu_i|X), \text{assuming}\ u_i\ \text{independent}\\
                  =&amp; \sum k_i^2 var(u_i|X)\\
                  =&amp; \sum[\frac{X_i - \bar X}{\sum (X_i - \bar X)^2}]^2\sigma^2\\
                  =&amp; \frac{\sum(X_i - \bar X)^2}{[\sum (X_i - \bar X)^2]^2}\sigma^2 = \frac{\sigma^2}{\sum (X_i - \bar X)^2}
\end{align}`
]
]

--

If u&lt;sub&gt;i&lt;/sub&gt; is not independent, then cov(k, u) &gt; 0, and this estimator is underestimated.

(That is, the assumption is .large[.red[IMPORTANT]])

---

class: small

## Uncertainty of OLS Estimators: &amp;beta;&lt;sub&gt;0&lt;/sub&gt;

`\begin{align}
var(\hat \beta_0|X) =&amp; var(\bar Y - \hat\beta_1\bar X|X),\\
                   =&amp; var[\frac{\sum(\beta_0 + \beta_1X_i + u_i)}{n} - \hat\beta_1\bar X|X],\\
                  =&amp; var(\frac{\sum u_i}{n}|X) + var(\hat\beta_1\bar X|X), \beta_0 + \beta_1X_i\text{constant and independent},\\
                  =&amp; var(\frac{\sum u_i}{n}|X) + var(\hat\beta_1\bar X|X),\\
                  =&amp; \frac{var(\sum u_i|X)}{n^2} + \bar X^2var(\hat\beta_1|X),\\
                  =&amp; \frac{n\sigma^2}{n^2} + \frac{\bar X^2\sigma^2}{\sum (X_i - \bar X)^2},\\
                  =&amp; \sigma^2[\frac{1}{n} + \frac{\bar X^2}{\sum (X_i - \bar X)^2}],\\
                  =&amp; \sigma^2[\frac{\sum (X_i - \bar X)^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}] = \sigma^2[\frac{\sum X_i^2 - \sum\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}],\\
                  =&amp; \sigma^2[\frac{\sum X_i^2 - n\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}]= \sigma^2\frac{\sum X_i^2}{n\sum (X_i - \bar X)^2}.
\end{align}`

---

## Bonus: Theory

General statement to describe .red[causality] and relevant phenomena.

.center[&lt;img src = "images/srf_conspiracy.jpg" height = 300&gt;]

???

内华达51区（《美恐》S10 双面，怪诞小镇制片Alex Hirsch 新动画Inside job; 

--

.center[
Scientific Theory

+ Logical consistency
+ Empirical .red[falsifiable]
]


---

class: center, middle

## Good Scientific Theory

--

Large explanatory power
+ Importance
+ Range
+ Applicability

???

+ Importance: How much X affects Y
+ Range: generality
+ Applicability: How often affecting in the reality

--

Parsimony

--

Satisfying elaboration

--

Clearly framed (formal?)

--

Explaining important phenomena

--

Prescriptive richness (+policy suggestion?)

---

class: center

## Conspiracy

&lt;img src="images/srf_covid5G.jpg" height = 380 /&gt;

What makes a conspiracy theory NOT a good scientific theory?

???

Rothschild family

---

class: small

## How to Test a Theory

| .small[Necessity\\Sufficiency]&lt;sup&gt;&amp;midast;&lt;/sup&gt; | Yes             | No                |
|-----------------------|-----------------|-------------------|
| Yes                   | Double-Decisive | Smoking Gun       |
| No                    | Hoop            | Straw-in-the-Wind |

.footnote[\* Evera, Stephen Van. 1997. Guide to Methods for Students of Political Science. Ithaca: Cornell University Press.]

???

1. Hoop test: the accused in the state when murder happens? If not innocent, but passing does not mean being guilty.
1. Smoking-gun test: a smoking gun seen in a suspect's hand moments after a shooting is quite conclusive proof of guilt, but a suspect not seen with a smoking gun is not proven innocent.
1. Double-decisive test: hoop + smoking-gun, like a camera record
1. Straw-in-the-wind test: Richard Liu seemed friendly with the girl.

---

## Wrap Up

.pull-left[
+ OLS: A simulation
+ Regression: Get the mean right
+ Linearity: Related as a line

+ Population Regression function: 

Y&lt;sub&gt;i&lt;/sub&gt; = &amp;beta;&lt;sub&gt;0&lt;/sub&gt; + &amp;beta;&lt;sub&gt;1&lt;/sub&gt;X&lt;sub&gt;1&lt;/sub&gt; + u&lt;sub&gt;i&lt;/sub&gt;

+ Sample Regression Function

`\(Y_i = \hat\beta_0 + \hat\beta_1X_i + \hat u_i\)`
]

--

.pull-right[
`\begin{align}
\hat\beta_1 =&amp; \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}\\
=&amp; r_{X, Y}\frac{s_Y}{s_X}\\
\hat\beta_0 =&amp; \bar Y - \hat\beta_1\bar X\\
var(\hat \beta_1|X) =&amp; \frac{\sigma^2}{\sum (X_i - \bar X)^2}\\
var(\hat \beta_0|X) =&amp; \sigma^2\frac{\sum X_i^2}{n\sum (X_i - \bar X)^2}
\end{align}`
]

--

Good theory: Large explanatory power, parsimony, satisfying elaboration, clearly framed, explaining important phenomena, prescriptive richness.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
