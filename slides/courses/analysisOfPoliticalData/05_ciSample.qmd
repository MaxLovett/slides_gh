---
title: "Confidence Intervals and Distribution Comparison"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    number-sections: true
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%

revealjs-plugins:
  - spotlight

# lightbox: auto
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---

```{r setup, include = FALSE}


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)
```

## Overview {.unnumbered}

1. Sample properties
1. Confidence interval
1. Application


:::{.callout-tip .fragment}
## Connection with previous lectures

1. Still one single variable
1. Distribution in the last lecture is the distribution of a *random variable*
    - &rarr; the reference to calculate the confidence intervals
1. Also relating with prediction ("expectation")
:::

# Sample property

## Random Variable

- A random variable is a [mathematical function]{.red} that assigns numerical values to each possible outcome from a random experiment or process in the population.
    - > Technically, a random (stochastic) variable is a type of variable in statistics whose possible values depend on the *outcomes* of a certain random phenomenon.

:::{.callout-note .fragment}
## Different "variables" ![](https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png){height=20}

- Variable in algebra: $Y = log((\sqrt{X})^{\frac{1}{e}}) - 15.$
- Variable in data analysis: "I asked about the respondents' gender, age, and education."

:::{.notes}
Not 1 map 1 or 1 map all, but there is a chance

Not fixed but may change
:::

:::


- Distribution for a random variable: The description of [how likely]{.red} a random variable takes one of its possible states.

## Distribution and random variable

- Remember what's the definition of a probability distribution?
    - The [mathematical function]{.red} that gives the probabilities of occurrence of different possible outcomes for an experiment.

:::{.fragment .r-fit-text style="text-align:center; margin-top: 1em"}
*Which is a function,    
the random variable or the distribution?*
:::

:::{.notes}
- Both
    - Random variable: Outcomes &rarr; values. 
    - Probability distribution: Probabilities &rarr; values.
:::

:::{.fragment style="text-align:right; margin-top: 2em"}
Next step: Random variable (distribution) &rarr; population
:::



## Populaiton {auto-animate=true}

::::{.overlay-container}

:::{.overlay-text}
- A random variable is a [function]{.red} that assigns numerical values to each possible outcome from a random process [in the population]{.green}.
- Population is the [collection]{.red} of [all possible]{.green} outcomes of a random variables
- The probability distribution is a [function]{.red} defining how these outcomes occur/exist in the population.
:::

:::{.overlay-image .fragment data-id="sex"}

```{r sex_population}

# Create datasets for population and sample
df_tsuSex <- data.frame(
  Sex = rep(c("Male", "Female"), 2),
  Count = c(2.42, 1, 7, 9),
  Group = rep(c("Population", "Sample"), each = 2)
) %>% 
  group_by(Group) %>%
  mutate(Percentage = Count / sum(Count) * 100)

df_tsuSex |>
  filter(Group == "Population") |>
  ggplot(aes(x = Sex, y = Percentage)) +
  geom_bar(stat = "identity", alpha = 0.4, fill = gb_cols("gold")) +
  ggtitle("Population Distribution")
```

:::


::::

:::{.notes}
2024

- Official news is "Áï•Ë∂Ö 2:1"https://www.tsinghua.edu.cn/info/1182/113140.htm
- ÁΩëÊòìÔºöÁî∑Â•≥ÊØî‰æãÊòØ2.42Ôºö1Ôºàhttps://m.163.com/dy/article/JAI5076G0549R6EJ.html?clickfrom=subscribe&spss=adap_pcÔºâ(which means about 29% are females)
:::


## Random Sample {auto-animate=true}

A sample: 2024 students in our class

::: {data-id="sex"}

```{r sex_compare}
ggplot(df_tsuSex, aes(x = Sex, y = Percentage, fill = Group)) +
  geom_bar(stat = "identity", position = "identity", alpha = 0.4) +
  labs(title = "Population vs Sample", y = "Percentage") +
  theme(legend.position = c(0.1,0.9)) +
  scale_fill_gb()
```
:::

:::{.fragment style="text-align:center"}
*Is this a good sample?*
:::



## A random sample

> A subset of individuals or observations selected from a larger population in such a way that each observation has an [equal probability]{.red} of being chosen.


:::{style="text-align:center; margin-top: 1em"}
- Why is this good? 
    - Random sample proerties
        - Finite ~ 
        - Large ~
:::


## Finite random sample properties

+ **Unbiasedness**: Produce the right [expectation]{.red} on coverage, E(p) = &pi;
    - Assuming independent and identical distribution (i.i.d)^[*W. or w.o., rep?*]

:::{.notes}
- Unbiasedness: the expectation is the true value
  - Under the assumption that the random variables X1,X2,...,Xn are independently and identically distributed (i.i.d) with mean Œº, each of the terms E(X1),E(X2),...,E(Xn) equals Œº. Sampling with **replacement**
  - $ E(\bar{X}) = E\left(\frac{1}{n}(X_1 + X_2 + ... + X_n)\right) = \frac{1}{n}(E(X_1) + E(X_2) + ... + E(X_n)) = \frac{1}{n}(n\mu) = \mu. $
- How to achieve unbiasedness: 
    1. Design: Randomization(prior), why? no confounders
    1. Post-dgp: weight(post), manually collect, ËÄÅÂ∞ÜÂá∫È©¨
:::

+ **Efficiency**: [Smaller]{.red} variance of an [unbiased]{.green} estimator^[$\text{Var}(aX) = a^2 \cdot \text{Var}(X).$]
    - \begin{align}
      Var(\bar{X}) =& \frac{\sigma^2}{n}, \\
                 =& Var(\frac{1}{n}(X_1 + X_2 + ... + X_n)), \\
                 =& \frac{1}{n^2} \cdot Var(X_1 + X_2 + ... + X_n) = \frac{1}{n^2} \cdot n\sigma^2.
      \end{align}
    - $SE = \frac{\sigma}{\sqrt{n}}$, that is, how far $\bar X$ [disperse]{.red} from &mu;.

:::{.notes}
- [The median will generally be better than the mean if there are heavy tails, while the mean will be best with light tails.](https://stats.stackexchange.com/questions/136671/for-what-symmetric-distributions-is-sample-mean-a-more-efficient-estimator-tha)
- FSM: Flying Spaghetti Monster
:::

:::{.fragment style="text-align:center"}
*How can you increase the efficiency?*
:::


## Large random sample properties

:::: {.columns}

::: {.column .fragment width="50%"}
+ **Convergence**:  $p\lim_{n \to \infty}X_n = a, a\in R.$

![](https://drhuyue.site:10002/sammo3182/figure/ci_convergence.gif){.fragment fig-align="center" height=400}

:::{.notes}
- When a sequence of random variables stabilizes to a certain probabilistic behavior as n &rarr; &infin; , the sequence is said to show stochastic convergence.
- *Two Views of Convergence*
    1. In *probability*: Values in the sequence eventually take a [constant value]{.red} (i.e. the limiting distribution is a point mass).
    1. In *distribution*: Values in the sequence continue to vary, but the variation eventually comes to follow an [unchanging distribution]{.red} (i.e. the limiting distribution is a well characterized distribution)
:::

:::

::: {.column width="50%"}
+ **Consistency**:        
  $p\lim_{n \to \infty}\hat{\theta}_n = \theta.$

![](https://drhuyue.site:10002/sammo3182/figure/ci_consistency.gif){.fragment fig-align="center" height=400}

:::{.notes}
An estimator q<sub>n</sub> is consistent if the sequence q<sub>1</sub>...q<sub>n</sub> converges in probability to the true parameter value &theta; as sample size n grows to infinity
:::

:::

::::

## About Consistency

- [Minimal]{.red} requirement for estimators
- May perform [badly]{.red} in small samples

:::{.fragment}
> Only if a sequence of estimators is [unbiased and converges to a value]{.red}, then it is consistent, as it must converge to the *true* value.
:::


- Does unbiasedness imply consistency?
    - How to estimate the expected height of our class?
- Does consistency imply unbiasedness?
    - ${1 \over n}\sum x_{i}+{1 \over n}$ consistent? unbiased?


:::{.notes}
- e.g., Let's estimate the mean height of our class. To do so, we randomly draw a sample from the student population and measure their height. Then what estimator should we use? (Appendix C of Introductory Econometrics by Jeffrey Wooldridge)
    - Options: Both estimator are **unbiased** (üò±!!!)
        1. The mean height of the sample;
        1. The height of the student we draw first.
    - E(X&#772;) = E(X<sub>1</sub>) = &mu;;
    - Var(X1)=&sigma;<sup>2</sup> forever; var(X&#772;) = &sigma;<sup>2</sup>/n.


Say we want to estimate the mean of a population. While the most used estimator is the average of the sample, another possible estimator is simply the first number drawn from the sample. This estimator is unbiased, because  E(X1)=Œº  due to the random sampling of the first number. 

Yet the estimator is not consistent, because as the sample size increases, the variance of the estimator does not reduce to 0. Rather it stays constant, since Var(X1)=&sigma;2 , which the population variance, again due to the random sampling. 
The additional information of an increasing sample size is simply not accounted for in this estimator.

- ${1 \over n}\sum x_{i}+{1 \over n}$ consistent? unbiased?
    - Consistent: As $n\rightarrow \infty$, the estimator approaches the correct value, so it is consistent.
    - Biased: $E({1 \over n}\sum x_{i}+{1 \over n}) = E({1 \over n}\sum x_{i})+E({1 \over n}) = \mu + {1 \over n}\neq \mu.$
:::



# Confidence Interval

## Learning the population from a sample

![](https://drhuyue.site:10002/sammo3182/figure/ci_ringToss.gif){.fragment fig-align="center" height=200}

- X: a random sample from a probability distribution with parameter &theta;.
- *Confidence interval* (CI) for the parameter &theta;, with confidence level &gamma;: An interval determined by random variables u(X) and v(X) with the property: $Pr\{u(X)<\theta <v(X)\}\ =\ \gamma = 1 - \alpha, \quad \forall\theta.$

- Human language: In a [repeatedly]{.red} sampling, the percentage of the samples that could contain &theta;


## Calculated CI (Two-tailed)^[Explained in the next lecture.]

:::: {.columns}

::: {.column width="60%"}

+ $\bar X \pm Z_{\alpha/2}SE$
    - &alpha;: 1 - Confident level;
    - Z-score: $Z = \frac{X - \mu}{\sigma}$ [*Learned*]{.fragment}
+ Proportion: $\pi = P \pm Z_{\alpha/2}\sqrt{\frac{P(1 - P)}{n}}$

:::{.notes}
s is the sample standard distribution, $SE(\bar{X}) = \sqrt{\frac{s^2}{n}}$
:::

:::{.callout-important .fragment}
## Interpretation

In 100 times sampling of ..., there are ... samples (... of the chance) that the [CI could contain the true value]{.red}.
:::

:::

::: {.column .fragment width="40%"}

*How can you get smaller CI?*

![](https://drhuyue.site:10002/sammo3182/figure/ci_hardCI.jpeg){fig-align="center" height=400}
:::

:::{.notes}

1. Large N
1. Large &alpha;
    - &alpha; = 0.05, Z = `r qnorm(0.975)`
    - &alpha; = 0.1, Z = `r qnorm(0.95)`

:::

::::


## When N is not that large

![](https://drhuyue.site:10002/sammo3182/figure/ci_largeNumber.gif){fig-align="center" height=250}

:::{.fragment}

Solution: A fatter-tailed distribution

```{r zvst}
#| echo: false
#| fig.align: "center"
#| fig.height: 2.5

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, aes(colour = "Normal"), size = 1.5) +
  stat_function(fun = function(x) dt(x, df = 3), aes(colour = "Student's t"), size = 1.5) + 
  ylab("Probability Density") + 
  xlab("") +
  labs(color = "Distribution") +
  scale_color_gb() +
  theme(legend.position = c(0.85, 0.8))
```

:::{.notes}
- Small N, more probability to get dispersed result; more "uncommon" event can happen when the sample is smaller
    - t = (&Phi; / &chi;<sup>2</sup>)^(1/2)
- History: 
    - First derived as a posterior distribution in 1876 by Helmert and L√ºroth.
    - William Sealy Gosset, an English statistician, chemist and brewer, published in English language in *Biometrika* in 1908 using the pseudonym "student." Fisher called it the "student distribution"
:::

:::

## T/Student's distribution ![](https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png){fig-align="center" height=30}

:::{layout-ncol}
![](https://drhuyue.site:10002/sammo3182/figure/ci_student_en.png){.fragment fig-align="center" height=250}

![](https://drhuyue.site:10002/sammo3182/figure/ci_student.png){fig-align="center" height=300}
:::

:::{.notes}
A researcher at Guinness had previously published a paper containing trade secrets of the Guinness brewery. The economic historian Stephen Ziliak discovered in the Guinness Archives that to prevent further disclosure of confidential information, the Guinness Board of Directors allowed its scientists to publish research on condition that they do not mention "1) beer, 2) Guinness, or 3) their own surname".[4] To Ziliak, Gosset seems to have gotten his pen name "Student" from his 1906‚Äì1907 notebook on counting yeast cells with a haemacytometer, "The Student's Science Notebook"[1][5] Thus his most noteworthy achievement is now called Student's, rather than Gosset's, t-distribution and test of statistical significance
:::

## Estimating Method

:::: {.columns}

::: {.column .nonincremental width="50%"}
+ For mean
    + &sigma; known, $\mu = \bar X \pm Z_{\alpha/2}\frac{\sigma}{\sqrt n}$
    + &sigma; unknown
        + N &geq; 100ish, then $\bar X \pm Z_{\alpha/2}\frac{s}{\sqrt n}$;
        + N < 100ish, then $\bar X \pm t_{\alpha/2}\frac{s}{\sqrt n}.$
:::

::: {.column .nonincremental width="50%"}
+ For proportion
    + &pi; known, $\Pi = P \pm Z_{\alpha/2}\sqrt{\frac{\pi(1 - \pi)}{n}}$;
    + &pi; unknown, $\Pi = P \pm t_{\alpha/2}\sqrt{\frac{\pi(1 - \pi)}{n}}$.
:::

::::


- **Degree of Freedom**: Student's T critical points are relative to the d.f.
  + For CI: n - 1; for regression: n - k - 1
- **Procedure**: Define measurement &rarr; set up &alpha; &rarr; calculate the d.f. and z/t scores &rarr; calculate the lower and upper bounds




# Application

## What can CI do

:::{style="text-align:center"}

+ Does an event just happen by chance?
+ Is Sample A different from Sample B?

:::

:::: {.columns .fragment }

::: {.column width="70%"}
üå∞ Your friend, who is  the accountant for "Dr. Hu's Amazing Team," was asked by the organizer to help set a budget for a thank-you lunch after the team‚Äôs successful conference organization. 
The organizer, though unwilling to spend much, didn't want to seem too stingy either. 
After calculating the average cost of lunch for a random sample of [nine]{.red} team members, your friend found it to be ¬•29, with a ¬•3 deviation, and proposed a budget of ¬•31 per person. 
The organizer, however, criticized her, claiming "¬•26 is enough" and accusing her of not understanding statistics.
[*Is the organizer PUA your friend?*]{.fragment}
:::

::: {.column width="30%"}
![Created by [DALL¬∑E.](https://openai.com/dall-e)](https://drhuyue.site:10002/sammo3182/figure/ci_drhu1.png){fig-align="center" height=300}
:::

::::


:::{.notes}
The ruler: Standard Normal distribution (Z-score)
:::


## PUA or not

:::{style="text-align:center"}
N =9, (X&#772; 29, s 3) vs. 26
:::

:::: {.columns}

::: {.column width="70%"}
> PUA = Blaming someone for something they did nothing wrong.

The organizer blamed your friend for a unreasonable budget &rArr; "Unreasonable" means a guess out of the 95% CI of the sample

1. Set &alpha; = 0.05;
1. N = 9, t distribution, d.f. = 9 - 1 = 8;
1. t(&alpha; < 1 - 0.05/2) = `r round(qt(.975, df = 8), digits = 4)` (`qt(.975, df = 8)`).
1. $CI = 29 \pm t_{\frac{0.975}{2}}(\frac{3}{\sqrt{9}})$, i.e, [`r round(29 - qt(.975, df = 8), digits = 0)`, `r round(29 + qt(.975, df = 8), digits = 0)`].
:::

::: {.column width="30%"}
![Created by [DALL¬∑E.](https://openai.com/dall-e) ](https://drhuyue.site:10002/sammo3182/figure/ci_drhu2.png){fig-align="center" height=num}
:::

::::

:::{.fragment style="text-align:center"}
**What's the interpretation?**
:::


:::{.notes}
[Inference]{.blue}: If we make repeated sampling from the lunch expenses, there are 95% of the samples in which the interval between 27.5 and 31.8 contains the true mean. 
Therefore, an estimation of 31 sounds [fine]{.red}.
Your friend was treated unjustly.
:::

## Comparing Samples

Assuming X<sub>1</sub> and X<sub>2</sub> are i.i.d,

* &sigma; is known, $\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm Z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$.
* &sigma; is unknown, $\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm t_{\alpha/2}\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}$
   + $(\bar X_1 - \bar X_2) \pm t_{\alpha/2}S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$ when the population of the samples are identical, i.e., $\sigma_1 = \sigma_2$, where $S_p^2 = \frac{\sum(X_1 - \bar X_1)^2 + \sum(X_2 - \bar X_2)^2}{(n_1 - 1) + (n_2 - 1)}$, d.f.: $(n_1 - 1) + (n_2 - 1)$, [a.k.a., the "Difference in Means".]{.fragment}
- Aggregated (propotional) data: $\Pi_1 - \Pi_2 \pm Z_{\alpha/2}\sqrt{\frac{P_1(1 - P_1)}{n_1} + \frac{P_2(1 - P_2)}{n_2}}$


## üå∞

:::: {.columns}

::: {.column width="60%"}
![Created by [DALL¬∑E.](https://openai.com/dall-e)](https://drhuyue.site:10002/sammo3182/figure/ci_modules.webp){fig-align="center" height=550}
:::

::: {.column width="40%"}
The "Learning R with Dr. Hu and His Friends" workshop includes two series: "R Survivor" (basic) and "R Expert" (advanced). 
The five "R Survivor" workshops received 932, 586, 796, 501, and 351 downloads, while the "R Expert" workshops had 771, 917, 326, 825, and 399 downloads.  
[*Which series is more popular?*]{.fragment}
:::

::::


## Solution


```{r eg-compare}
vec_survivor <- c(932, 586, 796, 501, 351)
vec_expert <- c(771, 917, 326, 825, 399)
```

:::{.nonincremental}
- R Survivor: 932, 586, 796, 501, and 351 downloads
- R Expert: 771, 917, 326, 825, and 399 downloads
:::

Set &alpha; = 0.05; small N &rarr; t.

\begin{align}
\bar X_1 =& (932 + 586 + 796 + 501 + 351)/5 = `r mean(vec_survivor)`; \bar X_2 = `r mean(vec_expert)` \\
S_p^2 =& \frac{`r sum((vec_survivor - mean(vec_survivor))^2)` + `r sum((vec_expert - mean(vec_expert))^2)`}{(5 - 1) + (5 - 1)} = `r (sum((vec_survivor - mean(vec_survivor))^2) + sum((vec_expert - mean(vec_expert))^2)) / (4 + 4)`;\\
\mu_1 - \mu_2 =& (`r mean(vec_survivor)` - `r mean(vec_expert)`) \pm `r round(qt(.975, df = 7), digits = 4)` (\sqrt{`r (sum((vec_survivor - mean(vec_survivor))^2) + sum((vec_expert - mean(vec_expert))^2)) / (4 + 4)`}\sqrt{1/5 + 1/5}), \\
=& `r mean(vec_survivor) - mean(vec_expert)` \pm `r round(qt(.975, df = 8) * sqrt((sum((vec_survivor - mean(vec_survivor))^2) + sum((vec_expert - mean(vec_expert))^2)) / (4 + 4) * 2/5), digits = 4)`.
\end{align}


:::{.fragment}
[Inference:]{.red} If we make repeated sampling from the audience size of these lectures, there are 95% of the samples in which the interval between -378.8763 and 350.0763 contains the true mean. 
*The CI covers 0*. [That is, the difference is [no different]{.red} from zero statistically.]{.fragment}
:::


## When Not IID

:::: {.columns}

::: {.column width="50%" .fragment .fade-in-then-semi-out}
**IID**

![Observations are selected [without regard to]{.red} who is in the other condition, a.k.a., **independent and identical distributed** (IID).](https://drhuyue.site:10002/sammo3182/figure/ci_independentSample.gif){fig-align="center" height=400}



:::

::: {.column .fragment width="50%"}
**Matched**

![Observations are [matched to]{.red} someone in the other condition.](https://drhuyue.site:10002/sammo3182/figure/ci_matchedSample.jpg){fig-align="center" height=400}

:::

::::



## Difference in Difference in Means

:::{.fragment}
*Independent sample*

$$\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm t_{\alpha/2}\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}$$
:::


:::{.fragment}
*Matched samples*

$D = X_1 - X_2$, then $\Delta = \bar D \pm t_{\alpha/2}\frac{S_D}{\sqrt{n}}$, where $S_D = \frac{\sum(D - \bar D)^2}{n - 1}$. 
:::

:::{.fragment}
*Matched proportions (Aggregate data)*

$\hat{P}_D = \frac{n_{10} - n_{01}}{n}$, then $\Delta = \hat{P}_D \pm Z_{\alpha/2} \cdot \sqrt{\frac{n_{10} + n_{01}}{n^2}},$ where $n$ = Number of matched pairs, $n_{10}$ = Number of pairs where the first observation is a success and the second is not, $n_{01}$ = Number of pairs where the first observation is a failure and the second is a success.

:::


:::{.notes}
This formula uses the discordant pairs ($n_{10}$ and $n_{01}$) to calculate the confidence interval for the difference in proportions in matched samples.


Why using a different method for matched samples? Not IID for individuals, but might for the difference
:::

## An example {auto-animate=true}

:::: {.columns}

::: {.column width="40%"}

![](https://drhuyue.site:10002/sammo3182/figure/ci_hsk.png){fig-align="center" height=num}

:::

::: {.column width="60%"}
Four students must achieve HSK-4 to apply to Chinese universities (fantasy). 
For higher-tier universities, they need even higher scores (fictional). 
After reviewing their initial exam results, they underwent preparatory training. 
The tables below show their performance in two subsequent exams.

[*Was the preparatory training helpful?*]{.fragment}


| Student 	| Tim 	| Frank 	| Emily 	| Elise 	|
|---------	|-----	|------	|-------	|------	|
| Before 	| 57  	| 57   	| 73    	| 65   	|
| After 	| 64  	| 66   	| 89    	| 71   	|
:::

::::


## Solution

| Student 	| Tim 	| Frank 	| Emily 	| Elise 	|
|---------	|-----	|------	|-------	|------	|
| Before 	| 57  	| 57   	| 73    	| 65   	|
| After 	| 64  	| 66   	| 89    	| 71   	|

Set &alpha; = 0.05,
\begin{align}
D =& X_1 - X_2 \Rightarrow \bar D = \sum D / n = (7 + 9 + 16 + 6) / 4 = 9.5\\
S_D^2 =& \sum(D_i‚Äã‚àí\bar{D})^2 = 6.25 + 0.25 + 42.25 + 12.25 = 61 / 3 \Rightarrow S_D \approx 4.51 \\
\therefore \Delta =& 9.5 \pm 3.18\times \frac{4.51}{\sqrt{4}} = 9.5 \pm 7.18
\end{align}


:::{.fragment}
[Inference]{.red}: If we make repeated sampling from these students, there are 95% of the samples in which the interval between 2.32 and 16.68 contains the true mean of the difference.
**The CI is above 0, that is, students did get better.**
:::



## Another üå∞

:::: {.columns}

::: {.column width="80%"}
Gallop drew a pair of 1500 samples from the American population. In the sample of 1980, there are 52% Democrats, and 46% in the 1985 sample. Were the Democrats the same for two years, given the 95% CI?

:::{.fragment}
Solution: Let &alpha; = 0.05,

\begin{align}
\Pi_1 - \Pi_2 &= (0.46 - 0.52) \pm 1.96\sqrt{\frac{0.46 * 0.54}{1500} + \frac{0.52 * 0.48}{1500}} \\
&\approx -0.06 \pm 0.036.
\end{align}
:::

:::

::: {.column width="20%"}
![](https://drhuyue.site:10002/sammo3182/figure/ci_gallop.png){fig-align="center" height=200}
:::

::::


:::{.fragment}
[Inference]{.red}:
If we make repeated sampling from the Amercian population, there are 95% of the samples in which the interval between -0.042 and 0.03 contains the true mean.
**The CI contains 0. Thus, the proportion of Democrats in 1980 was not different from that in 1985 statistically at the 0.05 level.**
:::


## BFF: Different Views ![](https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png){height=20}

:::: {.columns}

::: {.column .fragment width="40%"}
*Bayesian*

- [Credible]{.red} interval
- $\theta_\text{prior-based r.v.} \in [a, b]_{fixed}$

![There are ...% of the chance that the true value lies in the CI.](https://drhuyue.site:10002/sammo3182/figure/ci_dart.gif){ fig-align="center" height=300}

:::{.notes}
Some percentage of the [posterior]{.red} distribution lies within an particular region.
:::

:::

::: {.column .nonincremental width="30%"}
*Frequentist*

- [Confidence]{.red} interval
- $\theta_{fixed} \in [a, b]_{r.v.}$ 

![There is 95% chance the CI could contain the true value (before any data is collected).](https://drhuyue.site:10002/sammo3182/figure/ci_ringToss.webp){fig-align="center" height=300}

:::

::: {.column .fragment width="30%"}

*Fiducial*

- (Fiducial) Conf interval
- $\theta_{r.v.} \in [a, b]_{fixed}$

![There is 95% chance the CI could contain the true value ~~(before any data is collected)~~.](https://drhuyue.site:10002/sammo3182/figure/ci_swan.jpg){fig-align="center" height=300}

:::

:::{.notes}
Fisher used it /f…™Ààdu É…ôl/, ÈÇìÁê¨Áíê intro to causal inference, lec 3, https://pro.yuketang.cn/v2/web/v3/playback/734415352261764864/slide/54/0
:::

::::


## Take-home point

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/ci_mindMap.png){fig-align="center" height=500}

![](https://drhuyue.site:10002/sammo3182/figure/ci_mindMap_structure.png){.fragment fig-align="center" height=600}
:::

# Appendix {.unnumbered}

## Stretch {.unnumbered}

{{< video https://drhuyue.site:10002/sammo3182/video/relax.mp4 title="YoyoÂ≠ô‰Ω≥Á•∫:ÊãØÊïë‰πÖÂùêÂÖö‰ΩìÊÄÅÔºÅÂäûÂÖ¨Èó¥ÈöôÁöÑ5ÂàÜÈíüÊãâ‰º∏ÔºåÈáçÊñ∞Êå∫ÊãîËµ∑Êù•ÔΩûÔºÅ" height=600 preload="auto" controls allowfullscreen>}}

:::{.notes}
https://www.bilibili.com/video/BV1E54y1r76o/?buvid=Y045340D2FD9FA3A46419EEFE4578279ECBD&from_spmid=main.space-search.0.0&is_story_h5=false&mid=7RnjBONLRMus4FQZFBWD2g%3D%3D&p=2&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=79FE08C9-F2B1-4C18-A16C-B31179817B42&share_source=WEIXIN&share_tag=s_i&timestamp=1728732278&unique_k=skSWw7j&up_id=390316092&vd_source=f38aeefd0d38cecba9017eeee43e71c8
:::

## Meditation {.unnumbered}

:::{style="text-align:center; margin-top: 2em"}
![ÊùæËå∏ÁöÑ‰∏ñÁïåÔºö5ÂàÜÈíüÊ≠£ÂøµÂÜ•ÊÉ≥-Ëá™‰ø°‰πãÂøÉ](https://drhuyue.site:10002/sammo3182/figure/ci_songrong.jpg){fig-align="center" height=400}

<audio controls preload="auto" src="https://drhuyue.site:10002/sammo3182/video/meditation.m4a" width=900></audio>
:::


:::{.notes}
https://www.xiaoyuzhoufm.com/episode/65b752ed0bef6c207457f51b?s=eyJ1IjogIjYwNDA1OGJiZTBmNWU3MjNiYmY3MjZiZSJ9
:::