---
title: "Confidence Intervals and Distribution Comparison"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    css: https://sammo3182.github.io/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    callout-appearance: simple
      
filters:
  - lightbox

revealjs-plugins:
  - spotlight

# lightbox: auto
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---

## Overview

```{r setup, include = FALSE}


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools
) 


# Functions preload
set.seed(313)

theme_set(theme_minimal())
```


1. Sample properties
1. Confidence interval
1. Application


# Sample property

## Random Variable (Formally Introduced)

> A random variable (stochastic variable) is a type of variable in statistics whose possible values depend on the outcomes of a certain random phenomenon. 

- A [mapping or function]{.red} from possible outcomes in a sample space to a measurable space, often the real numbers.

:::{.fragment}
:::{.callout-tip}
- Different from variable in algebra: $Y = log((\sqrt{X})^{\frac{1}{e}}) - 15.$
- Different from variable in data collection: "I asked about the respondents' gender, age, and education."

:::{.notes}
Not 1 map 1 or 1 map all, but there is a chance

Not fixed but may change
:::

:::
:::

- Distribution for a random variable
    - [The math function that gives the probabilities of occurrence of different possible outcomes for an experiment.]{.gray .small}
    - The description of [how likely]{.red} a random variable takes one of its possible states.

## Explore random variable


:::{style="text-align:center"}
  Parameter &lArr; sample
:::

Sample Properties

::: {.panel-tabset}
### Finite random sample

+ **Unbiasedness**: Produce the [right]{.red} answer on coverage, E(p) = &pi;
    - Assuming independent and identical distribution (i.i.d) [w. or w.o., rep?]{.small .fragment}

:::{.notes}
- Unbiasedness: the expectation is the true value
  - Under the assumption that the random variables X1,X2,...,Xn are independently and identically distributed (i.i.d) with mean Œº, each of the terms E(X1),E(X2),...,E(Xn) equals Œº. Sampling with **replacement**
  - $ E(\bar{X}) = E\left(\frac{1}{n}(X_1 + X_2 + ... + X_n)\right) = \frac{1}{n}(E(X_1) + E(X_2) + ... + E(X_n)) = \frac{1}{n}(n\mu) = \mu. $
- How to achieve unbiasedness: 
    1. Design: Randomization(prior), why? no confounders
    1. Post-dgp: weight(post), manually collect, ËÄÅÂ∞ÜÂá∫È©¨
:::

+ **Efficiency**: [Smaller]{.red} variance of an unbiased estimator
    - Focusing on the variance of an **unbiased** estimator
        - [$Var(\bar{X}) = Var(\frac{1}{n}(X_1 + X_2 + ... + X_n)) = \frac{1}{n^2} \cdot Var(X_1 + X_2 + ... + X_n) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$]{.small}^[$\text{Var}(aX) = a^2 \cdot \text{Var}(X).$]
        - $SE = \frac{\sigma}{\sqrt{n}}$, how far $\bar X$ [disperse]{.red} from &mu;.

:::{.notes}
- [The median will generally be better than the mean if there are heavy tails, while the mean will be best with light tails.](https://stats.stackexchange.com/questions/136671/for-what-symmetric-distributions-is-sample-mean-a-more-efficient-estimator-tha)
- FSM: Flying Spaghetti Monster
:::

:::{.fragment style="text-align:center"}
*How can you increase the efficiency?*
:::



### Large random sample

:::: {.columns}

::: {.column .fragment width="50%"}
+ **Convergence**:  $p\lim_{n \to \infty}X_n = a, a\in R.$

![](images/ci_convergence.gif){.fragment fig-align="center" height=250}

:::{.notes}
- When a sequence of random variables stabilizes to a certain probabilistic behavior as n &rarr; &infin; , the sequence is said to show stochastic convergence.
- *Two Views of Convergence*
    1. In *probability*: Values in the sequence eventually take a [constant value]{.red} (i.e. the limiting distribution is a point mass).
    1. In *distribution*: Values in the sequence continue to vary, but the variation eventually comes to follow an [unchanging distribution]{.red} (i.e. the limiting distribution is a well characterized distribution)
:::

:::

::: {.column width="50%"}
+ **Consistency**: $p\lim_{n \to \infty}\hat{\theta}_n = \theta.$

![](images/ci_consistency.gif){.fragment fig-align="center" height=250}

:::{.notes}
An estimator q<sub>n</sub> is consistent if the sequence q<sub>1</sub>...q<sub>n</sub> converges in probability to the true parameter value &theta; as sample size n grows to infinity
:::

:::

::::

:::

## About Consistency

- [Minimal]{.red} requirement for estimators
- May perform [badly]{.red} in small samples

:::{.fragment}
> Only if a sequence of estimators is [unbiased and converges to a value]{.red}, then it is consistent, as it must converge to the correct value.
:::


- Does unbiasedness imply consistency?
    - How to estimate the expected height of our class?
- Does consistency imply unbiasedness?
    - ${1 \over n}\sum x_{i}+{1 \over n}$ consistent? unbiased?


:::{.notes}
- e.g., Let's estimate the mean height of our university. To do so, we randomly draw a sample from the student population and measure their height. Then what estimator should we use? (Appendix C of Introductory Econometrics by Jeffrey Wooldridge)
    - Options: Both estimator are **unbiased** (üò±!!!)
        1. The mean height of the sample;
        1. The height of the student we draw first.
    - E(X&#772;) = E(X<sub>1</sub>) = &mu;;
    - Var(X1)=&sigma;<sup>2</sup> forever; var(X&#772;) = &sigma;<sup>2</sup>/n.


Say we want to estimate the mean of a population. While the most used estimator is the average of the sample, another possible estimator is simply the first number drawn from the sample. This estimator is unbiased, because  E(X1)=Œº  due to the random sampling of the first number. 

Yet the estimator is not consistent, because as the sample size increases, the variance of the estimator does not reduce to 0. Rather it stays constant, since Var(X1)=&sigma;2 , which the population variance, again due to the random sampling. 
The additional information of an increasing sample size is simply not accounted for in this estimator.

- ${1 \over n}\sum x_{i}+{1 \over n}$ consistent? unbiased?
    - Consistent: As $n\rightarrow \infty$, the estimator approaches the correct value, so it is consistent.
    - Biased: $E({1 \over n}\sum x_{i}+{1 \over n}) = E({1 \over n}\sum x_{i})+E({1 \over n}) = \mu + {1 \over n}\neq \mu.$
:::



# Confidence Interval

## Learning the population from the sample

:::{.fragment .fade-semi-out}

[Let X be a random sample from a probability distribution with parameter &theta;. 
A [confidence interval]{.red} for the parameter &theta;, with confidence level &gamma; is an interval determined by random variables u(X) and v(X) with the property: ]{.small}

$$Pr\{u(X)<\theta <v(X)\}\ =\ \gamma = 1 - \alpha, \quad {\text{ for every }}\theta.$$

:::

![](images/ci_ringToss.gif){.fragment fig-align="center" height=300}

- In a [repeatedly]{.red} sampling, the percentage of the samples that could contain &theta;


## Toss the Ring (Two-tailed)^[Explained in the next lecture.]

:::: {.columns}

::: {.column width="60%"}
+ Mean: $\mu = \bar X \pm Z_{\alpha/2}SE$
    - &alpha;: 1 - Confident level;
    - Z-score: $Z = \frac{X - \mu}{\sigma}$
    
:::{.notes}
s is the sample standard distribution, $SE(\bar{X}) = \sqrt{\frac{s^2}{n}}$
:::
    
:::

::: {.column width="40%"}
+ Proportion: $\pi = P \pm Z_{\alpha/2}\sqrt{\frac{P(1 - P)}{n}}$
:::

::::


:::{.fragment}
:::{.callout-important}
In 100 times sampling of ..., there are ... samples (... of the chance) that the CI could contain the true value.
:::
:::

:::{.fragment style="text-align:center; margin-top: 2em"}
*How can you get smaller CI?*

![](images/ci_hardCI.jpeg){fig-align="center" height=200}
:::

:::{.notes}
1. Large N
1. Large &alpha;
    - &alpha; = 0.05, Z = `r qnorm(0.975)`
    - &alpha; = 0.1, Z = `r qnorm(0.95)`
:::

## When N is not that large

![](images/ci_largeNumber.gif){fig-align="center" height=250}

:::{.fragment}

Solution: A fatter-tailed distribution

```{r zvst}
#| echo: false
#| fig.align: "center"
#| fig.height: 2.5

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, aes(colour = "Normal"), size = 1.5) +
  stat_function(fun = function(x) dt(x, df = 3), aes(colour = "Student's t"), size = 1.5) + 
  ylab("Probability Density") + 
  xlab("") +
  labs(color = "Distribution") +
  scale_color_gb() +
  theme(legend.position = c(0.85, 0.8))
```

:::{.notes}
- Small N, more probability to get dispersed result; more "uncommon" event can happen when the sample is smaller
    - t = (&Phi; / &chi;<sup>2</sup>)^(1/2)
- History: 
    - First derived as a posterior distribution in 1876 by Helmert and L√ºroth.
    - William Sealy Gosset, an English statistician, chemist and brewer, published in English language in *Biometrika* in 1908 using the pseudonym "student." Fisher called it the "student distribution"
:::

:::

## T/Student's distribution ![](images/ci_fsmrof.png){fig-align="center" height=30}

:::{.r-vstack}
![](images/ci_student.png){fig-align="center" height=300}

![](images/ci_student_en.png){.fragment fig-align="center" height=300}
:::

:::{.notes}
A researcher at Guinness had previously published a paper containing trade secrets of the Guinness brewery. The economic historian Stephen Ziliak discovered in the Guinness Archives that to prevent further disclosure of confidential information, the Guinness Board of Directors allowed its scientists to publish research on condition that they do not mention "1) beer, 2) Guinness, or 3) their own surname".[4] To Ziliak, Gosset seems to have gotten his pen name "Student" from his 1906‚Äì1907 notebook on counting yeast cells with a haemacytometer, "The Student's Science Notebook"[1][5] Thus his most noteworthy achievement is now called Student's, rather than Gosset's, t-distribution and test of statistical significance
:::

## Estimating Method

:::: {.columns}

::: {.column width="50%"}
+ For mean
    + &sigma; known, $\mu = \bar X \pm Z_{\alpha/2}\frac{\sigma}{\sqrt n}$
    + &sigma; unknown
        + N &geq; 100ish, then $\bar X \pm Z_{\alpha/2}\frac{s}{\sqrt n}$;
        + N < 100ish, then $\bar X \pm t_{\alpha/2}\frac{s}{\sqrt n}.$
:::

::: {.column width="50%"}
+ For proportion
    + &pi; known, $\Pi = P \pm Z_{\alpha/2}\sqrt{\frac{\pi(1 - \pi)}{n}}$;
    + &pi; unknown, $\Pi = P \pm t_{\alpha/2}\sqrt{\frac{\pi(1 - \pi)}{n}}$.
:::

::::


- *Degree of Freedom*: Student's T critical points are relative to the d.f.
  + For CI: n - 1
  + For regression: n - k - 1
- *Procedure*: Define measurement &rarr; set up &alpha; &rarr; calculate the d.f. and z/t scores &rarr; calculate the lower and upper bounds




# Application

## What can CIs do

:::{style="text-align:center"}

+ Is a sampled a random result (or not)?
+ Is Sample A different from Sample B?

:::

![](images/ci_gpa.png){.fragment fig-align="center" height=400}

:::{.notes}
The ruler: Standard Normal distribution (Z-score)
:::

## Application I: Random or Not

:::: {.columns}

::: {.column width="50%"}
[
Your friend is a team member and also the accountant for "Dr. Hu's Amazing Team."
The team organizer planned a lunch to express appreciation for the members' outstanding work in organizing a recent academic conference. 
However, the organizer turned out to be a Scrooge, hesitant to spend generously to convey gratitude, yet also wanting to avoid appearing overly frugal. 
In this context, he sought your friend's assistance in establishing a budget.
After researching the lunch cost from nine team members, your friend found that **the average lunch expense was ¬•29 with a ¬•3 deviation**. 
So, she proposed a budget of ¬•31 per person. 
The organizer accused your friend not doing her job well and know nothing about statistics.
"**A Ôø•26 is enough** according to the data" he claimed.
]{.small}
:::

::: {.column width="50%"}
![](images/ci_drhu1.png){fig-align="center" height=num}
:::

::::


## PUA or not

:::{style="text-align:center"}
N =9, (X&#772; 29, s 3) vs. 26
:::

:::: {.columns}

::: {.column width="70%"}
*Solution*: 

> PUA = Blaming someone for something they did nothing wrong.

&rArr; The organizer blamed your friend for a reasonable suggestion of her. &rArr; The guess was in the CI of the data.

1. Set &alpha; = 0.05;
1. N = 9, t distribution, d.f. = 9 - 1 = 8;
1. t(&alpha; < (1 - 0.05)/2) = `r round(qt(.975, df = 8), digits = 4)` (`qt(.975, df = 8)`).
1. $CI = 29 \pm t_{\frac{0.975}{2}}(\frac{3}{\sqrt{9}})$, i.e, [`r round(29 - qt(.975, df = 8), digits = 0)`, `r round(29 + qt(.975, df = 8), digits = 0)`].
:::

::: {.column width="30%"}
![](images/ci_drhu2.png){fig-align="center" height=num}
:::

::::

:::{.fragment style="text-align:center"}
**Job done?**
:::


:::{.notes}
[Inference]{.blue}: If we make repeated sampling from the lunch expenses, there are 95% of the samples in which the interval between 27.5 and 31.8 contains the true mean. 
Therefore, an estimation of 31 sounds [fine]{.red}.
Your friend was treated unjustly.
:::

## Application II: Comparing Samples

Assuming X<sub>1</sub> and X<sub>2</sub> are i.i.d,

* &sigma; is known, $\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm Z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$.
* &sigma; is unknown, $\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm t_{\alpha/2}\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}$
   + $(\bar X_1 - \bar X_2) \pm t_{\alpha/2}S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$ when the population of the samples are identical, i.e., $\sigma_1 = \sigma_2$, where $S_p^2 = \frac{\sum(X_1 - \bar X_1)^2 + \sum(X_2 - \bar X_2)^2}{(n_1 - 1) + (n_2 - 1)}$, d.f.: $(n_1 - 1) + (n_2 - 1)$.
   

:::{.fragment style="text-align:center; margin-top: 2em"}
A.k.a., Difference in Means
:::

## An example {auto-animate=true}

![](images/ci_modules.png){fig-align="center" height=num}

Two module series have been designed for the "Learning R with Dr. Hu and His Friends" workshop: "R survivor" (basic) and "R expert" (advanced).
"R survivor" consists of five workshops.
Each received 932, 586, 796, 501, and 351 downloads after one round of the workshops.
"R expert" also comprises five workshops, with download counts of 771, 917, 326, 825, and 399, respectively.

## An example {auto-animate=true}

:::{.small}
Two module series have been designed for the "Learning R with Dr. Hu and His Friends" workshop: "R survivor" (basic) and "R expert" (advanced).
"R survivor" consists of five workshops.
Each received 932, 586, 796, 501, and 351 downloads after one round of the workshops.
"R expert" also comprises five workshops, with download counts of 771, 917, 326, 825, and 399, respectively.
Which series is more popular? 
:::

```{r eg-compare}
vec_survivor <- c(932, 586, 796, 501, 351)
vec_expert <- c(771, 917, 326, 825, 399)
```


:::{.fragment}
Solution: Set &alpha; = 0.05; small N &rarr; t.

\begin{align}
\bar X_1 =& (932 + 586 + 796 + 501 + 351)/5 = `r mean(vec_survivor)`; \bar X_2 = `r mean(vec_expert)` \\
S_p^2 =& \frac{`r sum((vec_survivor - mean(vec_survivor))^2)` + `r sum((vec_expert - mean(vec_expert))^2)`}{(5 - 1) + (5 - 1)} = `r (sum((vec_survivor - mean(vec_survivor))^2) + sum((vec_expert - mean(vec_expert))^2)) / (4 + 4)`;\\
\mu_1 - \mu_2 =& (`r mean(vec_survivor)` - `r mean(vec_expert)`) \pm `r round(qt(.975, df = 7), digits = 4)` (\sqrt{`r (sum((vec_survivor - mean(vec_survivor))^2) + sum((vec_expert - mean(vec_expert))^2)) / (4 + 4)`}\sqrt{1/5 + 1/5}), \\
=& `r mean(vec_survivor) - mean(vec_expert)` \pm `r round(qt(.975, df = 8) * sqrt((sum((vec_survivor - mean(vec_survivor))^2) + sum((vec_expert - mean(vec_expert))^2)) / (4 + 4) * 2/5), digits = 4)`.
\end{align}
:::

:::{.fragment}
[Inference:]{.red} If we make repeated sampling from the audience size of these lectures, there are 95% of the samples in which the interval between -378.8763 and 350.0763 contains the true mean. 
*The CI covers 0*. [That is, the difference is [no different]{.red} from zero statistically.]{.fragment}
:::


## IID vs. NON-IID

:::: {.columns}

::: {.column width="50%" .fragment .fade-in-then-semi-out}
**IID**

![Observations are selected [without regard to]{.red} who is in the other condition, a.k.a., **independent and identical distributed** (IID).](images/ci_independentSample.gif){fig-align="center" height=400}



:::

::: {.column .fragment width="50%"}
**Matched**

![Observations are [matched to]{.red} someone in the other condition.](images/ci_matchedSample.jpg){fig-align="center" height=400}

:::

::::



## Difference in Difference in Means

:::{.fragment}
*Independent sample*

$$\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm t_{\alpha/2}\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}$$
:::


:::{.fragment}
*Matched samples*

$D = X_1 - X_2$, then $\Delta = \bar D \pm t_{\alpha/2}\frac{S_D}{\sqrt{n}}$, where $S_D = \frac{\sum(D - \bar D)^2}{n - 1}$. 
:::

:::{.fragment}
*Matched proportions (Aggregate data)*

$D = \Pi_1 - \Pi_2$ ,then $\Delta = D \pm Z_{\alpha/2}\sqrt{\frac{P_1(1 - P_1)}{n_1} + \frac{P_2(1 - P_2)}{n_2}}$
:::

:::{.fragment .r-fit-text}
How are the two methods different?
:::

:::{.notes}
Why using a different method for matched samples? Not IID for individuals, but might for the difference
:::

## An example {auto-animate=true}

:::: {.columns}

::: {.column width="40%"}

![](images/ci_hsk.png){fig-align="center" height=num}

:::

::: {.column width="60%"}
Four students are required to attain an HSK-4 to apply Chinese universities (fantasy).
To apply to higher-tier universities, they must achieve even higher scores (fantasy, again).
Upon assessing their initial examination scores, they engaged in preparatory training. The subsequent tables display their performance in two subsequent examinations.

Was the preparatory training helpful?


| Student 	| Tim 	| Frank 	| Emily 	| Anthony 	|
|---------	|-----	|------	|-------	|------	|
| HSK      	| 57  	| 57   	| 73    	| 65   	|
| HSK      	| 64  	| 66   	| 89    	| 71   	|
:::

::::


## An example {auto-animate=true}

| Student 	| Tim 	| Frank 	| Emily 	| Anthony 	|
|---------	|-----	|------	|-------	|------	|
| HSK      	| 57  	| 57   	| 73    	| 65   	|
| HSK      	| 64  	| 66   	| 89    	| 71   	|


:::{.fragment}
Solution: Let &alpha; = 0.05,
\begin{align}
D =& X_1 - X_2 \Rightarrow \bar D = \sum D / n = (7 + 9 + 16 + 12) / 4 = 11\\
S_D^2 =& 46 / 3 = 15.3 \Rightarrow S_D = 3.9 \therefore \Delta = 11 \pm 3.18\times \frac{39}{4} = 11 \pm 6
\end{align}
:::


:::{.fragment}
[Inference]{.red}: If we make repeated sampling from these students, there are 95% of the samples in which the interval between 5 and 17 contains the true mean of the difference.
**The CI is above 0, that is, students did get better.**
:::



## Another Example

:::: {.columns}

::: {.column width="80%"}
Gallop drew a pair of 1500 samples from the American population. In the sample of 1980, there are 52% Democrats, and 46% in the 1985 sample. Were the Democrats the same for two years, given the 95% CI?

:::{.fragment}
Solution: Let &alpha; = 0.05,

\begin{align}
\Pi_1 - \Pi_2 &= (0.46 - 0.52) \pm 1.96\sqrt{\frac{0.46 * 0.54}{1500} + \frac{0.52 * 0.48}{1500}} \\
&\approx -0.06 \pm 0.036.
\end{align}
:::

:::

::: {.column width="20%"}
![](images/ci_gallop.png){fig-align="center" height=200}
:::

::::


:::{.fragment}
[Inference]{.red}:
If we make repeated sampling from the Amercian population, there are 95% of the samples in which the interval between -0.042 and 0.03 contains the true mean.
**The CI contains 0. Thus, the proportion of Democrats in 1980 was not different from that in 1985 statistically at the 0.05 level.**
:::


## BFF: Different Views 

:::: {.columns}

::: {.column .fragment width="40%"}
*Bayesian*

- [Credible]{.red} interval
- $\theta_\text{prior-based r.v.} \in [a, b]_{fixed}$

![There are ...% of the chance that the true value lies in the CI.](images/ci_dart.gif){ fig-align="center" height=300}

:::{.notes}
Some percentage of the [posterior]{.red} distribution lies within an particular region.
:::

:::

::: {.column .nonincremental width="30%"}
*Frequentist*

- [Confidence]{.red} interval
- $\theta_{fixed} \in [a, b]_{r.v.}$ 

![There is 95% chance the CI could contain the true value (before any data is collected).](images/ci_ringToss.webp){fig-align="center" height=300}

:::

::: {.column .fragment width="30%"}

*Fiducial*

- (Fiducial) Conf interval
- $\theta_{r.v.} \in [a, b]_{fixed}$

![There is 95% chance the CI could contain the true value ~~(before any data is collected)~~.](images/ci_swan.jpg){fig-align="center" height=300}

:::

:::{.notes}
Fisher used it /f…™Ààdu É…ôl/, ÈÇìÁê¨Áíê intro to causal inference, lec 3, https://pro.yuketang.cn/v2/web/v3/playback/734415352261764864/slide/54/0
:::

::::


## Take-home point {auto-animate=true}

::: {style="text-align: center"}
![](images/ci_mindmap.png){height="600"}
:::

---

## {auto-animate=true}

::: {style="text-align: center"}
![](images/ci_fullmap.png){.lightbox height="600"}
:::

## Have a break

{{< video src="https://link.jscdn.cn/1drv/aHR0cHM6Ly8xZHJ2Lm1zL3YvcyFBcnR0dk83MHdLSU8xSDM3UzNHdnNiNExETzJUP2U9SmIxVGFw.mp4" height="600" >}}