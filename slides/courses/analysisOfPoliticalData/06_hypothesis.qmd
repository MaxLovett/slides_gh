---
title: "Hypothesis Testing: Concept and Operation"
subtitle: "Large N & Leeuwenhoek (70700173)"

author: "Yue Hu"

knitr: 
    opts_chunk: 
      echo: false

format: 
  revealjs:
    css: https://sammo3182.github.io/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: true
    preview-links: false # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    callout-appearance: simple
      
filters:
  - lightbox

revealjs-plugins:
  - spotlight

# lightbox: auto
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---


## Overview

```{r setup, include = FALSE}


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools
) 


# Functions preload
set.seed(114)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

- Understand hypothesis testing
    - Substantive concept
    - Operational concept
- Process a hypothesis test

# Understand hypothesis testing

## Substantive definition

:::{style="text-align:center"}
Theory (logic) &larr; real world
:::

:::: {.columns}

::: {.column width="30%"}
:::{.r-vstack}
![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hpy_checkBalance_ideal.jpg){.fragment fig-align="center" height=250}


[&darr;]{.fragment style="text-align:center}


![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hpy_checkBalance_real.png){.fragment fig-align="center" height=250}

:::

:::{.notes}
In June 2022, the Supreme Court overruled Roe vs. Wade by 6:3
:::
:::

::: {.column width="70%"}

:::{.r-hstack}
![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hpy_ChinaCollapse.jpg){.fragment height=400}

[&rarr;]{.fragment style="text-align:center}

![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hpy_ChinaNotCollapse.png){.fragment height=400}

:::
:::

:::{.notes}
Gordon Guthrie Chang (born July 5, 1951) is a columnist, author, and lawyer., In 2010, Chang wrote in The Christian Science Monitor that "China could fail soon" and predicted an economic crash.[8] In an article, "The Coming Collapse of China: 2012 Edition," published by the Foreign Policy magazine website, Gordon G. Chang admitted that his prediction was wrong but arguing that he was off only by one year: "Instead of 2011, the mighty communist party of China will fall in 2012. Bet on it."[9] On May 21, 2016, The National Interest published another article by Chang, "China's Coming Revolution." In it, he argued that the ruling class in China is divided and that it cannot deal with its economic problems. Chang claimed that would lead to a revolution, which would overthrow the Communist Party. Unlike for his other predictions, he did not give the exact year that those events would take place
:::

::::



## Operational definition

:::{style="text-align:center"}
Theory (logic) &rarr; real world (conceptual)
:::

:::{style="text-align:center"}
&darr;

Ideal world vs. real world
:::

:::{style="text-align:center"}
&darr;

Ideal world &ne; real world:  Really? 

Random distribution vs. empirical distribution
:::

:::{style="text-align:center"}
&darr;

Ideal world &ne; real world:  How? 

1. Identification
1. Correlation analysis + Causal inference

&darr;

Estimates (coefficients + [uncertainty]{.red})
:::

## How to get the coefficients

:::{.fragment .fade-in-then-semi-out}
*Probability*: The [chance]{.red} of occurrence of X<sub>i</sub> given the PDF of X, P(O|&theta;).
:::

:::{.fragment}
*Likelihood* : How probable a given set of [observations]{.red} is for certain values of the parameters of a distribution, &Lscr;(&theta;|O) = $\prod_{i=1}^n y_i$^[[**Maximum Likelihood Estimation**: Maximizing the probability that &Lscr;(&theta;|O) = P(O|&theta;).]{.fragment}]
:::

:::{.notes}

&Lscr; Lagrangian or Laplace transform

Formally, let O be the set of observed outcomes and &theta; be the set of parameters that describe the stochastic process. 

*Probability*: The integral, the area, the results.

*Likelihood*: The parameter, the product, the hypothesis.

&Lscr; can above 1.
:::

:::{.fragment}
```{r likelihood}
#| fig-align: center
#| fig-height: 4

plot_pr <- ggplot(data.frame(x = 0:10, 
                  pr = dbinom(x = 0:10, size = 10, prob = 0.5)), 
       aes(x = x, y = pr)) +
  geom_bar(stat = "identity") + 
  ylab(expression(paste("P(O|", theta, ")"))) +
  xlab("O(bservation)")

plot_lh <- ggplot(data.frame(x = seq(0,10,0.001)), aes(x = x)) +
  stat_function(fun = function(x) dnorm(x, mean =5, sd =1)) +
  ylab(expression(paste("L(", theta, "|O)"))) + 
  xlab(expression(theta))

plot_pr + plot_lh
```
:::

:::{.notes}
* Goal: Model (parameter) selection
    + What's the most appropriate estimation?
        + Maximizing the probability that &Lscr;(&theta;|O) = P(O|&theta;)
    + Point estimation: Unbiasdness
    + Interval estimation: Range of plausible values
    + Goodness of fit: Explained variances
    + Diagnostic estimation: What if the assumptions are violated
    
    
E.g., Given binomial distribution $L(\pi) = {n \choose s}\pi^s(1 - \pi)^{n - s}$, what &pi; reaches the maximum likelihood?

$$log[L(\pi)] = log{n\choose s} + s\cdot log(\pi) + (n - s)\cdot log(1 - \pi).$$


To get the peak value, we use the derivative:

$$\frac{dlog[L(\pi)]}{d\pi} = \frac{s}{\pi} - \frac{n - s}{1 - \pi}.$$


To minimize this, let it equal 0

\begin{align}
\frac{s}{\pi} - \frac{n - s}{1 - \pi} = 0 \Rightarrow \frac{s}{\pi} &= \frac{n - s}{1 - \pi},\\
s(1 - \pi) &= \pi(n - s),\\
s - s\pi &= n\pi - s\pi \Rightarrow\pi = \frac{s}{n}.
\end{align}

:::

## How to get uncertainty

- Parametric: CI estimation
    - ![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hyp_parametric.webp){height=200}

- Nonparametric: Bootstrap
    - ![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hyp_nonparametric.webp){height=300}


## Format

:::{style="text-align:center; margin-top: 1em"}
- Null (Random distribution): &Lscr;(null): Y = &beta;<sub>0</sub> + &epsilon;
- Theoretical (Your thought): &Lscr;(theory): Y = &beta;<sub>0</sub> + [&beta;<sub>1</sub>X]{.red} + &epsilon;
- Testing (Hypothesis testing): &Lscr;(null) vs. &Lscr;(theory)
:::

![](https://raw.githubusercontent.com/sammo3182/backup/master/images/20231022145849.png){.fragment fig-align="center" height=400}


# Process a hypothesis test

## Statistical Foundations

:::: {.columns}

::: {.column width="50%"}
Law of Large Number(**LLN**): For a sample with size n of a random variable X, 

$${\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {X_{i}}{n}} = \lim _{n\to \infty }\bar{X} = \mu.} $$

:::{.notes}
When n gets larger, the ratio of outcomes, X&#772;, will [converge to]{.red} the theoretical (population) average &mu;

Converge = concentrated around

Convergence in probability

Proof: https://blog.csdn.net/a19990412/article/details/85283729
:::
:::

::: {.column width="50%"}
Central limit theorem (**CLT**): For a random sample of n, X&#772; fluctuate around &mu; with an uncertainty,

$$When\ n \rightarrow \infty, Pr(\bar X) \sim \mathcal{N(\mu, \sigma)}. $$

:::{.notes}
When [independent]{.red} random variables are summed up, their properly normalized sum tends toward a normal distribution, even if the original variables themselves are not normally distributed.

Proof: https://zhuanlan.zhihu.com/p/93738110
:::

:::

::::

:::{.fragment style="text-align:center"}
When n gets larger...

&zwj;LLN: X&#772; will [approach]{.red} to &mu;   
&zwj;CLT: X&#772; will [distribute]{.red} normally.
:::


:::{.fragment style="text-align:center; margin-top: 2em"}
Contradictory?

:::{.notes}
Becoming a value or becoming a distribution?

CLT shows the the shape, LLN shows the location of the center.
:::

:::

## Steps (parametric test) {auto-animate=true}

1. An observed sample;
1. Making the IID assumption;
1. Set up the null (and alternative) hypotheses
1. Set the confidence level;
1. Compare the X&#772; with &mu;.


## IID {auto-animate=true}

:::{.nonincremental}

1. [An observed sample;]{.grayLight}
1. Making the IID assumption;^[[No need under [Lindeberg-Feller CLT](https://myweb.uiowa.edu/pbreheny/7110/wiki/lindeberg-feller.html).]{.fragment}]
    - Identical: X and Y are from the same distribution, ${\displaystyle F_{X}(x)=F_{Y}(x)\,\forall x\in I}$
    - Independent: ${\displaystyle F_{X,Y}(x,y)=F_{X}(x)\cdot F_{Y}(y)\,\forall x,y\in I}$
1. [Set up the null (and alternative) hypotheses]{.grayLight}
1. [Set the confidence level;]{.grayLight}
1. [Compare the X&#772; with &mu.]{.grayLight}

:::

:::{.notes}
Unbiased point estimate:

\begin{align}
Given \bar X = \frac{\sum X}{n}, E(\bar X) =& \frac{1}{n}[E(X_1) + E(X_2) + \dots + E(X_n)],\\
  =& \frac{1}{n}[\mu + \mu + \dots + \mu] = \mu.
\end{align}

Unbiased variance (standard error): 

\begin{align}
s(\bar X)^2 =& \frac{[E(s(X_1)^2) + E(s(X_2)^2) + \dots + E(s(X_n)^2)]}{n^2}\\
  =& \frac{1}{n}[\sigma^2 + \sigma^2 + \dots + \sigma^2] = \frac{\sigma^2}{n}\\
\therefore\ SE =& \frac{\sigma}{\sqrt{n}}.
\end{align}

Lindeberg-Feller CLT extend CLT to non-iid random variables, which does not require identical &sigma; but as long as there is not big outliers, CLT should be fine.

:::

## Hypothesis Setting {auto-animate=true}

:::{.nonincremental}

1. [An observed sample;]{.grayLight}
1. [Making the IID assumption;]{.grayLight}
    - [Identical: X and Y are from the same distribution, ${\displaystyle F_{X}(x)=F_{Y}(x)\,\forall x\in I}$]{.grayLight}
    - [Independent: ${\displaystyle F_{X,Y}(x,y)=F_{X}(x)\cdot F_{Y}(y)\,\forall x,y\in I}$]{.grayLight}
1. Set up the null (and alternative) hypotheses
    - H<sub>0</sub>: Specifying values for one or more population parameters in a random distribution (&mu;, &pi; rather than X&#772;, P);
    - H<sub>1</sub>: the population parameter is something other than the value in the stochastic status;
1. [Set the confidence level;]{.grayLight}
1. [Compare the X&#772; with &mu.]{.grayLight}

:::

## Confidence Interval {auto-animate=true}

:::{.nonincremental}

1. [An observed sample;]{.grayLight}
1. [Making the IID assumption;]{.grayLight}
    - [Identical: X and Y are from the same distribution, ${\displaystyle F_{X}(x)=F_{Y}(x)\,\forall x\in I}$]{.grayLight}
    - [Independent: ${\displaystyle F_{X,Y}(x,y)=F_{X}(x)\cdot F_{Y}(y)\,\forall x,y\in I}$]{.grayLight}
1. [Set up the null (and alternative) hypotheses]{.grayLight}
    - [H<sub>0</sub>: Specifying values for one or more population parameters in a random distribution (&mu;, &pi; rather than X&#772;, P);]{.grayLight}
    - [H<sub>1</sub>: the population parameter is something other than the value in the stochastic status;]{.grayLight}
1. Set the confidence level;
    - 1 - [&alpha;]{.red}
1. [Compare the X&#772; with &mu.]{.grayLight}

:::


## Let's talk about alpha

:::: {.columns}

::: {.column .fragment width="50%"}
![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hyp_errorType.png){fig-align="center" height=num}
:::

::: {.column .fragment width="50%"}
| Decision 	| H<sub>0</sub> T                   	| H<sub>0</sub> F                   	|
|----------	|--------------------------	|--------------------------	|
| Reject  	| Type I error (Pr = &alpha;)            	| P = 1 - &beta; 	|
| [Fail to]{.red} Reject 	| Pr = 1 - &alpha; 	|  Type II error (Pr = &beta;)            	|
:::

::::

:::{.notes}
Type I: 无中生有
Type II: 闪


&beta; is the power of the test.

Avoiding Type I is more emergent.
:::


## Why 0.05

It's not arbitrary, &alpha; = 0.05 &rArr; 1 - &alpha; = 0.95 (one-tailed) or 0.975 (two tailed)

```{r ci}
#| fig-align: center

funcShaded <- function(x) {
  y <- dnorm(x, mean = 0, sd = 1)
  y[x < 0 - qnorm(0.975) | x > (0 + qnorm(0.975))] <- NA
  return(y)
}

ggplot(data = data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                n = 10000,
                args = list(mean = 0, sd = 1)) +
  stat_function(
    fun = funcShaded,
    geom = "area",
    fill = gb_cols("gold"),
    alpha = 0.2
  ) +
  ylab("") + xlab("")
```

:::{.fragment .r-fit-text}
Well...it's actually [**arbitrary**]{.red} ANNNNNNNNNND [qualitative]{.red}.
:::

:::{.notes}
Not calculate the p-value and set the significance but set the confidence level and conduct the test. This is scientific
:::

## How to get p < 0.05

![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hyp_phacking.jpg){.fragment fig-align="center" height=300}

:::{.fragment}

:::{.callout-important}
> [守住拒腐防变防线，最紧要的是守住内心，从小事小节上守起，正心明道、怀德自重，勤掸“思想尘”、多思“贪欲害”、常破“心中贼”，以内无妄思保证外无妄动。]{.red}    
[—— 2022年3月1日，习近平在中央党校（国家行政学院）中青年干部培训班开班式上的讲话]{.golden}
:::

:::

:::{.fragment .large style="text-align:center"}
You [DON'T]{.red}!
:::


## Deploy Hypothesis testing {auto-animate=true}

:::{.nonincremental}

1. [An observed sample;]{.grayLight}
1. [Making the IID assumption;]{.grayLight}
1. [Set up the null (and alternative) hypotheses]{.grayLight}
1. [Set the confidence level;]{.grayLight}
1. Compare the X&#772; with &mu;, two steps:
    - SE
    - Critical value / Confidence intervals
    
:::


## Standard Error (Formal Introduction)

$$SE(\bar X) = \frac{s}{\sqrt{n}}.$$

:::{.fragment}

*E.g.* Given the population mean as 69 and standard deviation as 3.2, how would the mean of a random sample of four observations fluctuate? 

\begin{align}
E(\bar X) =& \mu = 69; \sigma = 3.2, \\
\therefore SE(\bar X) =& \frac{3.2}{\sqrt 4} = 1.6.
\end{align}

:::

## Correct SE ![](https://raw.githubusercontent.com/sammo3182/backup/master/images/ci_fsmrof.png){height=30}

:::{.callout-warning}
For small **population**, not small sample)
:::

- Finite population correction, FPC: Without replacement, SE of X&#772; [&uarr;]{.red}  
    - &rArr; uncertainty overestimated &rArr; correction: $\sqrt{\frac{N - n}{N - 1}}$
    - n = 1 (sampling only one sample), Then $SE = \frac{\sigma}{\sqrt{n}}\sqrt{\frac{N    - n}{N - 1}} = \frac{\sigma}{\sqrt{n}}$, no changes; 
    - n = N (sampling all), SE = 0. 
    - n = 1000, N = 100,000,000 (large sample of a large population), $FPC = \sqrt{\frac{100000000 - 1000}{100000000 - 1}}\approx .999$, little changes.
    - n = 100, N = 108 (large sample of a [small]{.red} population), $FPC = \sqrt{\frac{108 - 100}{108 - 1}}\approx 0.075$, some changes.

## What does the FPC correct

![](https://raw.githubusercontent.com/sammo3182/backup/master/images/20231022160554.png){fig-align="center" height=num}


## Statistical Test 
:::: {.columns}

::: {.column width="50%"}
- SE &rarr; Z(t)-score &rarr;
    1. Critical values (z/t), or
    1. Confidence intervals
:::

::: {.column .fragment width="50%"}
$$Z = \frac{\bar X - \mu}{SE}= \frac{\bar X - \mu}{\sigma/\sqrt n}.$$

$$Z = \frac{P - \pi}{\sqrt{\frac{\pi(1 - \pi)}{n}}}.$$

:::{.notes}

In a sample of N, the sample proportion P fluctuates around the population proportion &pi; with SE, $\sqrt{\frac{\pi(1 - \pi)}{n}}$. (Why?)

> Proof: 
> $\sigma^2 = E(X^2) - \mu^2 = \pi - \pi^2 = \pi(1 - \pi)$.
&there4; $\sigma = \sqrt{\pi(1 - \pi)}, SE = \sigma/\sqrt{n} = \sqrt{\frac{\pi(1 - \pi)}{n}}.$

Then, $Z = \frac{P - \pi}{\sqrt{\frac{\pi(1 - \pi)}{n}}}$, the confidence interval: $\pi = P \pm Z\sqrt{\frac{P(1 - P)}{n}}.$

:::

:::

::::


::: {.panel-tabset .fragment}
## Individual-level Data

:::{.fragment}
E.g. Given $\mu$ = 72 and $\sigma$ = 9, and a random sample of 10. Calculate the probabilities of P(X > 80) and P(X&#772; > 80)

$Z = \frac{80 - 72}{9} = .89\Rightarrow P(Z > .89) =$ `r round(1 - pnorm(.89), digits = 4)`;

$Z = \frac{80 - 72}{9/\sqrt{10}} = 2.81\Rightarrow P(Z > 2.81) =$ `r round(1 - pnorm(2.81), digits = 4)`.

:::

## Aggregate Data

:::{.fragment}
E.g., Given the Republican are 60% of the U.S. population, what's the probability that the Republican are the minority in a random sample of 100 people from the national population?

Minority means $P(\pi < 0.5)$. 

Then, $Z = \frac{0.5 - 0.6}{\sqrt{\frac{0.6(1 - 0.6)}{100}}} =$ `r round(0.1/(sqrt(0.6 * 0.4/100)), digits = 4)`, therefore, P(Z < `r round(0.1/(sqrt(0.6 * 0.4/100)), digits = 4)`) = `r round(pnorm(-0.1/(sqrt(0.6 * 0.4/100))), digits = 4)`.

:::

:::


## Full Process Illustration

Given a virus can influence 10% of the population. 
Now there's a sample of senior people, n = 527, within which there are 14% infected.
Are senior people more likely to be victimized?


- Assuming the sample is IID sampled and $H_0: \pi \leq 10; H_1: \pi > 10.$ 
- &alpha; = 0.05
- Confidence-interval method:
  - $\pi = 0.14 \pm 1.96 * \sqrt{\frac{0.14 * (1 - 0.14)}{527}} = 0.14 \pm 0.03,$ that is [0.11, 0.17] > 0.1. $H_0.$ rejected.
- Critical value method
  - $Z_{obs} = \frac{P - \pi}{\sqrt{\frac{\pi(1 - \pi)}{n}}} = \frac{14 - 10}{\sqrt{\frac{0.1 * 0.9}{527}}} = 3.06.$ Given the level of $\alpha = 0.05$, $Z_{critical}$ = `r round(qnorm(0.975), digits = 4)` < $Z_{obs}$,^[[In r, `qnorm(0.975)`. 0.975, huh?]{.fragment}] therefore reject the $H_0.$


## One or Two Tailed Test

![](https://raw.githubusercontent.com/sammo3182/backup/master/images/hyp_tailed.png){fig-align="center" height=400}

- Most applications are one-tail tests, while most software gives two-tail results.
- One-tailed test more often apply the critical value than confidence interval method.

## Take-home point

::: {style="text-align: center"}
![](https://raw.githubusercontent.com/sammo3182/backup/master/images/20231022141302.png){height="600"}
:::
