---
title: "Multicollinearity & Heteroskedasticity"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
institution: "Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---

class: middle, center

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse, flexable, icons, xaringanExtra, ggeffects
) 


use_xaringan_extra(c("tile_view", # O
                     "broadcast",
                     "panelset",
                     "tachyons",
                     "fit_screen"))
use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = FALSE  #<<
)


# Functions preload
set.seed(313)

theme_set(theme_minimal())
```


*Where Have We Arrived*

### Occurance
Basis of data: Probability theory (descriptive stats, probablity, distribution)
Basis of scientific approach: Uncertainty (confidence intervals, hypothesis testing)

&dArr;

### Relationship
Associations (parametric and non-parametric) and simple OLS

&dArr;

### Mechanism
Analysis of multi-dimensions 

---

## Why Multiple Regression

In other words, why *controls*? 

--

<img src = "images/mh_drStrange1.gif" height = 350 width = 480 >

--

<img src = "images/mh_drStrange2.gif" height = 350 width = 480 >

???

We wanna understand things realistically and predict them.

Things happens all together, while we care what's the most important

--

Except for controls, what else should we do?

---

## Using the Power Correctly

1. Linearity in the parameter; &larr; .green[.large[&#9083;]] Lecture 11
1. .gray[Nonstochastic X ("given X," a.k.a., "X is fixed");  &larr; For Granted]
1. .gray[X has positive noninfinite variance (var(X)); &larr; For Granted]
1. Correct specification;  &larr; .green[.normal[&check;]] Lecture 11
1. .gray[Identification (N > K; K = 2 for a simple OLS);  &larr; DGS]
1. Mean zero errors (E(u<sub>i</sub>|X<sub>i</sub>) = 0); &larr; .green[.large[&#9083;]] Lecture 11 (Omited variables, etc.)
1. No covariance between X<sub>i</sub> and u<sub>i</sub>;
1. No autocorrelation;
1. Homoskedasticity;
1. No perfect collinearity

.center[Any question about we've learned so far?]

---

.center[.red[Quiz Time! Quiz Time!]]

Tell me whether the following Up-zhu did things correctly:

.center[
<video width="1000" height="450" controls>
<source src="https://outin-03f65b0c44ec11ec96dc00163e024c6a.oss-cn-shanghai.aliyuncs.com/sv/32a38aa2-17d4035e0d1/32a38aa2-17d4035e0d1.mp4?Expires=1637463567&OSSAccessKeyId=LTAIVVfYx6D0HeL2&Signature=rM%2F0LbJBGeeA4TgP9aus40s5wFM%3D" type="video/mp4">
</video>
]


---

.pull-left[
.small[
1. Linearity in the parameter; &larr; .green[.large[&#9083;]] Lecture 11 & .red[14 (moderation)]
1. .gray[Nonstochastic X ("given X," a.k.a., "X is fixed");  &larr; For Granted]
1. .gray[X has positive noninfinite variance (var(X)); &larr; For Granted]
1. Correct specification;  &larr; .green[.normal[&check;]] Lecture 11
1. .gray[Identification (N > K; K = 2 for a simple OLS);  &larr; DGS]
1. Mean zero errors (E(u<sub>i</sub>|X<sub>i</sub>) = 0); &larr; .green[.large[&#9083;]] Lecture 11 (Omited variables, etc.) & .red[14 (Missing)]
1. No covariance between X<sub>i</sub> and u<sub>i</sub>; &larr; .red[Lecture 13]
1. No autocorrelation; &larr; .red[Lecture 13]
1. Homoskedasticity; &larr; .red[**Lecture 12**]
1. No perfect collinearity &larr; .red[**Lecture 12**]
]
]


.pull-right[
## Overview

1. Collinearity
    + Violation
    + Diagnosis
    + Solution
1. Heteroscedasticity
    + Violation
    + Diagnosis
    + Solution
]
---

class: inverse, bottom

# Collinearity

---

## Perfect Collinearity

$$X_{1i} = \hat\delta_0 + \hat\delta_1X_{2i} + \hat r_{1i}$$

When cov(X<sub>1</sub>, X<sub>2</sub>) = 1, r<sub>1i</sub> = 0, then $\hat\beta_1 = \frac{\sum\hat r_{1i}y_i}{\hat r_{1i}^2}$ cannot be estimated.

--

e.g., Let's say in the PRF Y<sub>i</sub> = &beta;<sub>0</sub> + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + u, X<sub>2</sub> = 1 + 2X<sub>1</sub>, then

\begin{align}
Y_i =& \hat\beta_0 + \hat\beta_1X_{1} + \hat\beta_2X_{2} + \hat u \\
    =& \hat\beta_0 + \hat\beta_1X_{1} + \hat\beta_2(1 + 2X_1) + \hat u \\
    =& (\hat\beta_0 + \hat\beta_2) + (\hat\beta_1 + 2\hat\beta_2)X_1 + \hat u\\
\Rightarrow Y_i =& \hat\alpha_0 + \hat\alpha_1X_{1} + \hat u
\end{align}

PRF (&beta;<sub>1</sub> or &beta;<sub>2</sub>) is non-identifiable. 

--

Problematic, but .red[only] when "perfect"

???

PRF: population regression function

---

## Multicollinearity

**X**<sub>2</sub> is not a transformation of X<sub>1</sub>.In this case, the estimators (**&beta;**) remain .red[unbiased], yet

\begin{align}
var(\hat\beta_1|X) =&\frac{\sigma^2\sum(X_{2i} - \bar X_2)^2}{\sum(X_{1i} - \bar X_1)^2\sum(X_{2i} - \bar X_2)^2 - [\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]^2}\\
=&\frac{\sigma^2}{\sum(X_{1i} - \bar X_1)^2(1 - \beta_{12}^2)}, \text{where }\beta_{12} = cov(X_1, X_2).
\end{align}


As &beta;<sub>12</sub> increases, the variance also increases (What does that mean?).

---

## Diagnosis

**Variance Inflation Factors** (VIF, [1, +&infin;]): A measure of how much the variance of the estimated coefficient &beta;<sub>k</sub> is "inflated" by the correlation among the predictor variables.

$$VIF = \frac{1}{1 - {R}^{2}_{k}} = \frac{1}{Tolerance}$$

--

1 means that no correlation among the k<sup>th</sup> predictor and the remaining predictor variables, and hence the variance of the coefficient is not inflated at all.

--

Rule of thumb: 4, 10    
(In most cases, you don't need this, why?)

???

Perfect then unidentified; but if your core X is insignificant.

---

## Solution

1. Removed the highly correlated variables.
1. Remeasure the correlated variables (e.g., EFA, CFA, IRT)

--

.center[When do you need to do the above?]

---

class: inverse, bottom

# Heteroscedasticity

---

##  Heteroscedasticity

Homo/heteroscedasticity &asymp;     Homo/heterogeneity (of variance)

???

The word “heteroscedasticity” comes from the Greek, and quite literally means data with a different (hetero) dispersion (skedasis).

Regression: Heteroscedasticity

ANOVA: Heterogeneity (of variance)

--

The CLR Assumption says: var(u<sub>i</sub>|X) = &sigma;<sup>2</sup>

--

### Violation

$$var(u_i|X) = \sigma_i^2, \sigma_i^2 \neq \sigma_j^2, \forall i, j.$$

---

background-image: url("images/heteroscedasticity1.png")
background-position: center
background-size: contain

---

background-image: url("images/heteroscedasticity2.webp")
background-position: center
background-size: contain

---

class: small

## Consequence
.pull-left[
.red[Unbiased], expected consistent, but inefficient

\begin{align}
var(\hat\beta|X) =& var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X),\\
                 =& var(\frac{\sum(X_i - \bar X)Y_i}{\sum(X_i - \bar X)^2}|X),\\
                 =& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}var(Y_i|X),\\
                 =& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\sigma_i^2.
\end{align}
]

--

.pull-right[
\begin{align}
H_0: var(\hat\beta|X) =& \sum(X_i - \bar X)^2;\\
\Leftrightarrow\frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\sigma_i^2 =& \frac{\sigma^2}{\sum(X_i - \bar X)^2},\\
\sum(X_i - \bar X)^2\frac{\sigma_i^2}{\sigma^2} =& \sum(X_i - \bar X)^2
\end{align}

Only when $\frac{\sigma_i^2}{\sigma^2} = 1$, the estimate is efficient.
]

---

### Diagnosis

Park test, Goldfeld-Quandt test, Breusck-Pagan(-Godfreg) test, White test

**Goldfeld-Quandt test**:

1. Divide the domain of X to three parts, $\frac{n -c}{2}, c, \frac{n -c}{2}$. 
1. Regress Y on X as usual for each part.
1. Calculate $\sigma_L^2$ from part 1 and $\sigma_H^2 = \frac{\sum^n_{i = \frac{n + c}{2} + 1}}{\frac{n - c}{2} - k}\hat u_i^2$ from part 3.
1. Then test $H_0: \sigma_L^2 = \sigma_H^2$
    + Statistics: $\frac{\sigma_L^2}{\sigma_H^2}\sim F_{\frac{n - c - 2k}{2}, \frac{n - c - 2k}{2}}$

Theoretically, the smaller the c is, the more power the test has. However, too small c may also cause the difference between low and high groups difficult to be identified.

---

**Park test**

Ocular-inspection test: Use the scalar points of $\hat u_i^2$ against $X_i$, or $\hat u_i^2$ against $Y_i$

Regress $ln(\hat u_i^2)$ on some $X_{ki}$:

$$ln(\hat u_i) = \hat\delta_0 + \hat\delta_1ln(X_{ki}) + \hat\gamma_i.$$

Do the t-test of coefficient in $ln(X_i)$: $H_0: \hat\delta_1 = 0.$

???

Ocular: 望远镜

<img src="images/mh_parkTest.png" height = 200 />

---

.pull-left[
**Breusck-Pagan(-Godfreg) test**

Assuming $\boldsymbol{\sigma_i^2} = \boldsymbol{X\delta} + \boldsymbol{v_i}$


Regress $\hat u_i^2$ on $X_1, X_2, \cdots, X_{k - 1}$, 

$H_0: \sigma_1 = \sigma_2 =\cdots=\sigma_{k - 1} = 0$


$H_1$: At least one of the &sigma;&ne;0


Statistics $F_{k - 1, n - k}$
]

--

.pull-right[
**White Test**

1. Regress Y on X to get $\hat u_i^2$
1. Regress $\hat u_i^2$ on $\sum X_i + \sum X_i^2 + \sum X_iX_j$ or $\hat Y_i$.
1. Test this model against a model with no variables (F test).
]

---

## Solution

White covariance matrix, weighted least square, FGLS......

**White covariance matrix**     
(a.k.a., Heteroscedasitic-consistent covariance matrix).

$$E(u_i|X) = E\{\sqrt{[u_i - E(u_i)]^2}|X\}$$

Since E(u<sub>i</sub>|X) = 0 by assumption, we can estimate $u_i$ with $\hat u_i^2$ and estimate $var(\hat\beta_1|X)$ with $\frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\hat u_i^2.$

--

The estimates turn out to be .red[biased], but converge asymptotically in n to the true distribution. (What does this imply?) 

--

Means it's fine when N is large enough.

---

**Weighted least square**

Reduce the substantive effect of the change in X by $\frac{1}{\sigma_i}$, in order to squeeze the value towards the middle:

Given &sigma;<sub>i</sub> is known, 

\begin{align}
\frac{Y_i}{\sigma_i} =& \frac{\beta_0}{\sigma_i} + \frac{\beta_1}{\sigma_i}X_i + \frac{u_i}{\sigma_i};\\
\text{Then, } var(\frac{u_i}{\sigma_i}) =& \frac{1}{\sigma_i^2}var(u_i) = \frac{\sigma_i^2}{\sigma_i^2} = 1;\\
\Rightarrow Y_i^* =& \beta_0X^*_{0i} + \beta_1X^*_{1i} + u_i^*. 
\end{align}

The last equation is homoscedastistic.

---

However, in most cases, we don't know &sigma;<sub>i</sub>. So, usually we assume var(u<sub>i</sub>)&sim; X<sub>1i</sub>, i.e., var(u<sub>i</sub>) = &sigma;<sub>i</sub><sup>2</sup> = &sigma;<sup>2</sup>X<sub>i</sub> = h<sub>i</sub>&sigma;<sup>2</sup>.

.pull-left[
Given the goal var(u<sup>\*</sup><sub>i</sub>) = var(u<sup>\*</sup><sub>j</sub>), &forall; i,j. 

\begin{align}
u^*_i =& \frac{u_i}{\sqrt{h_i}}\\
var(\frac{u_i}{\sqrt{h_i}}) =& \frac{var(u_i)}{h_i} = \frac{h_i\sigma^2}{h_i} = \sigma^2\\
\Rightarrow Y_i^* =& \frac{Y_i}{\sqrt{h_i}}\\
            X_{0i}^* =& \frac{1}{\sqrt{h_i}}\\
            X_{1i}^* =& \frac{X_{1i}}{\sqrt{h_i}}, \text{ assuming } X_i\in R^+.
\end{align}

]

.pull-right[
In practice, there are different ways to estimate the weight, one need to carefully choose the proper one.
]

---

**Feasible Generalized Linear Squares** (FGLS)

\begin{align}
\boldsymbol{Y} =& \boldsymbol{X\beta} + \boldsymbol{u}; \\
var(\boldsymbol{u}) =& \Omega_{n\times n} = \left(
       \begin{array}{cccc}
       \sigma_1^2 & 0 & \cdots & 0\\
       0 & \sigma_2^2 & \cdots & 0\\
       \vdots & \vdots & \vdots & \vdots \\
       0 & 0 & \cdots & \sigma_n^2\\
       \end{array}\right)\\
\text{Then, } \boldsymbol{\hat\beta_{GLS}} =& (\boldsymbol{X'\Omega X})^{-1}(\boldsymbol{X'\Omega Y})\\
\text{Let }\boldsymbol{H}: \boldsymbol{\Omega} =& \boldsymbol{HH^{-1}},\\
\text{Then, } \boldsymbol{H^{-1}Y} =& \boldsymbol{H^{-1}X\beta} + \boldsymbol{H^{-1}u}
\end{align}

---

In this case

\begin{align}
\boldsymbol{H^{-1}u} =& \boldsymbol{H^{-1}(H^{-1})'}var(\boldsymbol{u})\\
                     =& (\boldsymbol{HH'})^{-1}\boldsymbol{\Omega} = \boldsymbol{\Omega}^{-1}\boldsymbol{\Omega} = \boldsymbol{I}.\\
var(\boldsymbol{\hat\beta_{GLS}}) =& (\boldsymbol{X'X})^{-1}(\boldsymbol{X'\Omega X})(\boldsymbol{X'X})^{-1}
\end{align}

* If there's no heteroscedasticity, $\boldsymbol{\Omega} = \sigma\boldsymbol{I}.$
* If there is heteroscedasticity and $\boldsymbol{\Omega}$ is known, then WLS (a special type of GLS).
* If $\boldsymbol{\Omega}$ is unknown, run $\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{u}$ to estimate $\boldsymbol{\Omega}$ with $\boldsymbol{\hat\Omega} = \boldsymbol{\hat u\hat u'}$

--

.center[NB: This method does not get SE, and also .red[biased] for small N.]

---

## "Sandwich" Matrix

FGLS is a type of "sandwich" estimator.

In a more general view, let $\boldsymbol{Q} = \boldsymbol{X'X}$ and 

\begin{align}
\boldsymbol{Q} = \left(
\begin{array}{cccc}
\sigma_1^2 & 0 & \cdots & 0\\
0 & \sigma_2^2 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sigma_n^2\\
\end{array}\right)
\end{align}

Then for regular OLS, $var(\beta) = \sigma^2(\boldsymbol{X'X})^{-1} = \sigma^2\boldsymbol{Q}^{-1}$.

But when heteroscedasticity occurs, $var(\boldsymbol{\beta}|X)\neq \sigma^2\boldsymbol{Q}^{-1}$.

Instead, let $\boldsymbol{G} = \boldsymbol{X'GX}$, then $var(\boldsymbol{\beta}|X) = \boldsymbol{Q^{-1}GQ}^{-1}.$

---

## Heterogeneity in TSCS Data

TSCS: Time-series cross-sectional data.   
Sometimes, they are called "pooled data", "panel data"(wrong!)

Dealing with heterogeneity in TSCS data:

1. Robust standard error
1. Fixed effect
1. Multilevel modeling

---

## Robust Standard Error

**Sandwich SE**

In the FGLS version, the "meat" can be identified only when T > N, and before this the autocorrelation ($cov(u_i, u_j|X_i, X_j) = 0, \forall i, j$) has to be eliminated.

--

**Panel-corrected SE**

An alternative version of the sandwich SE. &Omega; is clustered by time periods: $\hat\Omega_{ij} = \frac{\sum^T_{t = 1}\epsilon_{it}\epsilon_{jt}}{T}$

--

**Cluster SE**

Adjust SE to account for correlations within clusters.

---

**Within-between model**

1. Run separate models in each group
1. Run an aggregate regression: 

Let $\bar Y_i = \frac{\sum^n_{i=1}Y_{it}}{n_i}, \bar X_i = \frac{\sum^n_{i=1}X_{it}}{n_i}, \bar u_i = \frac{\sum^n_{i=1}u_{it}}{n_i}$. Then, 

\begin{align}
\bar{Y_i} =& \beta_0 + \beta_1\bar{X_i} + \epsilon_i + u_i\\
Y_{it} - \bar{Y_i} =& (\beta_0 - \beta_0) + \beta_1(X_{it} - \bar{X_i}) + (u_{it} - u_i)\\
\hat{Y_i} =& \beta_1\hat{X_{it}} + \hat{u_{it}}, \text{(a.k.a., the within model)}\\
Y_{it} =& \beta_0 + \beta_1(X_{it} - \bar{X_i}) + \beta_2\bar{X_i} + u_{it}, \text{(a.k.a., the between model)}
\end{align}

---

## Fixed Effect

**Least Square with Dummy Variables** (LSDV)

Type I: 

$$Y_{it} = \beta_1X_{it} + \alpha_i + u_{it},$$ in which &alpha; is unit-specific mean differences (unit fixed effect). 

--

Type II: 

$$Y_it = \sum^T_{t = 1}\delta_tD_{t_i} + \beta_1X_{it} + u_{it},$$ in which $\delta_tD_{t_i}$ is the fixed effect for time.

---

## Issues of LSDV

1. Adding a lot of variables (risk of using out of the d.f.);
1. Additional high multicollinearity;
1. Losing time invariant variables;
1. Inefficient estimates of FE on binary and dependent variables.

???

For the last two concerns, considering using duration models.

--

Alternatives solutions:

1. Demeaning
1. Fixed effect vector decomposition (FEVD)

Con : 

1. Can't offer sufficient information (heterogeneous to what extent?).
1. Difficult to calculate substantive effects, e.g., first differences.

---

## Ultimate Solution

Modeling it, a.k.a., *Pan-Ta*~

Heteroscedasticity can be theoretically meaningful, esp., when researchers focus on how the variance changes. 

e.g., Assuming var(u<sub>i</sub>) = &sigma;<sup>2</sup>e<sup>&gamma;<sub>1</sub>X<sub>2i</sub></sup>. 

If &gamma;<sub>1</sub> = 0, then the model is homoscedastistic; otherwise, one has to use MLE to test H<sub>0</sub>: &gamma;<sub>1</sub> = 0.

---

## Multilevel Modeling

a.k.a., hierarchical modeling

Basic kind: (Two-Level) Random Intercept

\begin{align}
Y_{ij} = \beta_{0j}& + \beta_{1j}X_{ij} + \epsilon_{ij}, \epsilon_{ij}\sim N(0, \sigma^2)\\
       \beta_{0j}& = \gamma_{00} + \gamma_{01}Z_j + u_{0j}, u_{0j}\sim N(0, \tau^2)
\end{align}

Z is the group indicator.

--

**Intraclass correlation**: $\rho = \frac{\tau^2}{\sigma^2 + \tau^2}.$

???

Represent the similarity between individual and group level. 0 means no group effect.

---

Advanced kind: Random Slop

.pull-left[
\begin{align}
Y_{ij} = &\beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij}\\
       &\beta_{0j} = \gamma_{00} + \gamma_{01}Z_j + u_{0j}\\
       &\beta_{1j} = \gamma_{10} + u_{1j}.\\
\text{Assume}\left(
\begin{array}{c}
u_{0j}\\ u_{1j}\end{array}\right)&\sim BVN\left[\left(\begin{array}{c}
0\\0\end{array}\right), \left(\begin{array}{cc}
\tau_0^2 & \tau_0\tau_1\\ 
\tau_0\tau_1 & \tau_1^2
\end{array}\right)\right]
\end{align}
]

???

BVN: Bivariate Normal

The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together. Visually, the bivariate normal distribution is a three-dimensional bell curve.

--

.pull-right[

\begin{align}
\left(\begin{array}{cc}
\tau_0^2 & \tau_0\tau_1\\ 
\tau_0\tau_1 & \tau_1^2
\end{array}\right)
\end{align}

1. Identity: &tau;<sub>0</sub><sup>2</sup> = &tau;<sub>1</sub><sup>2</sup> = 1;
1. .red[Exchangeable: &tau;<sub>0</sub><sup>2</sup>= &tau;<sub>1</sub><sup>2</sup>;]
1. Independent: &tau;<sub>0</sub><sup>2</sup>&tau;<sub></sub><sup>2</sup> = 0;
1. Unstructured: no specific relationships is assumed for the &tau;s.
]

???

Covariate Matrix: 


---

## Wrap Up

1. Collinearity
    + Violation: Identification
    + Diagnosis: VIF
    + Solution: Hmm...
1. Heteroscedasticity
    + Violation: Homoscedasticity
    + Diagnosis: Park test, Goldfeld-Quandt test, Breusck-Pagan(-Godfreg) test, White test
    + Solution
        + Sandwich SE
        + Fixed Effect
        + Multilevel Model

