---
title: "Gauss-Markov Theorem"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
institution: "Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse, icons, xaringanExtra
) 


use_xaringan_extra(c("tile_view", # O
                     "broadcast",
                     "panelset",
                     "tachyons",
                     "fit_screen"))
use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = FALSE  #<<
)


# Functions preload
set.seed(313)

theme_set(theme_minimal())
```

## Overview

.center[Y<sub>i</sub> = &beta;<sub>0</sub> + &beta;<sub>1</sub>X + u<sub>i</sub>]

1. Gauss-Markov Theorem
1. BLUE
1. Parameter Distributions
1. OLS vis-&aacute;-vis ANOVA

---

class: inverse, bottom

# Gauss-Markov Theorem

---

## Classic Linear Regression Model (CLRM)

1. Linearity in the parameter;
1. Nonstochastic X ("given X," a.k.a., "X is fixed");
1. X has positive noninfinite variance (var(X));
1. Correct specification;
1. .red[Identification] (N > K; K = 2 for a simple OLS);
1. .red[Mean zero errors] (E(u<sub>i</sub>|X<sub>i</sub>) = 0);
1. .red[Exogeneity]: No covariance between X<sub>i</sub> and u<sub>i</sub> (E(X<sub>i</sub>u<sub>i</sub>) = cov(x<sub>i</sub>, u<sub>i</sub>) =0);
1. .red[No autocorrelation] (E(u<sub>i</sub>, u<sub>j</sub>|X<sub>i</sub>, X<sub>j</sub>) = cov(u<sub>i</sub>, u<sub>j</sub>|X<sub>i</sub>, X<sub>j</sub>) = 0, &forall; i, j);
1. Homoskedasticity (constant variance of u<sub>i</sub>, var(u<sub>i</sub>|X) = &sigma;<sup>2</sup>);
1. No perfect collinearity (when there are more than one X, &nexists; X<sub>i</sub> s.t., X<sub>i</sub> = a + b&sum;<sub>j = 1</sub>b<sub>j</sub>X<sub>j</sub>)


???

s.t. such that

homoskedasticity: residual randomly distributed in the same variance across the values of x

---

## When CLRM Assumptions Hold

.center[<img src = "images/clrm_comfortable.gif">]

.bg-black.golden.ba.shadow-5.ph4.mt3[
.center[**Gaussâ€“Markov Theorem**]

In a linear regression model in which the errors are uncorrelated, have equal variances and expectation value of zero, the .red[best linear unbiased estimator (BLUE)] of the coefficients is given by the ordinary least squares (OLS) estimator, provided it exists.
]

---

.center[<img src="images/clrm_blue.gif" height = 600 />]

---

## Path to BLUE

Computationally,

.center[<img src="images/clrm_ols.gif" height="400"/>]

--

Statistically, unbiased + consistent.

---

class: small

## Unbiasedness

$E(\hat\beta_1|X) = \beta_1$

Proof:

\begin{align}
E(\hat\beta_1|X) =& E[\frac{\sum(X - \bar X)(Y - \bar Y)}{\sum(X - \bar X)^2}|X]= E[\frac{\sum(X - \bar X)Y}{\sum(X - \bar X)^2}|X],\\
                 =& \frac{1}{\sum(X - \bar X)^2}E[\sum(X - \bar X)Y|X] = \frac{\sum(X - \bar X)}{\sum(X - \bar X)^2}E(Y|X),\\
                 =& \frac{\sum(X - \bar X)}{\sum(X - \bar X)^2}(\beta_0 + \beta_1X + u) = \frac{\sum(X - \bar X)}{\sum(X - \bar X)^2}(\beta_0 + \beta_1X),\\
                 =& \frac{1}{\sum(X - \bar X)^2}[\beta_0\sum(X - \bar X) + \beta_1X\sum(X - \bar X)],\\
                 =& \frac{\beta_1\sum(X - \bar X)X}{\sum(X - \bar X)^2}, \text{given} \sum(X - \bar X) = \sum X - \sum\bar X = 0,\\
                 =& \frac{\beta_1\sum(X - \bar X)(X - \bar X)}{\sum(X - \bar X)^2} =\beta_1.\blacksquare
\end{align}


---

## Consistency

1. $var(\beta_1) = \frac{\sigma^2}{\sum (X_i - \bar X)^2}$. So when N increases, $\sum (X_i - \bar X)^2$ increases. It results $var(\beta_1)$ decreasing
    + $\displaystyle{\lim_{n\to\infty}} var(\beta_1) = 0.$
1. $var(\hat \beta_0|X) =\sigma^2\frac{\sum X_i^2}{n\sum (X_i - \bar X)^2}=\sigma^2\frac{\sum X_i^2}{n\sum X_i^2 - n\bar X^2}$
    + When there's a $X_{n + 1}$, the nominator $\displaystyle{\sum^n_{i = 1}}(X_i^2 + X_{n + 1}^2)$; the denominator $(n + 1)\displaystyle{\sum^n_{i = 1}}(X_i^2 + X_{n + 1}^2) - (n + 1)\bar X^2$
    + The denominator increases quicker than the nominator.

---

## Distributions of the OLS Parameters

For a classic normal linear regression model:

.pull-left[
\begin{align}
u_i\sim& \text{i.i.d.} N(0, \sigma^2)\\
\hat\beta_1\sim& N(\beta_1, \frac{\hat\sigma^2}{\sum (X_i - \bar X)^2})\\
\hat\beta_0\sim& N(\beta_0, \frac{\hat\sigma^2\sum X_i^2}{n\sum (X_i - \bar X)^2})
\end{align}
]

--

.pull-right[
Let's standardize the variances: 

\begin{align}
\frac{\hat\beta_1 - \beta_1}{\sqrt{\frac{\hat\sigma^2}{\sum (X_i - \bar X)^2}}}\sim& N(0, 1^2)\\
\frac{\hat\beta_0 - \beta_0}{\sqrt{\frac{\hat\sigma^2\sum X_i^2}{n\sum (X_i - \bar X)^2}}}\sim& N(0, 1^2)\\
\frac{\hat\sigma^2}{\frac{\sigma^2}{n - 2}}\sim& \chi^2_{n - 2}
\end{align}
]

---

```{r chisq, fig.width=10, fig.height=7, fig.align='center'}
ggplot(data.frame(x = c(0, 10)), aes(x = x)) +
  stat_function(fun = function(x) dchisq(x, df = 2), aes(colour = "2")) +
  stat_function(fun = function(x) dchisq(x, df = 4), aes(colour = "4")) +
  stat_function(fun = function(x) dchisq(x, df = 6), aes(colour = "6")) +
  ylab("Probability Density") + 
  xlab("") +
  labs(color = "d.f.")
```

.center[&chi;<sup>2</sup>: adding up n squared normals, using to test the variance.]

---

class: inverse, bottom

# OLS vis-&aacute;-vis ANOVA

---

.panelset[

.panel[.panel-name[ANOVA Original]

| Source    	| Sum Square                                    	| d.f.  	| Mean Square                      	|
|-----------	|-----------------------------------------------	|-------	|----------------------------------	|
| Treat 	| $SST = \sum n_i (\bar X_i - \bar{\bar{X}})^2$ 	| K - 1 	| MST = SST/(K - 1)                	|
| Error     	| $SSE = \sum \sum (X_{ik} - \bar{X_i})^2$      	| N - K 	| MSE = SSE/(N - K)                	|
| Total     	| $SS = SST + SSE$                              	| N - 1 	| $F_{\alpha, K-1, N-1} = MST/MSE$ 	|

$$F_{\alpha, K-1, N-1} = MST/MSE$$

]

.panel[.panel-name[ANOVA CLRM]

|      	| $\sum(Y_i - \bar Y)^2$               	| $= \hat\beta_1^2(X_i - \bar X)^2$          	| $+ \sum\hat u_i^2$             	|
|------	|--------------------------------------	|--------------------------------------------	|--------------------------------	|
|      	| SST                                  	| SSE                                        	| SSR                            	|
| d.f. 	| n - 1                                	| 1                                          	| n - 2                          	|
| MSS  	| $\frac{\sum(Y_i - \bar Y)^2}{n - 1}$ 	| $\frac{\hat\beta_1^2\sum(X_i - \bar X)}{1}$ 	| $\frac{\sum\hat u_i^2}{n - 2}$ 	|

$\frac{MSS_{SSE}}{MSS_{SSR}} = \frac{\hat\beta_1^2(X_i - \bar X)^2\sim\chi^2}{\sigma^2\sim\chi^2}\sim F_{1, n - 2}$

$F_{1, n - 2}\sim\frac{\beta_1^2\sum(X_i - \bar X)^2}{\sigma^2}=\frac{\beta_1^2}{\frac{\sigma^2}{\sum(X_i - \bar X)^2}} = (\frac{\bar X - \mu}{\hat\sigma_X})^2$

As known, $\frac{\bar X - \mu}{\hat\sigma_X}\sim t$, therefore, F provides identical information as t.
]

]

???

n-1: Used out 1 to calculate $\bar Y$;
1: The only thing varies is $\hat\beta_1$;
n-2: Used out to calculate $\hat\beta_0, \hat\beta_1$.


Meaningless content:

\begin{align}
E[\hat\beta_1^2\sum(X_i - \bar X)|X] =& E\{[\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}]\sum(X_i - \bar X)^2\}\\
=& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}E[(\sum(X_i - \bar X)(Y_i - \bar Y))^2|X]\\
=& \frac{E\{[\sum(X_i - \bar X)Y_i]^2|X\}}{\sum(X_i - \bar X)^2}\\
=& \frac{E\{[\sum(X_i - \bar X)(\beta_0 + \beta_1X_i + u_i)]^2|X\}}{\sum(X_i - \bar X)^2}\\
=& \frac{E\{[\sum(X_i - \bar X)(\beta_1X_i + u_i)]^2|X\}}{\sum(X_i - \bar X)^2}\\
=& \frac{E\{[\beta_1\sum(X_i - \bar X)X_i + \sum(X_i - \bar X)u_i)]^2|X\}}{\sum(X_i - \bar X)^2}\\
=& \frac{E\{[\beta_1\sum(X_i - \bar X)(X_i - \bar X) + \sum(X_i - \bar X)u_i)]^2|X\}}{\sum(X_i - \bar X)^2}\\
=& \frac{E\{[\beta_1\sum(X_i - \bar X)^2 + \sum(X_i - \bar X)u_i)]^2|X\}}{\sum(X_i - \bar X)^2}\\
=& \frac{E\{[\beta_1\sum(X_i - \bar X)^2 + \sum(X_i - \bar X)u_i)]^2|X\}}{\sum(X_i - \bar X)^2}
\end{align}

\begin{align}
=& \frac{1}{\sum(X_i - \bar X)^2}\{E\{[\beta_1\sum(X_i - \bar X)^2]^2|X\} + E\{[\sum(X_i - \bar X)u_i)]^2|X\}\}\\
&+ E\{2\beta_1\sum(X_i - \bar X)^2\sum(X_i - \bar X)u_i)|X\}\\
=& \frac{1}{\sum(X_i - \bar X)^2}\{[\beta_1\sum(X_i - \bar X)^2]^2 + E\{[\sum(X_i - \bar X)u_i)]^2|X\}\\
&+ 2\beta_1\sum(X_i - \bar X)^2\sum(X_i - \bar X)E(u_i|X)\}\\
=& \frac{[\beta_1\sum(X_i - \bar X)^2]^2 + \sum(X_i - \bar X)^2\sigma^2 + 0}{\sum(X_i - \bar X)^2}\\
=& \beta_1^2\sum(X_i - \bar X)^2 + \sigma^2
\end{align}


nominator transformation

Ignore $\beta_0$ for now

Then $F_{1, n - 2}\sim\frac{\hat\beta_1^2(X_i - \bar X)^2}{\sigma^2} = \frac{\beta_1^2\sum(X_i - \bar X)^2 + \sigma^2}{\sigma^2} = \frac{\beta_1^2\sum(X_i - \bar X)^2}{\sigma^2} + 1$


$[\beta_1\sum(X_i - \bar X)]^2$, constant

---

class: inverse, bottom

# <img src = "images/ci_fsmrof.png" height = 60> OLS in Linear Algebra

---

## Elementary to Linear Algebra

\begin{align}
Y_i =& \beta_0 + \beta_iX_i + \epsilon_i\\
\boldsymbol{Y} =& \boldsymbol{X\beta} + \boldsymbol{\epsilon}\\
\left(\begin{array}{c}
Y_1\\
Y_2\\
\vdots\\
Y_n\end{array}\right)=& 
\left(\begin{array}{cc}
1 & X_1\\
1 & X_2\\
\vdots & \vdots\\
1 & X_n\end{array}\right) 
\left(\begin{array}{c}
\beta_1\\
\beta_2\\
\vdots\\
\beta_n\end{array}\right) +
\left(\begin{array}{cc}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n\end{array}\right)
\end{align}

.center[
* **Y**: Response vector;
* **X**: Design matrix;
* **&beta;**: Parameter vector;
* **&epsilon;**: Error vector;
]

???

the X is a diagonal matrix, writing in such a format showing each line has one single element

---

## Covariance Matrix

Accroding to the .red[homoscedasiticity] assumption of OLS, the covariance matrix of the error is:

$$\sigma^2\{\epsilon\}_{n\times n} = \sigma^2\boldsymbol{I}_{n\times n} = \sigma^2\{\boldsymbol{Y}\}_{n\times n}.$$

In other words, $\epsilon\sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I})$.

---

## Estimators

Goal: Finding the &beta; minimizing the squared residuals

$$\sum\epsilon^2 = \boldsymbol{\epsilon'\epsilon} = (\boldsymbol{Y} - \boldsymbol{X}\beta)'(\boldsymbol{Y} - \boldsymbol{X}\beta)$$

Then, seek for the value of &beta; that lets the derivative of the above equation respected of &beta; to be 0.

\begin{align}
\hat\beta =& (\boldsymbol{X'X})^{-1}\boldsymbol{X'Y}.\\
var(\beta) =& \sigma^2\boldsymbol{X'X})^{-1}, \text{where}\ \sigma^2 = \frac{\boldsymbol{\epsilon'\epsilon}}{n - k}.
\end{align}

???

' means transpose (exchanging the rows and columns)

---

## Differential Rules for Linear Algebra

How to conduct derivatives for matrix: 

\begin{align}
\frac{\boldsymbol{a'b}}{\boldsymbol{b}} =& \frac{\boldsymbol{b'a}}{\boldsymbol{b}} = \boldsymbol{a}\\
\frac{\boldsymbol{b'Ab}}{\boldsymbol{b}} =& 2\boldsymbol{Ab} = 2\boldsymbol{b'A}
\end{align}

**A** is an arbitrary symmetric matrix


According to the above rules,

\begin{align}
\frac{d2\boldsymbol{\beta'X'Y}}{\boldsymbol{\beta}}=& \frac{d2\boldsymbol{\beta'(X'Y)}}{\boldsymbol{\beta}} = 2\boldsymbol{X'Y}\\
\frac{d2\boldsymbol{\beta'X'X\beta}}{\boldsymbol{\beta}} =& \frac{d2\boldsymbol{\beta'(X'X\beta)}}{\boldsymbol{\beta}} = 2\boldsymbol{X'X\beta}
\end{align}


---

class: small

## $\hat\beta = \beta$
.pull-left[
\begin{align}
\boldsymbol{\epsilon\epsilon'}=& \frac{d(\boldsymbol{Y} - \boldsymbol{X}\beta)'(\boldsymbol{Y} - \boldsymbol{X}\hat\beta)}{d\hat\beta} = 0,\\
0 =& -2\boldsymbol{X'(\boldsymbol{Y} - \boldsymbol{X}\hat\beta)},\\
\boldsymbol{X'Y} =& \boldsymbol{X'X}\hat\beta,\\
\hat\beta =& (\boldsymbol{X'X})^{-1}\boldsymbol{X'Y}.\\
\text{Within this}, \boldsymbol{X'X} =& \left(\begin{array}{cc}
n & \sum X_i\\
\sum X_i & \sum X_i^2
\end{array}\right),\\
\Rightarrow (\boldsymbol{X'X})^{-1} =& \frac{\left(\begin{array}{cc}
\sum X_i^2 & -\sum X_i\\
-\sum X_i & n
\end{array}\right)}{nS_X},\\
\boldsymbol{X'Y} =& \left(\begin{array}{c}
\sum Y_i\\
-\sum X_iY_i
\end{array}\right).\\
\end{align}
]

.pull-right[
\begin{align}
\text{then, } E(\hat\beta) =& [\boldsymbol{(X'X)^{-1}X'}](\boldsymbol{X}\beta + \epsilon),\\ 
=& [\boldsymbol{(X'X)^{-1}X'X}\beta] + [\boldsymbol{(X'X)^{-1}X'\epsilon}],\\
          =& \beta.\\
var(\beta) =& \sigma^2(X'X)^{-1}, \text{where}\ \sigma^2 = \frac{\epsilon'\epsilon}{n - k}.\blacksquare
\end{align}

**X**<sup>-1</sup>: Inverse matrix;
**X**': Transposition.
]

---

## $\boldsymbol{X'\epsilon} = 0$

\begin{align}
\boldsymbol{X'Y} =& \boldsymbol{X'X}\hat\beta,\\
\boldsymbol{X'(X\hat\beta + \epsilon)} =& \boldsymbol{X'X}\hat\beta,\\
\boldsymbol{X'\epsilon} =& 0.\blacksquare
\end{align}

---

## Residual

For the predicted Y, $$\hat{\boldsymbol{Y}} = \boldsymbol{X}\beta = \boldsymbol{X(X'X)^{-1}X'Y} = \boldsymbol{[X(X'X)^{-1}X']Y},$$

$H = [X(X'X)^{-1}X']$ is called the hat matrix.

Then, $$\epsilon = \boldsymbol{Y} - \hat{\boldsymbol{Y}} = \boldsymbol{Y} - \boldsymbol{HY} = \boldsymbol{(I - H)Y}.$$

--

Two properties of **H**:

1. Symmetric: $\boldsymbol{H = H'};\boldsymbol{(I - H) = (I - H)'}$
2. Idempotent: $\boldsymbol{H^2 = H; (I - H)(I - H) = (I - H)}$

???
Idempotent: å¹‚ç­‰


---

## Wrap Up

1. Gauss-Markov Theorem: "Ten Commandments"
1. Path to BLUE
    + Unbiaseness
    + Consistency
1. Parameter Distributions
    + Normal for &beta;s
    + &chi;<sup>2</sup> for &sigma;<sup>2</sup>
1. OLS vis-&aacute;-vis ANOVA
    + F and t are consistent
1. OLS in Linear Algebra: Language of statisticians

---

<iframe src="https://player.bilibili.com/player.html?aid=842607310&bvid=BV1E54y1r76o&cid=248007878&page=2" scrolling="no" border="0" frameborder="yes" framespacing="0" allowfullscreen="true" height = "600" width = "1000"> </iframe>

