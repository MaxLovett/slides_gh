---
title: "Simple OLS and Properties of Estimators"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
knitr: 
    opts_chunk: 
      echo: false
editor_options: 
  chunk_output_type: console
format: 
  revealjs:
    
    number-sections: true
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: true
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
revealjs-plugins:
  - spotlight
# lightbox: auto
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---


## Overview {.unnumbered}

```{r setup, include = FALSE}

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools,
  greekLetters,
  XICOR
) 


# Functions preload
set.seed(114)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```


1. Road to OLS
1. OLS Components

# Road to OLS

## Where are We

Goal: Understanding data patterns in terms of [random variable]{.red}

:::: {.columns}

::: {.column width="50%"}
*Path through*

1. Probability theory
    - PDF/CDF
    - Confidence intervals/p-values
1. Description
    - Single: Moments
    - Relation: Association
1. Hypothesis testing
    - Hypothesis setup
    - Statistical inference

:::

::: {.column width="50%" style="text-align:center"}

:::{.fragment}
*Path forward* 

Linear (OLS)

- Simple linear regression
- Multiple regression
:::

:::{.fragment}
&darr;

Generalized linear

- MLE(Binary/Ordered/Nominal)
:::


:::{.fragment}
&darr;

Nonlinear

- Moderation (& mediation)
:::

:::

::::



## Review: Expectation

:::: {.columns}

::: {.column width="37%"}
\begin{align}
E(X)_{disc.} =& \sum^n_{i = 1} X_if(X_i).\\
E(X)_{cont.} =& \int^{+\infty}_{-\infty}X_if(X_i)dX,\\
    =& \mu.\\
\sigma^2 =& E[X - E(X)]^2, \\
=& E[(X - \mu)^2],\\
=& E(X^2 - 2X\mu + \mu^2),\\
=& E(X^2) - 2\mu E(X) + \mu^2,\\
=& E(X^2) - \mu^2.
\end{align}
:::

::: {.column .fragment width="63%"}

\begin{align}
cov(X, Y) =& \sum(X - \mu_X)(Y - \mu_Y)p(X, Y),\\
          =& E[X - E(X)][Y - E(Y)],\\
          =& E[XY - X\cdot E(Y) -\\
          &Y\cdot E(X) + E(X)E(Y)],\\
          =& E(XY) - E(Y)E(X) -\\
          &E(X)E(Y) + E(X)E(Y),\\
          =& E(XY) - E(X)E(Y).
\end{align}

:::

::::


## Review (Continued): Variance

:::{.fragment}
\begin{align}
var(aX + b) =& E[(aX + b) - E(aX + b)]^2 = E[aX - aE(X)]^2, \\
=& a^2E[X - E(X)]^2 = a^2var(X).
\end{align}
:::

:::{.fragment}
$$var(aX_1 + bX_2+ c) = a^2var(X_1) + b^2var(X_2) + 2ab\cdot cov(X_1, X_2).$$
:::

:::{.fragment}
When X<sub>i</sub> is i.i.d., 
$$var(a_1X_1 + a_2X_2 + ... + a_nXn) = var(\sum a_iX_i) = \sum a_i^2var(X_i).$$
:::


:::{.fragment}
$$var(\bar X) = var(\frac{\sum X_i}{n}) = \sum^n_{i = 1}\frac{var(X_i)}{n^2} = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$$
:::


:::{.callout-note .fragment}
## Comparison of deviation measurements

Pop: $\sigma^2 = \frac{\sum (X_i - \mu)^2}{N}$; Sample: $s^2 = \frac{\sum (X_i - \bar{X})^2}{\color{darkred}{n - 1}}$; Sample means: $\text{Var}(\bar{X}) = \frac{\sigma^2}{n}$; $\text{SE}(\bar{X}) = \frac{\sigma}{\sqrt{n}}.$
:::

:::{.notes}
1. When calculating the population mean, &mu;, we have the full information. The mean is merely the mean; so does it when calculating the sample mean.
1. When calculating the sample variance, however, our goal is to estimate the population, with the sample mean as a reference.
1. However, the sample mean introduces bias because $\bar{X}$ tends to be closer to the individual data points than the true population mean ($\mu$).
1. We use $n - 1$ to reduce the influence of this bias. The method is called Bessel's correction, which compensates for the underestimation of variability due to using $\bar{X}$. This adjustment gives a more accurate estimate of the population variance.
1. When calculating the variances of means of a linear combination of random variables ($var(a_1X_1 + a_2X_2 + ... + a_nXn)$), we do not estimate the population but being interested in how the mean of a sample varies, which is inherently different from measuring the variance within the sample data itself.
1. In SE calculation, we include the correciton in &sigma; as the nominator, the $\sqrt{n}$ reflects merely the number of independent observations in the sample. It uses $n$ in the denominator because it reflects the precision of the sample mean estimate, considering the combined information from all $n$ observations.
:::

## Review (Continued): Property of a good estimator $\hat \theta$

* Unbiased: On average, the estimator gives the right answer, formally, $E(\hat\theta) = \theta.$
* Consistent: As the sample size increases, the variance decreases.
* Efficiency: Smallest variance among *unbiased* estimators^[[![](https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png){height=30} RMES: Root mean square error, $\sqrt{bias^2 + var}$.]{.fragment}]

:::{.fragment}
```{r efficiency}
#| fig.align: 'center'
#| fig-height: 3

ggplot(data.frame(x = c(-7, 7)), aes(x = x)) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 0.5), aes(color = "0.5")) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1), aes(color = "1")) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 2), aes(color = "2")) +
  ylab("Probability\n Density") + 
  xlab("X") +
  labs(color = "SD")
```
:::

:::{.notes}
Sometimes, one may want to trade off a little bias against variance.

**Root Mean Square Error (RMSE)** is widely used across various fields, especially in statistics, machine learning, and predictive modeling, where accuracy of estimates or predictions is critical. Here are some key scenarios where RMSE is commonly used or referred to:

### 1. **Model Evaluation in Machine Learning and Regression Analysis**
   - **Predictive Accuracy**: RMSE is frequently used to assess the accuracy of models by measuring the average difference between predicted and observed values. For example, in linear regression, it shows how well the regression line fits the data.
   - **Comparing Models**: RMSE is used to compare the performance of multiple models on the same data. Lower RMSE values indicate better predictive accuracy, making it a convenient metric for model selection.
   - **Cross-Validation and Hyperparameter Tuning**: During cross-validation or tuning of model hyperparameters, RMSE is used as a scoring metric to optimize the model.

### 2. **Forecasting and Time Series Analysis**
   - **Evaluating Forecast Accuracy**: In forecasting, such as demand prediction or stock price forecasting, RMSE measures how close predicted values are to actual outcomes. It is one of the primary metrics to evaluate models like ARIMA, exponential smoothing, or other time series models.
   - **Measuring Seasonal Adjustments**: RMSE can be used to check the accuracy of seasonal adjustments and trend components in time series data.

### 3. **Statistical Estimation**
   - **Assessing Estimator Quality**: RMSE is valuable for evaluating estimators in cases where both bias and variance are of concern. It provides a comprehensive measure of estimation error, especially when working with biased estimators.
   - **Choosing Between Biased and Unbiased Estimators**: RMSE allows for comparing biased and unbiased estimators on an equal footing by quantifying the total error (bias + variance) of each estimator. 

### 4. **Quality Control and Engineering**
   - **Process Control**: In manufacturing or engineering, RMSE can measure deviations of observed measurements from target values, providing insight into the precision and accuracy of processes.
   - **Signal Processing**: RMSE is often used in signal processing to quantify the error between processed signals and the original signal, particularly when reducing noise.

### 5. **Environmental and Climate Science**
   - **Model Validation**: Climate models, weather forecasts, and environmental simulations often rely on RMSE to assess the accuracy of predicted temperatures, precipitation levels, or pollution concentrations.
   - **Spatial Accuracy**: In geographic and environmental studies, RMSE can evaluate the spatial accuracy of geospatial models and remote sensing measurements.

### 6. **Econometrics and Financial Modeling**
   - **Evaluating Financial Models**: RMSE is used to assess the accuracy of econometric models in forecasting economic indicators, like GDP growth or inflation rates, by comparing predictions to actual outcomes.
   - **Risk Assessment**: In finance, RMSE helps quantify the accuracy of risk models by showing the average deviation between predicted and actual returns or risk measures.

### 7. **Deep Learning and Neural Networks**
   - **Loss Function**: RMSE (or mean squared error, MSE) is often used as a loss function in regression-based neural networks to minimize the difference between predicted and target values.
   - **Evaluation Metric**: For neural network models performing regression tasks, RMSE is used as an evaluation metric to interpret prediction accuracy on test data.

### Why RMSE is Preferred in Many Applications
- **Sensitivity to Large Errors**: RMSE penalizes larger errors more than smaller errors (due to squaring), making it particularly useful when large errors are more detrimental in the application.
- **Interpretability**: Since RMSE is in the same units as the data, it’s easy to interpret and communicate results.

In summary, RMSE is a versatile metric used in any situation where quantifying the total error (combining bias and variance) in predictions or estimates is essential, providing a clear, interpretable measure of model accuracy.
:::

## Ordinary Least Squares (OLS)

:::{.fragment}
One type of [simulation]{.red} of the reality among many others (unnecessarily the best one).
:::

:::{.r-hstack}
![](https://drhuyue.site:10002/sammo3182/figure/ols_leastSquare_correct.gif){.fragment height=350}
![](https://drhuyue.site:10002/sammo3182/figure/ols_leastSquare.gif){.fragment height=350}
:::

:::{.callout-important .fragment}
Interpretation: How does variable Y change on average [when]{.red} some explanatory variable X changes (while the others don't)? 
:::

:::{.notes}

![](https://drhuyue.site:10002/sammo3182/figure/ols_illustrateSimpleOLS.png)

:::

# OLS Component

## Linearity

:::{style="text-align:center"}
*Which X has the linear relationship with Y?*
:::

\begin{align}
Y_i =& \beta_0 + \beta_1X_i + \epsilon_i;\\
Y_i =& \frac{1}{\beta_0} + \frac{X_i}{\beta_1} + \epsilon_i;\\
Y_i =& \beta_0 + \beta_1ln(X_i) + \epsilon_i;\\
Y_i =& \beta_0 + \frac{\beta_1}{X_i} + \epsilon_i;\\
Y_i =& \beta_0 + X_i^{\beta_1} + \epsilon_i.\\
\end{align}



:::{.notes}
- First two are linear.

![](https://drhuyue.site:10002/sammo3182/figure/ols_linearity.png)

:::


## OLS Formally^[[![](https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png){height=30} &beta;: Estimand/parameter, function: estimator, result: estimate]{.fragment}]

\begin{align}
Y_i =& \beta_0 + \beta_1X_1 + \epsilon_i.\\
E(Y_i|X_i) =& E(\beta_0 + \beta_1X_1 + \epsilon_i|X_i),\\
           =& E(\beta_0|X_i) + E(\beta_1X_1|X_i) + E(\epsilon_i|X_i),\\
           =& \beta_0 + \beta_1X_i + E(\epsilon_i|X_i).
\end{align}


:::{.notes}
- Estimand: Parameter in the population which is to be estimated in a statistical analysis
- Estimator: A rule for calculating an estimate of a given quantity based on observed data. Function of the observations, i.e., how observations are put together
- Estimation: The process of finding an **estimate**, or approximation, which is a value that is usable for some purpose even if input data may be incomplete, uncertain, or unstable (value derived from the best information available)
:::


:::{.fragment}
&epsilon<sub>i</sub>: Things we can't explain but [hope]{.red} to be zero.
:::

:::{.notes}
$\beta_1X_i$ consistent

E(\epsilon_i|X_i) assumed 0
:::

:::{.fragment}
**Sample Regression Function (SRF)**

\begin{align}
Y_i =& (\hat\beta_0 + \hat\beta_1X_i) + \hat \epsilon_i,\\
    =& \hat Y_i + \hat \epsilon_i.\\
    =& E(Y_i|X_i) + \epsilon_i,\\
\Leftrightarrow \epsilon_i =& Y_i - E(Y_i|X_i).
\end{align}
:::

## Minimizing Sum of Squares

![](https://drhuyue.site:10002/sammo3182/figure/ols_minimizeSS.gif){fig-align="center" height=600}

## A Good Estimator &rarr; Minimizing Expected $\epsilon$ {.smaller}

&beta;<sub>0</sub> and &beta;<sub>1</sub> that make $\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2 = 0$


\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_1} = -\sum 2X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i) =& 0;\\
\Leftrightarrow \sum X_iY_i - \sum X_i\hat\beta_0 - \sum\hat\beta_1 X_1^2 =& 0;\\
(\sum X_iY_i =& \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2);\\
\sum X_iY_i - \sum X_i(\bar Y - \hat\beta_1\bar X) - \sum\hat\beta_1 X_1^2 =& 0;\\
\sum X_iY_i - \sum X_i\bar Y + \sum X_i\hat\beta_1\bar X - \sum\hat\beta_1 X_1^2 =& 0;\\
\sum X_i(Y_i - \bar Y) + \hat\beta_1\sum X_i(\bar X - X_i) =& 0;\\
\sum X_i(Y_i - \bar Y) =& \hat\beta_1\sum X_i(X_i - \bar X);\\
\hat\beta_1 =& \frac{\sum X_i(Y_i - \bar Y)}{\sum X_i(X_i - \bar X)}.
\end{align}

---

A transformation gadget:

\begin{align}
\sum X_i(Y_i - \bar Y) =& \sum X_i(Y_i - \bar Y) - \bar X(n\bar Y - n\bar Y)\\
                       =& \sum X_i(Y_i - \bar Y) - \bar X(\sum Y_i - \sum\bar Y)\\
                       =& \sum X_i(Y_i - \bar Y) - \bar X\sum (Y_i - \bar Y)\\
                       =& \sum (X_i - \bar X)(Y_i - \bar Y) \blacksquare
\end{align}

:::{.notes}
 "Quod Erat Demonstrandum" (QED) which loosely translated means "that which was to be demonstrated".
:::

:::{.fragment}
\begin{align}
\hat\beta_1 =& \frac{\sum X_i(Y_i - \bar Y)}{\sum X_i(X_i - \bar X)}, \text{using the above}\\
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}.
\end{align}
:::

## $\beta_1$

:::{.fragment}
\begin{align}
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2},\\
        =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum (X_i - \bar X)^2}\color{darkred}{\sqrt{\sum (Y_i - \bar Y)^2}}}\cdot\frac{\color{darkred}{\sqrt{\sum (Y_i - \bar Y)^2}}}{\sqrt{\sum (X_i - \bar X)^2}},\\
        =& r_{X, Y}\frac{s_Y}{s_X}.
\end{align}
:::

:::{.callout-tip .fragment}
So, when the variance of Y(s<sub>Y</sub>) increases, &beta;<sub>1</sub> increases.
:::

:::{.fragment}
A special case: Standardized X and Y, i.e., $s_Y, s_X$ are 1s, then,

$$\beta_1 = r_{X, Y}\frac{s_Y}{s_X} = r_{X,Y}.$$

:::


:::{.notes}
&beta;<sub>0</sub> is efficient (minimizng the u) depending on &beta;<sub>1</sub>
:::

## Linear nature of the OLS coefficient^[$\sum X_i(Y_i - \bar Y) = \sum(X_i - \bar X)(Y_i - \bar Y).$]
:::: {.columns}

::: {.column width="50%"}
\begin{align}
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2},\\
            =& \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)Y_i,\\
            =& \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)(\beta_0 + \beta_1X_i + \epsilon_i),\\
            =& \frac{1}{\sum (X_i - \bar X)^2}[\sum (X_i - \bar X)(\beta_0 + \beta_1X_i) + \sum (X_i - \bar X)\epsilon_i],\\
            =& \frac{\sum (X_i - \bar X)(\beta_0 + \beta_1X_i)}{\sum (X_i - \bar X)^2} + \frac{\sum (X_i - \bar X)\epsilon_i}{\sum (X_i - \bar X)^2}.
\end{align}
:::

::: {.column .fragment width="50%"}
Let $k_i=\frac{X_i - \bar X}{\sum (X_i - \bar X)^2},$ then $\hat\beta_1 = \sum k_i\beta_0 + \sum k_i X_i\beta_1 + \sum k_i\epsilon_i.$ 

+ A [linear]{.red} combination with errors
+ Min/max(X) influences a lot.

:::{.notes}
$\sum(X_i - \bar X) = 0.$

β1=constant+β1constant+variation due to error

A special case: $\hat\beta_1 = \beta_1 + \sum k_i\epsilon_i$.
:::
:::

::::


## Var($\beta_1$){.smaller}

:::: {.columns}

::: {.column width="30%"}

$\hat\beta_1 = \sum k_i\beta_0 + \sum k_i X_i\beta_1 + \sum k_i\epsilon_i.$ 

:::{.fragment}
\begin{align}
\sigma^2 =& var(\epsilon_i|X),\\
         =& var(Y_i - \hat\beta_0 - \hat\beta_1X|X),\\
         =& \frac{\sum(\hat \epsilon_i^2)}{n - 2},\\
         =& \hat\sigma^2.
\end{align}
:::

:::{.notes}
β1=constant+β1constant+variation due to error

n - 2: $\hat\beta_0, \hat\beta_1$, also the extension of Bessel's correction in regression
:::

:::




::: {.column .fragment width="70%"}
\begin{align}
var(\hat \beta_1|X) =& var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X)\\
                  =& var(\beta_1 + \sum k_i\epsilon_i|X)\\
                  =& var(\sum k_i\epsilon_i|X), \text{given}\ \beta_1\sum k_iX_i \text{constant}\\
                  =& \sum var(k_i\epsilon_i|X), \text{assuming}\ \epsilon_i\ \text{independent}\\
                  =& \sum k_i^2 var(\epsilon_i|X)\\
                  =& \sum[\frac{X_i - \bar X}{\sum (X_i - \bar X)^2}]^2\sigma^2\\
                  =& \frac{\sum(X_i - \bar X)^2}{[\sum (X_i - \bar X)^2]^2}\sigma^2 = \frac{\sigma^2}{\sum (X_i - \bar X)^2}
\end{align}
:::
::::

:::{.callout-important .fragment .large}
## Assumption is IMPORTANT!!!

If &epsilon;<sub>i</sub> is not independent, then cov(k, &epsilon;) > 0, and this estimator is underestimated.
:::


## $\beta_0$

\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =& -\sum 2(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0\\
\Leftrightarrow\sum Y_i - \sum\hat\beta_0 - \sum\hat\beta_1X_i =& 0\\
\sum Y_i =& \sum\hat\beta_0 + \sum\hat\beta_1X_i\\
                         =& n\hat\beta_0 + \hat\beta_1\sum X_i\\
\hat\beta_0 =& \frac{\sum Y_i}{n} - \hat\beta_1\frac{\sum X_i}{n} = \bar Y - \hat\beta_1\bar X
\end{align}


:::{.callout-note .fragment}
## Normal equations

\begin{align}
\sum Y_i =& n\hat\beta_0 + \hat\beta_1\sum X_i\\
\sum X_iY_i =& \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2
\end{align}
:::

:::{.notes}
$\hat\beta_0, \hat\beta_1$ are called [co]{.red}efficients.
:::


## Var($\beta_0$){.smaller}

\begin{align}
var(\hat \beta_0|X) =& var(\bar Y - \hat\beta_1\bar X|X),\\
                   =& var[\frac{\sum(\beta_0 + \beta_1X_i + \epsilon_i)}{n} - \hat\beta_1\bar X|X],\\
                  \because \beta_0 +& \beta_1X_i\text{ is constant & independent},\\
                  =& var(\frac{\sum \epsilon_i}{n}|X) + var(\hat\beta_1\bar X|X), \\
                  =& var(\frac{\sum \epsilon_i}{n}|X) + var(\hat\beta_1\bar X|X) = \frac{var(\sum \epsilon_i|X)}{n^2} + \bar X^2var(\hat\beta_1|X),\\
                  =& \frac{n\sigma^2}{n^2} + \frac{\bar X^2\sigma^2}{\sum (X_i - \bar X)^2} = \sigma^2[\frac{1}{n} + \frac{\bar X^2}{\sum (X_i - \bar X)^2}],\\
                  =& \sigma^2[\frac{\sum (X_i - \bar X)^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}] = \sigma^2[\frac{\sum X_i^2 - \sum\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}],\\
                  =& \sigma^2[\frac{\sum X_i^2 - n\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}]= \sigma^2\frac{\sum X_i^2}{n\sum (X_i - \bar X)^2}.
\end{align}



## Characteristics of OLS Coefficients

:::: {.columns}

::: {.column width="50%"}
1. Calculated using [observed]{.red} data
1. Unique point estimates
1. SRF passes through $(\bar X, \bar Y)$
1. $\bar{\hat Y} (\text{predicted}) = \hat Y (\text{observed})$, $\frac{\sum\hat Y_i}{n} = \frac{\sum Y_i}{n}$
1. $\bar{\hat \epsilon_i} = \frac{\sum{\hat \epsilon_i}}{n} = 0$
1. $\sum X_i\hat \epsilon_i = 0$
:::


::: {.column .fragment width="50%"}

\begin{align}
cov(X_i, \epsilon_i) =& 0,\\
\frac{\sum(X_i - \bar X)(\hat \epsilon_i - \bar{\hat \epsilon_i})}{n-1} =& 0,\\
\frac{\sum X_i\hat \epsilon_i}{n-1} =& 0. \blacksquare
\end{align}


:::{.notes}
\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =& 0\\ -2\sum[Y_i - (\hat{\beta_0} + \hat{\beta_1}X_i)] =& 0\\
\sum (Y_i-\hat Y_i) = \sum{\hat \epsilon_i} =& 0\\
\Rightarrow \frac{\sum{\hat \epsilon_i}}{n} = \bar{\hat \epsilon_i} =& 0 \blacksquare
\end{align}
:::

:::
::::



## Take-home point

::: {style="text-align: center"}
![](https://drhuyue.site:10002/sammo3182/figure/ols_mindmap.png){height="600"}
:::

# Appendix {.appendix .unnumbered}

## Stretch {.unnumbered}

{{< video https://drhuyue.site:10002/sammo3182/video/relax.mp4 title="Yoyo孙佳祺:拯救久坐党体态！办公间隙的5分钟拉伸，重新挺拔起来～！" height=600 preload="auto" controls allowfullscreen>}}

:::{.notes}
https://www.bilibili.com/video/BV1E54y1r76o/?buvid=Y045340D2FD9FA3A46419EEFE4578279ECBD&from_spmid=main.space-search.0.0&is_story_h5=false&mid=7RnjBONLRMus4FQZFBWD2g%3D%3D&p=2&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=79FE08C9-F2B1-4C18-A16C-B31179817B42&share_source=WEIXIN&share_tag=s_i&timestamp=1728732278&unique_k=skSWw7j&up_id=390316092&vd_source=f38aeefd0d38cecba9017eeee43e71c8
:::

## Meditation {.unnumbered}

:::{style="text-align:center; margin-top: 2em"}
![松茸的世界：5分钟正念冥想-自信之心](https://drhuyue.site:10002/sammo3182/figure/ci_songrong.jpg){fig-align="center" height=400}

<audio controls preload="auto" src="https://drhuyue.site:10002/sammo3182/video/meditation.m4a" width=900></audio>
:::


:::{.notes}
https://www.xiaoyuzhoufm.com/episode/65b752ed0bef6c207457f51b?s=eyJ1IjogIjYwNDA1OGJiZTBmNWU3MjNiYmY3MjZiZSJ9
:::