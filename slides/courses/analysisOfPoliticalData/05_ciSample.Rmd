---
title: "Confidence Intervals and Distribution Comparison"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
institution: "Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable, drhutools,
  knitr, # dependency
  descr, stringr, broom, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(313)

theme_set(theme_minimal())
```


## Overview

1. Confidence Intervals
1. Distribution Comparison
1. Hypothesis Testing


---

class: inverse, bottom

# Confidence Intervals

---

## Samples Properties

.left-column[
### Finite ~
]

.right-column[
+ **Unbiasedness**: Produce the .red[right] answer on coverage (üòµ).
    + E(p) = &pi;.
    + How to? Randomization, weight
]

--

.right-column[
+ **Efficiency**: smaller variance of .magenta[unbiased estimators]
    + How to increase efficiency? Hint: $SE = \frac{\sigma}{\sqrt{n}}$
]

--

.right-column[
<img src = "images/ci_fsmrof.png" height = 60> For a normal distribution with &sigma;<sup>2</sup> = 1, the median = &pi;/2n &approx; 1.57/n > 1/n (mean), so for a standard normal, mean is *1.57 times more* effiicent than median.
]

???

Unbiasedness: the expectation is the true value

How to achieve unbiasedness: 
1. Design: Randomization(prior), why? no confounders
1. Post-dgp: weight(post), manually collect, ËÄÅÂ∞ÜÂá∫È©¨

The median will generally be better than the mean if there are heavy tails, while the mean will be best with light tails.

https://stats.stackexchange.com/questions/136671/for-what-symmetric-distributions-is-sample-mean-a-more-efficient-estimator-tha

FSM: Flying Spaghetti Monster


---

## Properties of Samples

.left-column[
### Finite ~
### Large ~
]

.right-column[

+ Convergence

.center[<img src="images/ci_convergence.gif" height = 200 />]

When a sequence of random variables stabilizes to a certain probabilistic behavior as n &rarr; &infin; , the sequence is said to show stochastic convergence.

$$p\lim_{n \to \infty}X_n = a, a\in R.$$

]

---

## Properties of Samples

.left-column[
### Finite ~
### Large ~
]

.right-column[

+ Convergence

*Two Views of Convergence*

1. In *probability*: Values in the sequence eventually take a .red[constant value] (i.e. the limiting distribution is a point mass).

1. In *distribution*: Values in the sequence continue to vary, but the variation eventually comes to follow an .red[unchanging distribution] (i.e. the limiting distribution is a well characterized distribution)

]

---

## Properties of Samples

.left-column[
### Finite ~
### Large ~
]

.right-column[

+ .gray[Convergence]
+ Consistency

An estimator q<sub>n</sub> is consistent if the sequence q<sub>1</sub>...q<sub>n</sub> converges in probability to the true parameter value &theta; as sample size n grows to infinity:

$$p\lim_{n \to \infty}\hat{\theta}_n = \theta.$$

* .red[Minimal] requirement for estimators
* May perform .red[badly] in small samples

Only if a sequence of estimators is .red[unbiased and converges to a value], then it is consistent, as it must converge to the correct value.
]

---

class: center, middle

## Does Unbiasedness Imply Consistency?

Let's estimate the mean height of our university. To do so, we randomly draw a sample from the student population and measure their height. 
Then what estimator should we use: 
1. The mean height of the sample;
1. The height of the student we draw first.

--

Of course, the meanÔºÅ   
You "leave it here, leave it here"?

--

.red[BUT,]    
Both estimator are **unbiased** (üò±!!!)   

E(X&#772;) = E(X<sub>1</sub>) = &mu;;

Var(X1)=&sigma;<sup>2</sup> forever.

???

‰Ω†ÊêÅËøôÊêÅËøôÂì™Ôºü

Why the mean? 

Appendix C of Introductory Econometrics by Jeffrey Wooldridge

Say we want to estimate the mean of a population. While the most used estimator is the average of the sample, another possible estimator is simply the first number drawn from the sample. This estimator is unbiased, because  E(X1)=Œº  due to the random sampling of the first number. 

Yet the estimator is not consistent, because as the sample size increases, the variance of the estimator does not reduce to 0. Rather it stays constant, since Var(X1)=&sigma;2 , which the population variance, again due to the random sampling. 
The additional information of an increasing sample size is simply not accounted for in this estimator.


---

## Does Consistency Imply Unbiasedness?

--

.center[
<img src = "images/ci_knowNothing.gif" height = 500>
]

---

## Does Consistency Imply Unbiasedness?

An estimator of the mean: ${1 \over n}\sum x_{i}+{1 \over n}$ 

Is it consistent? 

Is it unbiased?

$$E({1 \over n}\sum x_{i}+{1 \over n}) = E({1 \over n}\sum x_{i})+E({1 \over n}) = \mu + {1 \over n}\neq \mu.$$

???

as $n\rightarrow \infty$, the estimator approaches the correct value, so it is consistent.

---

## C. Interval

.pull-left[
*Frequentist CI*

* .red[Confidence] Intervals 
* In a .red[repeatedly] sampling, the percentage of the samples that could contain &mu;

> "In 100 times sampling of ..., there are ... samples that the CI could contain the true value."

]

--

.pull-right[
*Bayesian CI*

* .red[Credible] intervals
* Some percentage of the .red[posterior] distribution lies within an particular region.

> "There are ...% of the chance that the true value lies in the CI."
]

---

## CI Calculation

+ Mean: $\mu = \bar X \pm Z_{\alpha/2}SE$<sup>\*</sup>
    + Proportion: $\pi = P \pm Z_{\alpha/2}\sqrt{\frac{P(1 - P)}{n}}$

&alpha;: 1 - Confident level;

SE(X&#772;) = $\frac{s}{\sqrt{n}}$, how far could the sample mean .red[disperse] from the population mean.

Z-score: $Z = \frac{X - \mu}{\sigma}$.

--

.center[Q: How to get smaller CI?]

???

1. Large N
1. Large &alpha;

--

.center[Q: Why using Z-score?]

---

## Why CI: Cross-Distribution Comparison

.center[<img src = "images/ci_inchMetric.gif" height = 300>

The ruler: Standard Normal distribution (Z-score)
]

--

+ Is the sample different from the ruler? 
+ Is sample A different from sample B, given the scale of the ruler?

---

## When N Is Not That Large

.center[<img src = "images/ci_largeNumber.gif" height = 200>]

--

Solution: A fatter-tailed distribution

```{r zvst, fig.height=4, fig.align='center'}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, aes(colour = "Normal"), size = 1.5) +
  stat_function(fun = function(x) dt(x, df = 3), aes(colour = "Student's t"), size = 1.5) + 
  ylab("Probability Density") + 
  xlab("") +
  labs(color = "Distribution") +
  scale_color_gb() +
  theme(legend.position = c(0.85, 0.8))
```

???

Small N

First derived as a posterior distribution in 1876 by Helmert and L√ºroth.

William Gosset published in English in 1908, used the pseudonym "student." Fisher called it the "student distribution"

---

## Degree of Freedom

Student's T critical points are relative to the d.f.

+ For CI: n - 1
+ For regression: n - k - 1

---

## CI Formulas

.pull-left[
+ For mean
    + &sigma; known, $\mu = \bar X \pm Z_{\alpha/2}\frac{\sigma}{\sqrt n}$
    + &sigma; unknown
        + N &geq; 100ish, then $\bar X \pm Z_{\alpha/2}\frac{s}{\sqrt n}$;
        + N < 100ish, then $\bar X \pm t_{\alpha/2}\frac{s}{\sqrt n}.$
 ]       

--

.pull-right[
+ For proportion
    + &pi; known, $\Pi = P \pm Z_{\alpha/2}\sqrt{\frac{\pi(1 - \pi)}{n}}$;
    + &pi; unknown, $\Pi = P \pm t_{\alpha/2}\sqrt{\frac{\pi(1 - \pi)}{n}}$.
]

---

## Application: "Comparing with the Ruler"

For nine cars, the mean mileage per gallon is 29.5 and standard deviation is 3. Is 31 for the mileage is a reasonable guess?

--

 Solution: Let's calculate the 95% CI of the guess.

--

d.f. = 9 - 1 = 8, therefore t(&alpha; < .975) = `r round(qt(.975, df = 8), digits = 4)` (`qt(.975, df = 8)` in r).

Then, CI = $29.5 \pm t_{\frac{0.975}{2}}(\frac{3}{\sqrt{9}})$, i.e, [27.5, 31.8].


.red[Narrative]: If we make repeated sampling from these cars, there are 95% of the samples in which the interval between 27.5 and 31.8 contains the true mean of the car mileage.

In terms of this, an estimation of 31 sounds fine.


---

## Application: "Comparing Sample A with B"

.pull-left[
*Independent sampling*

.center[<img src = "images/ci_independentSample.gif" >]

Observations are selected .red[without regard to] who is in the other condition, a.k.a., **independent and identical distributed** (IID).
]

--

.pull-right[
*Matched sampling*

.center[<img src = "images/ci_matchedSample.jpg" >]

Observations are .red[matched to] someone in the other condition.
]

---

## Difference in Means: Independent sample

.red[Assuming IID], 

* &sigma; is known, $\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm Z_{\alpha/2}\sqrt{\frac{\sigma_1}{n_1} + \frac{\sigma_2}{n_2}}$.
* &sigma; is unknown, $\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm t_{\alpha/2}S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$
   + Assuming the population have the same variance $\sigma_1 = \sigma_2$, where $S_p^2 = \frac{\sum(X_1 - \bar X_1)^2 + \sum(X_2 - \bar X_2)^2}{(n_1 - 1) + (n_2 - 1)}$
   + d.f.: $(n_1 - 1) + (n_2 - 1)$.

---

## Practice

In a class, there are two groups, the scores of members in group one are 64, 66, 89, 77, the scores of members in group two are 56, 71, and 53, are the mean scores statisically different between the two groups?

--

 Solution:

\begin{align}
\bar X_1 =& 296/4 = 74; \bar X_2 = 60; \\
S_p^2 =& \frac{398 + 186}{(4 - 1) + (3 - 1)} = 116.5;\\
\mu_1 - \mu_2 =& (74 - 60) \pm 2.57 (\sqrt{116.5}\sqrt{1/4 + 1/3}) = 14 \pm 21.
\end{align}


.red[Narrative]:    
If we make repeated sampling from these scores, there are 95% of the samples in which the interval between -7 and 35 contains the true mean of the scores.
*The CI includes 0*. **In other words, there's no statistical difference between the means.**

---

## Difference in Means: Matched Sample

*Matched samples*

$D = X_1 - X_2$, then $\Delta = \bar D \pm t_{\alpha/2}\frac{S_D}{\sqrt{n}}$, where $S_D = \frac{\sum(D - \bar D)^2}{n - 1}$. 

*Matched proportions* (Aggregate data)

$D = \Pi_1 - \Pi_2$ ,then $\Delta = D \pm Z_{\alpha/2}\sqrt{\frac{P_1(1 - P_1)}{n_1} + \frac{P_2(1 - P_2)}{n_2}}$

C.f., Independent sample

$\mu_1 - \mu_2 = (\bar X_1 - \bar X_2) \pm t_{\alpha/2}S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$, where $S_p^2 = \frac{\sum(X_1 - \bar X_1)^2 + \sum(X_2 - \bar X_2)^2}{(n_1 - 1) + (n_2 - 1)}$

???

Why using a different method for matched samples? Not IID for individuals, but might for the difference

---

## Practice

Four students first conducted an examination and went through a review section. Then, they took the examinations again. The scores are shown following. Did they get better after the review?

| Student 	| Amy 	| Bill 	| Becky 	| Mark 	|
|---------	|-----	|------	|-------	|------	|
| E1      	| 57  	| 57   	| 73    	| 65   	|
| E2      	| 64  	| 66   	| 89    	| 71   	|

--

\begin{align}
D =& X_1 - X_2 \Rightarrow \bar D = \sum D / n = (7 + 9 + 16 + 12) / 4 = 11\\
S_D^2 =& 46 / 3 = 15.3 \Rightarrow S_D = 3.9 \therefore \Delta = 11 \pm 3.18\times \frac{39}{4} = 11 \pm 6
\end{align}

--

.red[Narrative]: If we make repeated sampling from these students, there are 95% of the samples in which the interval between 5 and 17 contains the true mean of the difference.
The CI is above 0, that is, students did get better.

---

## Practice

Gallop drew a pair of 1500 samples from the American population. In the sample of 1980, there are 52% Democrats, and 46% in the 1985 sample. Were the Democrats the same for two years, given the 95% CI?

 Solution: Let &alpha; = 0.05,

\begin{align}
\Pi_1 - \Pi_2 &= (0.46 - 0.52) \pm 1.96\sqrt{\frac{0.46 * 0.54}{1500} + \frac{0.52 * 0.48}{1500}} \\
&\approx -0.06 \pm 0.036.
\end{align}

--

.red[Narrative]: If we make repeated sampling from the Amercian population, there are 95% of the samples in which the interval between -0.042 and 0.03 contains the true mean.
The CI contains 0.
Thus, the proportion of Democrats in 1980 was not different from that in 1985 statistically at the 0.05 level.


---

## Hypothesis Testing with CIs 

Two ways: 

1. Using confidence intervals
1. Using critical t/z scores & p-value.

--

*Procedure*:

1. H<sub>0</sub>: Specifying values for one or more population parameters in a random distribution (&mu;, &pi; rather than X&#772;, P);

2. H<sub>1</sub>: the population parameter is something other than the value in the stochastic status;

3. .red[&alpha; = 1 - CI];

4. One-tailed/two-tailed test: Most applications are one-tail tests, while most software gives two-tail results.

---

## Practice

Given a virus can influence 10% of the population. Now there's a sample of the older people, n = 527, within which there are 14% infected the virus. Are the older people more likely to be victimized?

$H_0: \pi \leq 10; H_1: \pi > 10.$ 

--

Method 1:     
$\pi = 0.14 \pm 1.96 * \sqrt{\frac{0.14 * (1 - 0.14)}{527}} = 0.14 \pm 0.03,$ that is [0.11, 0.17] > 0.1. $H_0.$ reject.

--

Method 2:     
$Z = \frac{P - \pi}{\sqrt{\frac{\pi(1 - \pi)}{n}}} = \frac{14 - 10}{\sqrt{\frac{0.1 * 0.9}{527}}} = 3.06.$ Given the level of $\alpha = 0.05$, therefore reject the $H_0.$


---

## What's &alpha;?

.pull-left[<img src="images/ci_errorType.png" height = 500 />]

--

.pull-right[
| Decision 	| Reject                   	|  .red[Fall to] Reject                  	|
|----------	|--------------------------	|--------------------------	|
| H<sub>0</sub> TRUE  	| Type I error (Pr = &alpha;)            	| Pr = 1 - &alpha; 	|
| H<sub>0</sub> FALSE 	| P = 1 - &beta; 	|  Type II error (Pr = &beta;)            	|

]

???

Type I: Êó†‰∏≠ÁîüÊúâ
Type II: Èó™


&beta; is the power of the test.

Avoiding Type I is more emergent.

---

## Why &alpha; = 0.975/&beta; = 0.95?

.pull-left[
```{r ci}

funcShaded <- function(x) {
  y <- dnorm(x, mean = 0, sd = 1)
  y[x < 0 - qnorm(0.975) | x > (0 + qnorm(0.975))] <- NA
  return(y)
}

ggplot(data = data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                n = 10000,
                args = list(mean = 0, sd = 1)) +
  stat_function(
    fun = funcShaded,
    geom = "area",
    fill = gb_cols("gold"),
    alpha = 0.2
  ) +
  ylab("") + xlab("")
```
]

--

.pull-right[.Huge[.red[Arbitrary!!!]]]

---

## Wrap Up

.pull-left[
*Confident Intervals*

Sample properties: Unbiasedness, efficiency, convergence, consistency

*Distribution Comparison*

1. With the ruler
    + $\mu = \bar X \pm Z_{\alpha/2}SE$
    + t score
1. A vs. B.
    + $\Delta = \bar D \pm t_{\alpha/2}\frac{S_D}{\sqrt{n}}$
]

.pull-right[
*Hypothesis testing*

1. CI approach
1. Critical-value approach
]
