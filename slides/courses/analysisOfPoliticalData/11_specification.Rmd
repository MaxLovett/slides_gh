---
title: "Model Specification"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - ../../../css/zh-CN_custom.css
      - ../../../css/styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    chakra: ../../../libs/remark-latest.min.js # to show slides offline
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable,
  knitr, # dependency
  descr, stringr, broom, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(114)
```

## Misspecification

If the true model is $Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i$, but we specify it as $Y_i = \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde u_i$, how does $\tilde\beta_1$ compare to $\beta_1$?

---

class: small

## &beta;

\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + u_i\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (u_i - \bar u)\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (u_i - \bar u)(X_1 - \bar X_1)\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\ 
           &+ \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},
\end{align}

where $\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$.

---

class: small

\begin{align}
E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1]\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(u_i|X_1)\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1
\end{align}

Accordingly, if &delta; = 0, $X_1 = \delta_0 + r$, the model may increase the risk of Type I error; otherwise, the model missing a variable will always produce an biased estimation unless X<sub>2</sub> is an irrelevant variable.

---

class: small

## Error

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon, \epsilon = \beta_2X_{2} + u_i$$

* $E(\epsilon|X_1) = E(\beta_2X_{2} + u_i) = \beta\mu_2\neq0.$
* 
\begin{align}
var(\epsilon|X_1) =& var(\beta_2X_{2} + u_i|X_1)\\ 
=& \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2
\end{align}
    + All values are fixed, that is, still homoscedasiticity.
* 
\begin{align}
& E(\epsilon_i\epsilon_j|X_1) - E(\epsilon_i|X_1)E(\epsilon_j|X_1) \\
=& E[(\beta_2X_{2i} + u_i)(\beta_2X_{2j} + u_j)|X_1] - \beta_2^2\mu^2_{X_2}\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + u_iu_j|X_1) - \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2E(X_{2i}X_{2j}|X_1) - \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2} = \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}
    + Only when $\beta_2^2cov(X_{2i}, X_{2j}) = 0$ (which is often not) there's no autocorrelation


---

* 
\begin{align}
E(X_1u) =& E[X_1(\beta_2X_2 + u)] \\
=& E(X_1\beta_2X_2) + E(X_1u)\\
=& \beta_2E(X_1X_2)\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}]\\
When&\ cov(X_1,X_2)\neq 0 \neq 0.
\end{align}
    + Increase the difficulty to isolate X from u
    
---

class: small

## Type of Measurement

* Nominal: Order doesn't matter
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.
    + Can't do regression unless being broken up into indicator variables.
--

* Indicator: Binary usually, "when x has a value of X, Y is 1", &#x1D7D9;(x = X).
    + In a regression, the information of the baseline is captured by the intercept
    + e.g., $Y = \beta_0 + \beta_1X_i + u_i,$ where X is either male (0) or female(1), then

$$\beta_0: E(Y|X = male); \beta_1: E(Y|X = female) - E(Y|X = male)$$

???

blackboard 1

---

* Ordinal: 
    + Non-interval (only the order matters); 
    + Interval (same intervals)

--

* Continuous (ratio): Can credibly calculate the marginal effects, because marginal effects are derivatives, and only continuous variable can do derivative; other types can do this mathematically, but not accurately.

???

Interval: meaningful distance, feeling thermometer's 0 is not nothing but a strong feeling
Ratio: interval variables with meaningful zero value (meaning absence); same ratio conveys the same meaning

e.g., -50 to 0 to 50 find two pair of scores with a ratio of 1:3, transform them into 0-100 scale, the ratio changes.

---

## Nonlinear effect

e.g., Y increases with X ( $\frac{\partial Y}{\partial X} > 0$ ) at a decreasing rate ( $\frac{\partial^2 Y}{\partial^2 X} < 0$ )

---

```{r nonlinear, fig.width=8, fig.height=4, fig.show='hold'}

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u))

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u))
```

---

class: small

$$Y = \beta_0 + \beta_1X + \beta_2X^2 + u$$

Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$

* H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
* H<sub>1</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X > 0

That is, if the increasing speed is positive.

Level set: &alpha; = 0.05

Statistics: $\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim t_{n - 3}$

\begin{align}
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)} \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}
\end{align}

--

.magenta[NB]: One can't simply say if the null hypothesis is rejected, because it may not be a coherent conclusion in the entire domain of X, due to the nonlinearity. So, a better way to describe it is "in the range from a to b, the hypothesis can be rejected."

---

## Substantive Significance

Substantive interpretation: margins. 

The most important point is the inflation point (where the sign of the derivate changes).