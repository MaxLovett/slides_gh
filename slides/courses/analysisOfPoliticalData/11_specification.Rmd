---
title: "Model and Data Issues"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - ../../../css/zh-CN_custom.css
      - ../../../css/styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    chakra: ../../../libs/remark-latest.min.js # to show slides offline
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable, ggeffects,
  knitr, # dependency
  descr, stringr, broom, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(114)
```

class: inverse, bottom

# Model Issues

---

## Misspecification

If the true model is $Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i$, but we specify it as $Y_i = \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde u_i$, how does $\tilde\beta_1$ compare to $\beta_1$?

---

class: small

## &beta;

\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + u_i\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (u_i - \bar u)\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (u_i - \bar u)(X_1 - \bar X_1)\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\ 
           &+ \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},
\end{align}

where $\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$.

---

class: small

\begin{align}
E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1]\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(u_i|X_1)\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1
\end{align}

Accordingly, if &delta; = 0, $X_1 = \delta_0 + r$, the model may increase the risk of Type I error; otherwise, the model missing a variable will always produce an biased estimation unless X<sub>2</sub> is an irrelevant variable.

---

class: small

## Error

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon, \epsilon = \beta_2X_{2} + u_i$$

* $E(\epsilon|X_1) = E(\beta_2X_{2} + u_i) = \beta\mu_2\neq0.$
* 
\begin{align}
var(\epsilon|X_1) =& var(\beta_2X_{2} + u_i|X_1)\\ 
=& \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2
\end{align}
    + All values are fixed, that is, still homoscedasiticity.
* 
\begin{align}
& E(\epsilon_i\epsilon_j|X_1) - E(\epsilon_i|X_1)E(\epsilon_j|X_1) \\
=& E[(\beta_2X_{2i} + u_i)(\beta_2X_{2j} + u_j)|X_1] - \beta_2^2\mu^2_{X_2}\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + u_iu_j|X_1) - \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2E(X_{2i}X_{2j}|X_1) - \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2} = \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}
    + Only when $\beta_2^2cov(X_{2i}, X_{2j}) = 0$ (which is often not) there's no autocorrelation


---

* 
\begin{align}
E(X_1u) =& E[X_1(\beta_2X_2 + u)] \\
=& E(X_1\beta_2X_2) + E(X_1u)\\
=& \beta_2E(X_1X_2)\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}]\\
When&\ cov(X_1,X_2)\neq 0 \neq 0.
\end{align}
    + Increase the difficulty to isolate X from u
    
---

class: small

## Type of Measurement

* Nominal: Order doesn't matter
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.
    + Can't do regression unless being broken up into indicator variables.
--

* Indicator: Binary usually, "when x has a value of X, Y is 1", &#x1D7D9;(x = X).
    + In a regression, the information of the baseline is captured by the intercept
    + e.g., $Y = \beta_0 + \beta_1X_i + u_i,$ where X is either male (0) or female(1), then

$$\beta_0: E(Y|X = male); \beta_1: E(Y|X = female) - E(Y|X = male)$$

???

blackboard 1

---

* Ordinal: 
    + Non-interval (only the order matters); 
    + Interval (same intervals)

--

* Continuous (ratio): Can credibly calculate the marginal effects, because marginal effects are derivatives, and only continuous variable can do derivative; other types can do this mathematically, but not accurately.

???

Interval: meaningful distance, feeling thermometer's 0 is not nothing but a strong feeling
Ratio: interval variables with meaningful zero value (meaning absence); same ratio conveys the same meaning

e.g., -50 to 0 to 50 find two pair of scores with a ratio of 1:3, transform them into 0-100 scale, the ratio changes.

---

## Nonlinear effect

e.g., Y increases with X ( $\frac{\partial Y}{\partial X} > 0$ ) at a decreasing rate ( $\frac{\partial^2 Y}{\partial^2 X} < 0$ )

---

```{r nonlinear, fig.width=8, fig.height=4, fig.show='hold'}

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u))

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u))
```

---

## Marginal Effect

Discrete:

$$Pr(Y|x = X_{n + 1}) - Pr(Y|x = X_n)$$

Continuous: 

$$\lim_{\Delta x\to0} \frac{ f(x + \Delta x) - f(x)}{\Delta x}$$

--

.magenta[Hint]: The marignal effect of an OLS is its &beta;s

---

### Averagte Marginal Effect (AME)

1. Calculate the marginal effect of each variable x for each observation
1. Calculate the average.

--

### Marginal Effect at the Mean (MEM)

1. Calculate the marginal effect of each variable x for each's mean value

--

### Marginal Effect at Representative Values (MER)

1. Calculate the marginal effect of each variable x for value(s) of interest

---

class: small

## Marginal Effect of A Nonlinear Transformation

$$Y = \beta_0 + \beta_1X + \beta_2X^2 + u$$

Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$

* H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
* H<sub>1</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X > 0

That is, if the increasing speed is positive.

Level set: &alpha; = 0.05

Statistics: $\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim t_{n - 3}$

\begin{align}
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)} \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}
\end{align}

--

.magenta[NB]: One can't simply say if the null hypothesis is rejected, because it may not be a coherent conclusion in the entire domain of X, due to the nonlinearity. So, a better way to describe it is "in the range from a to b, the hypothesis can be rejected."

---

## Substantive Significance

* Max - min
* First difference
* Marginal effects


```{r substantive, fig.width=10, fig.height=5, fig.show='hold'}

df_nl3 <- tibble(x = seq(-5, 5,length.out = 1000),
                y = x - x^2 + rnorm(1000,0,sd = 10))

fit1 <- lm(y ~ x, data = df_nl3)
me1 <- ggeffect(fit1, terms = "x")

ggplot(data = df_nl3, aes(x, y, color = "Predicted Values")) +
  geom_smooth() + 
  geom_line(data = me1, aes(x, predicted, color = "Marginal Effect")) +
  geom_ribbon(data = me1, aes(x, predicted, ymin=conf.low,ymax=conf.high), alpha = 0.3) +
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) +
  ylab("E(Y|X)") + xlab("X")

```

---

class: inverse, bottom

# Data Issues

---

## Missing Data

Let's define  **X**, **Y**, and m<sub>ij</sub> = 1 if X<sub>i</sub> is missing.

\begin{align}
\boldsymbol{D} =& 
\left(\begin{array}{cc}
X_1 & Y_1\\
X_2 & Y_2\\
X_3 & Y_3\\
X_4 & Y_4\end{array}\right);
\boldsymbol{D^{Observed}} =
\left(\begin{array}{cc}
X_1 & Y_1\\
X_2 & Y_2\\
X_3 & Y_3\\
X_4 & Y_4\end{array}\right);\\
\boldsymbol{M} =& 
\left(\begin{array}{cc}
0 & 0\\
1 & 0\\
0 & 0\\
1 & 0\end{array}\right);
\boldsymbol{D^M} =
\left(\begin{array}{cc}
 & \\
X_2 & \\
 & \\
X_4 & \end{array}\right)
\end{align}

---

## Type of Missing

### Missing completely at random (MCAR)

P(M|D) = P(M)

### Missing at random (MAR)

P(M|D) = P(M|D<sup>Observed</sup>)

### Non-ignorable (NI/MNAR)

P(M|D) &ne; P(M|D<sup>Observed</sup>)

---

## Consequence of Missing

|      | Summary Stats | Regression          | SE vs. complete |
|------|---------------|---------------------|-----------------|
| MCAR | Unbiased      | Unbiased/consistent | Inefficient     |
| MAR  | Biased        | .magenta[Unbiased/consistent] | Inefficient     |
| NI   | Biased        | Biased              | Inefficient     |

???

Unbiasedness in MAR is because the censoring of the data is based on an irrelevant variable;
Biasedness in NI is because of the censor with an X within the model

---

## Solution

**Ignore it**: listwise deletion

???

Even for MAR, droping the entire entry due to the missing would cause bias, because there is no missing in Y but Y is dropped because of the missing of X; cov(X, u) &ne; 0, so often parwise deletion

--

**Fill it manually**: hot decking (using neayby approximate values, e.g., mean)

--

**Imputation**

1. Interpolation
1. Extrapolation
1. Regression imputation: $M = \boldsymbol{X\gamma} + u$
    + For multidimentional variable use joint distribution and iterative chain.
    
???

<img src="images/interpolation.png" height = 200 />
<img src="images/extrapolation.png" height = 200 />

    
---

Multiple imputation: 