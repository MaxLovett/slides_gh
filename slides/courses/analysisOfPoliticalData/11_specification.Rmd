---
title: "Model Specification"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
institution: "Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse, flexable, icons, xaringanExtra, ggeffects
) 


use_xaringan_extra(c("tile_view", # O
                     "broadcast",
                     "panelset",
                     "tachyons",
                     "fit_screen"))
use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = FALSE  #<<
)


# Functions preload
set.seed(313)

theme_set(theme_minimal())
```
## Overview

"How can you set things wrong?"

.center[
### Measuring .red[variables] wrong

Proxy variable    
Measurement error
]
.pull-left[
### Specifying a wrong .red[model]

Functional form   
Omitted variable
]
.pull-right[
### Interpreting the .red[result] wrong

Nonlinear & substantive significance
]

---

class: inverse, bottom

# Measurement Problem

---

## Proxy Variable: One Type of Measurement Error

True model: Y = &beta;<sub>0</sub>  + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + &beta;<sub>3</sub>X<sub>3</sub><sup>*</sup> + u.

--

However, X<sub>3</sub><sup>\*</sup> is unobserved, instead, we know X<sub>3</sub><sup>*</sup> = &delta;<sub>0</sub> + &delta;<sub>1</sub>X<sub>3</sub> + v. 

In other words, 

.center[Y = (&beta;<sub>0</sub> + &beta;<sub>3</sub>&delta;<sub>0</sub>) + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + &beta;<sub>3</sub>&delta;<sub>1</sub>X<sub>3</sub> + (u + &beta;<sub>3</sub>v).]

--

Assuming cov(u,v) = 0, 

.center[E(u + &beta;<sub>3</sub>v|X) = 0;]

.center[var(u + &beta;<sub>3</sub>v|X) = &beta;<sub>3</sub><sup>2</sup>&sigma;<sub>v</sub><sup>2</sup> + &sigma;<sub>u</sub><sup>2</sup>.]

???

cov(u,v) = 0: u, v independent

--

To produce unbiased estimates:

.center[cov(u, X<sub>3</sub>) = cov(v, X<sub>1</sub>) = cov(v, X<sub>2</sub>) = 0.]

---

## Measurement Error

1. Measurement in Y;
1. Measurement in X.

---

## Y = Y<sup>*</sup> + e

\begin{align}
Y^* =& \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + u\\
Y =& \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + (u + e)
\end{align}

.center[
### Consequence

+ &beta; Unbiased 
+ var(&beta;) &uarr;
]

---

class: small

## X = X<sup>*</sup> + e

If E(eX) = 0, then Y = &beta;<sub>0</sub> + &beta;<sub>1</sub>(X<sub>1</sub> - e) + u = &beta;<sub>0</sub> + &beta;<sub>1</sub>X<sub>1</sub> + (u - &beta;<sub>1</sub>e).

--

If E(eX) &ne; 0, then
.pull-left[
\begin{align}
cov(e, X_1) =& E(eX_1) - E(e)E(X_1) = E(eX_1)\\
          =& E[e(X_1^* + e)] = E(eX_1^*) + E(e^2) \\
          =& \sigma_e^2\neq0\\
cov(u - \beta_1e, X_1) =& cov(-\beta_1e, X_1) = -\beta_1cov(e, X_1)\\
                       =& -\beta_1\sigma_e^2.\\
\Rightarrow\ plim(\hat\beta_1) =& \beta_1 + \frac{cov(u - \beta_1e, X_1)}{var(X_1)} \\
                               =& \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^2}
                               = \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^{*2} + \sigma^2_e} \\
                               =& \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_e}\beta_1.
\end{align}
]

???
plim: probability limit operation, convergence in probability

--

.pull-right[
Unbiased .red[only] when var(X<sub>1</sub>) = var(X<sub>1</sub><sup>\*</sup> + e),    
i.e., &sigma;<sup>2</sup><sub>X<sub>1</sub></sub> = &sigma;X<sub>1</sub><sup>\*2</sup>+ &sigma;<sub>e</sub><sup>2</sup>

Otherwise, $|\hat\beta_1| < \beta_1$, a.k.a., .red[attenuation] bias. 

+ &beta;<sub>1</sub>: underestimated;
+ Affecting the estimations of others in unknown ways .
]
---

## Measurement Type Issue

* Nominal: 
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.
    + Can't do regression unless being broken up into indicator variables.
--

* Indicator: Binary usually
    + $Y = \beta_0 + \beta_1X_i + u_i,$ where X is either male (0) or female(1)
        + &beta;<sub>0</sub>: E(Y|X = male);
        + &beta;<sub>1</sub>: E(Y|X = female) - E(Y|X = male).

???

In a regression, the information of the baseline is captured by the intercept

---

class: inverse, bottom

# Specification Problems

---

## Problem Types

1. Functional form
1. .gray[Omitted variable]
1. .gray[Nonlinear effect]

---

## Functional Form Issue

When there are two ways to specify the model:

\begin{align}
Y =& f_1(X_1, X_2, X_3) + u =\beta_1X_1 + \beta_2X_2 + \beta_3X_3 + u \\
Y =& f_2(X_1, X_2, X_3) + u =\beta_4X_1^2 + \beta_5X_2^3 + \beta_6X_3^4 + u
\end{align}

Which specification is .red[correct]?

---

Test &beta;<sub>1</sub> in Y = f<sub>1</sub>(X) + f<sub>2</sub>(X) + u?

--

Probably not. How many models are correct?

???

Only one true model 

--

Let's assume f<sub>1</sub> is the true model:

$$var(\hat\beta_1) = \frac{\sigma^2}{(X_1 - \bar X_1)^2(1 - \rho_{12})}.$$

&rho;<sub>12</sub>: Correlations between X<sub>1</sub> and X<sub>2</sub>, such as between X and X<sup>2</sup>

When &rho;<sub>12</sub> &uarr;......

???

Variance is inflated when &rho;<sub>12</sub> &uarr;

---

### Solution

.center[Y = f<sub>1</sub>(X) + f<sub>2</sub>(X) + u]

Joint test: H<sub>0</sub>: &beta;<sub>1</sub> = &beta;<sub>2</sub> = &beta;<sub>3</sub> = 0 or &beta;<sub>4</sub> = &beta;<sub>5</sub> = &beta;<sub>6</sub> = 0

Statistics: Davidson-Mackinnon test

1. Run $Y = f_2(X) + u$ and calculate the expected value $\hat Y = E[Y|f_2(X)]$
1. Run $Y = f_1(X) + \theta\hat Y + u$, and test if &theta; = 0.

---

## Problem Types

1. .gray[Functional form]
1. Omitted variable
1. .gray[Nonlinear effect]

---

class: small

## Omitted Variable

\begin{align}
True: Y_i =& \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i\\
Specified: Y_i =& \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde u_i
\end{align}

How does $\tilde\beta_1$ compare to $\beta_1$?

\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + u_i\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (u_i - \bar u)\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (u_i - \bar u)(X_1 - \bar X_1)\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},\\
\Rightarrow E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1]\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(u_i|X_1)\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1
\end{align}

---

$\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$ ( $X_1 = \delta_0 + \delta_1X_2 + r$).


$\tilde\beta_1$: .red[Biased], unless X<sub>2</sub> is an irrelevant variable.

Even if &delta;<sub>1</sub> = 0, $X_1 = \delta_0 + r$, the model may increase the risk of Type I error

???

False positive

---

class: small

How About the Residual?

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon,$$

$$\epsilon = \beta_2X_{2} + u_i.$$


$E(\epsilon|X_1) = E(\beta_2X_{2} + u_i) = \beta\mu_2\neq0$
+ Against the assumption.
  

$var(\epsilon|X_1) = var(\beta_2X_{2} + u_i|X_1)= \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2$
+ All values are fixed, that is, still homoscedasitic.  

$E(X_1\epsilon) = \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}],$ 
+ When cov(X<sub>1</sub>, X<sub>2</sub>)&ne;0 &rArr; Increase the difficulty to isolate X from u.
    
$E(\epsilon_i\epsilon_j|X_i)\neq 0$

\begin{align}
E(\epsilon_i\epsilon_j) - E(\epsilon_i)E(\epsilon_j) =& E[(\beta_2X_{2i} + u_i)(\beta_2X_{2j} + u_j)] - \beta_2^2\mu^2_{X_2},\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + u_iu_j) - \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2E(X_{2i}X_{2j}) - \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}

---

\begin{align}
E(X_1\epsilon) =& E[X_1(\beta_2X_2 + u)], \\
=& E(X_1\beta_2X_2) + E(X_1u),\\
=& \beta_2E(X_1X_2),\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}],\\
When\ cov(X_1,X_2)\neq& 0, E(X_1\epsilon) \neq 0.
\end{align}

+ When cov(X<sub>1</sub>, X<sub>2</sub>) &ne; 0, E(X<sub>1</sub>&epsilon) &ne; 0.

---

## Problem Types

1. .gray[Functional form]
1. .gray[Omitted variable]
1. Nonlinear effect

---

## Nonlinear effect

.pull-left[
```{r nonlinear, fig.width=8, fig.height=4, fig.show='hold', fig.align='center'}

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u)) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )
```
]

--

.pull-right[
Unbiased efficient .red[linear] estimates, but how to explain it? 
]
---

## Marginal Effect

Discrete:

$$Pr(Y|x = X_{n + 1}) - Pr(Y|x = X_n)$$

Continuous: 

$$\lim_{\Delta x\to0} \frac{ f(x + \Delta x) - f(x)}{\Delta x}$$
--

Every value of X after X<sub>2</sub> has a marginal effect.
Which one is the X's effect?

---

## Three Types of Marginal Effects

.pull-left[

*Averagte Marginal Effect (AME)*

1. Calculate the marginal effect of each variable x for .red[each observation].
1. Calculate the average.


*Marginal Effect at the Mean (MEM)*

Calculate the marginal effect of each variable x for .red[each's mean value].

]

--

.pull-right[
*Marginal Effect at Representative Values (MER)*

Calculate the marginal effect of each variable x for .red[value(s) of interest].

.center[<img src = "images/spe_interaction2.jpg" height = 350>]
]

---

## Marginal Effect of A Nonlinear Transformation

$$Y = \beta_0 + \beta_1X + \beta_2X^2 + u.$$

Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$
.pull-left[
* H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
* H<sub>1</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X > 0
]

.pull-right[
That is, if the increasing speed is positive.

Level set: &alpha; = 0.05
]

--

Statistics: $\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim t_{n - 3}$

\begin{align}
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)} \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}
\end{align}

---

## Attention!!

One can't simply say if the null hypothesis is rejected, because it may .red[not be a coherent conclusion] in the entire domain of X, due to the nonlinearity. 

A better way to describe: "In the range from a to b, the hypothesis can be rejected."

--

.center[
Or

Just plot it. (do this, cool kids!)
]

---

## Substantive Significance

* Max - min
* First difference
* Marginal effects

--

```{r substantive, fig.width=10, fig.height=6, fig.align='center'}

df_nl3 <- tibble(x = seq(-5, 5,length.out = 1000),
                y = x - x^2 + rnorm(1000,0,sd = 10))

fit1 <- lm(y ~ x, data = df_nl3)
me1 <- ggpredict(fit1, terms = "x")

ggplot(data = df_nl3) +
  geom_smooth(aes(x, y, color = "Predicted Values")) + 
  geom_line(data = me1, aes(x, predicted, color = "Marginal Effect")) +
  geom_ribbon(data = me1, aes(x, predicted, ymin=conf.low,ymax=conf.high), alpha = 0.3) +
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) +
  ylab("E(Y|X)") + xlab("X") + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )

```

---

## Wrap Up

.pull-left[
*Measuring variables wrong*

1. Proxy variable: Unbiased when cov(u, X<sub>3</sub>) = cov(v, X<sub>1</sub>) = cov(v, X<sub>2</sub>) = 0.
    
1. Measurement error
    + In Y: &beta; Unbiased, var(&beta;) &uarr;
    + In X: Attenuation bias (&beta; &darr;)

1. Measurement type issue (normal/binary)

]

--

.pull-right[
*Specifying a wrong model*

1. Functional form 
    + Davidson-Mackinnon test
1. Omitted variable: .red[Biased]

*Interpreting issues*

1. Nonlinear &rArr; marginal effect
1. Substantive significance
]