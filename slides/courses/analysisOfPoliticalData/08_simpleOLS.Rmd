---
title: "Simple OLS and Properties of Estimators"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - ../../../css/zh-CN_custom.css
      - ../../../css/styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    chakra: ../../../libs/remark-latest.min.js # to show slides offline
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  knitr, # dependency
  descr, stringr, broom, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(114)
```

class: inverse, bottom

# Understanding OLS
## Theoretical and Methodological Views

---

## Theory

General statement to describe causality and relevant phenomena.

---

## Good theory

1. Large explanatory power
    + Importance: How much X affects Y
    + Range: generality
    + Applicability: How often affecting in the reality
1. Parsimony
1. Satisfying elaboration
1. Clearly framed (formal?)
1. Falsifiable
1. Explaining important phenomena
1. Prescriptive richness (+policy suggestion)

---

## Conspiracy

.center[<img src="../../guestLecture/image/currencyWars.jpg" height = 380 />]

What makes a conspiracy different from a theory? 

???

Rothschild family

---

class: small

## How to Test a Theory (Causal Inference)

1. Hoop tests: Predictions of high certitude and no uniqueness provide decisive negative test
1. Smoking-gun tests: Predictions of high uniqueness and no certitude provide decisive positive tests:
1. Double-decisive tests: Predictions of high uniqueness and high certitude
1. Straw-in-the-wind tests: Predictions of low uniqueness and low certitude<sup>1</sup>

.footnote[[1] Evera, Stephen Van. 1997. Guide to Methods for Students of Political Science. Ithaca: Cornell University Press.]

???

1. Hoop test: the accused in the state when murder happens? If not innocent, but passing does not mean being guilty.
1. Smoking-gun test: a smoking gun seen in a suspect's hand moments after a shooting is quite conclusive proof of guilt, but a suspect not seen with a smoking gun is not proven innocent.
1. Double-decisive test: hoop + smoking-gun, like a camera record
1. Straw-in-the-wind test: Richard Liu seemed friendly with the girl.

---

## Ordinary Least Squares

One type of simulation of the reality (and one of a variety of types).

.red[NOT OFFERING CAUSALITY.]

### Used it properly

1. Always getting attention to the detailed data
1. Don't let the data drive you, you drive the data with appropriate methods.

---

## What We Already Know

* Population vs. sample

--

* Parameter vs. statistics

--

* Random variable 

???

a variable following a distributive feature, but no fixed value before creation.

--

* Distribution

???

For each value of a random variable X, the distribution tells us the probability of that value.

--

* Joint, marginal, and conditional distributions

???

Joint: $f(X_1, X_2) = P(X_1 = x_1, X_2 = x_2)$
Marginal: the integral of $f(X_1) or f(X_2)$ to the other variable
Conditional: $f(X_1|X2 = x_2)$

---

* Expectation:

\begin{align}
E(X)_{discrete} =& \sum^n_{i = 1} X_if(X_i)\\
E(X)_{continuous} =& \int^{+\infty}_{-\infty}X_if(X_i)dX.
\end{align}

* Variance:

\begin{align}
\sigma^2 = E[X - E(X)]^2 =& E[(X - \mu)^2]\\
              =& E(X^2 - 2X\mu + \mu^2)\\
              =& E(X^2) - 2\mu E(X) + \mu^2\\
              =& E(X^2) - \mu^2
\end{align}

???

Expectation is the mean.

---

class: small

### Rule of variance

1. $var(aX + b) = a^2var(X);$
1. $var(aX_1 + bX_2 + c) = a^2var(X_1) + b^2var(X_2) + 2ab\cdot cov(X_1, X_2).$
1. When $X_i$ is i.i.d., 
$var(a1X_1 + a_2X_2 + \cdots + a_nX_n) = var(\sum a_iX_i) = \sum a_i^2var(X_i)$
1. $var(\bar X) = var(\frac{\sum X_i}{n}) = \sum^n_{i = 1}\frac{var(X_i)}{n^2} = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$
1. $cov(X, Y) = E(XY) - E(X)E(Y)$
    + When X = Y, $cov(X, X) = E(X^2) - [E(X)]^2 = var(X)$


???
\begin{align}
var(aX + b) =& E[(aX + b) - E(aX + b)]^2\\
            =& E[aX + b - aE(X) - b]^2\\
            =& E[aX - aE(X)]^2 = a^2E[X - E(X)]^2 = a^2var(X)
\end{align}

\begin{align}
cov(X, Y) =& E[X - E(X)][Y - E(Y)]\\
          =& E[XY - XE(Y) - YE(X) + E(X)E(Y)]\\
          =& E(XY) - E(Y)E(X) - E(X)E(Y) + E(X)E(Y)\\
          =& E(XY) - E(X)E(Y)
\end{align}


---

## Property of Estimator $\hat \theta$

* Unbiased: On average, the estimator gives the right answer, formally, $E(\hat\sigma) = \sigma.$
* Consistent: As the sample size increases, the variance decreases.

```{r consistency, fig.height=4, fig.width=10}
ggplot(data.frame(x = c(0, 40)), aes(x = x)) +
  stat_function(fun = function(x) dgamma(x, shape = 9, scale = .5), aes(color = "small")) +
  stat_function(fun = function(x) dgamma(x, shape = 9, scale = 1), aes(color = "mediam")) +
  stat_function(fun = function(x) dgamma(x, shape = 9, scale = 2), aes(color = "large")) +
  ylab("Probability Density") + 
  xlab("X") +
  labs(color = "Distribution")
```

???

Consistency: n1 < n2 < n3

---

* Efficiency: Smallest variance among unbiased estimators

```{r efficacy, fig.height=4, fig.width=10}
ggplot(data.frame(x = c(-7, 7)), aes(x = x)) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 0.5), aes(color = "0.5")) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1), aes(color = "1")) +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 2), aes(color = "2")) +
  ylab("Probability Density") + 
  xlab("X") +
  labs(color = "SD")
```

* RMES: Root mean square error, $\sqrt{bias^2 + var}$
    + Trade off a little bias against variance.

---

class: inverse, bottom

# Regression
## Getting the mean right is a good thing
---Frederick J. Boehmke

---

## Regressions' Target

How does some outcome variable Y change with some explanatory variable X? 

E.g., Are the political ideology of the three types of voters different? 

$\mu_D = \mu_I = \mu_R.$

$\text{diff} = \frac{\bar X_R - \bar X_D}{SE}.$

???

<img src="images/illustrateSimpleOLS.png" height = 200 />

---

## Linearity

Y changes linearly with X.

In terms of the population regression function,

$$Y_i = \beta_0 + \beta_1X_1 + u_i$$

Therefore, 

\begin{align}
E(Y_i|X_i) =& E(\beta_0 + \beta_1X_1 + u_i|X_i)\\
           =& E(\beta_0|X_i) + E(\beta_1X_1|X_i) + E(u_i|X_i)\\
           =& \beta_0 + \beta_1X_i + E(u_i|X_i)
\end{align}

???

$\beta_1X_i$ consistent

E(u_i|X_i) assumed 0

---

Q. Which one shows a linear relationship between X and Y?

\begin{align}
Y_i =& \beta_0 + \beta_1X_i + ui\\
Y_i =& \beta_0 + \frac{\beta_1}{X_i} + ui\\
Y_i =& \beta_0 + \beta_1ln(X_i) + ui\\
Y_i =& \beta_0 + X_i^{\beta_1} + ui\\
Y_i =& \frac{1}{\beta_0} + \frac{X_i}{\beta_1} + ui
\end{align}

???

First three are linear.

The last one isn't linear until redefining.

---

\begin{align}
Y_i =& E(Y_i|X_i) + u_i\\
\Leftrightarrow u_i =& Y_i - E(Y_i|X_i)
\end{align}

There are things we don't know (or human errors).

---

## What Do Regressions Do?

Sample regression function (SRF): 

\begin{align}
Y_i =& \hat\beta_0 + \hat\beta_1X_i + \hat u_i\\
    =& \hat Y_i + \hat u_i
\end{align}

--

### Goal

To get the right mean, $\sum|\hat u_i| = 0$, then minimze $\sum|Y_i - \hat Y_i|$ (or $\sum|Y_i - (\hat\beta_0 + \hat\beta_1X_i)|$)

---

Since $|\hat u_i|$ is hard to be coped with, thus using $\hat u^2$. 

### Goal changes

Minimizing $|\hat u_i|$ &zigrarr; Minimizing $\hat u^2$ &zigrarr; Minimizing $\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2$

---

class: small

## How to Minimizing?

\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} =& -\sum 2(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0\\
\Leftrightarrow\sum Y_i - \sum\hat\beta_0 - \sum\hat\beta_1X_i =& 0\\
\sum Y_i =& \sum\hat\beta_0 + \sum\hat\beta_1X_i\\
                         =& n\hat\beta_0 + \hat\beta_1\sum X_i\\
\hat\beta_0 =& \frac{\sum Y_i}{n} - \hat\beta_1\frac{\sum X_i}{n}\\
                        =& \bar Y - \hat\beta_1\bar X
\end{align}

---

class: small

First, there's a fact; $\sum X_i(Y_i - \bar Y) = \sum(X_i - \bar X)(Y_i - \bar Y).$

Proof: 

\begin{align}
\sum X_i(Y_i - \bar Y) =& \sum X_i(Y_i - \bar Y) - \bar X\sum (Y_i - \bar Y)\\
                       =& \sum X_i(Y_i - \bar Y) - \bar X(\sum Y_i - \sum\bar Y)\\
                       =& \sum X_i(Y_i - \bar Y) - \bar X(n\bar Y - n\bar Y)\\
                       =& \sum X_i(Y_i - \bar Y)
\end{align}

---

class: small

\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_1} =& -\sum 2X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i)\\
=& 0\\
\Leftrightarrow \sum X_iY_i - \sum X_i\hat\beta_0 - \sum\hat\beta_1 X_1^2 =& 0\\
\sum X_iY_i =& \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2\\
\sum X_iY_i - \sum X_i(\bar Y - \hat\beta_1\bar X) - \sum\hat\beta_1 X_1^2 =& 0\\
\sum X_iY_i - \sum X_i\bar Y + \sum X_i\hat\beta_1\bar X - \sum\hat\beta_1 X_1^2 =& 0\\
\sum X_i(Y_i - \bar Y) + \hat\beta_1\sum X_i(\bar X - X_i) =& 0\\
\sum X_i(Y_i - \bar Y) =& \hat\beta_1\sum X_i(X_i - \bar X)
\end{align}

Given we know that $\sum X_i(Y_i - \bar Y) = \sum(X_i - \bar X)(Y_i - \bar Y),$

\begin{align}
\sum X_i(Y_i - \bar Y) =& \hat\beta_1\sum X_i(X_i - \bar X)\\
\sum (X_i - \bar X)(Y_i - \bar Y) =& \hat\beta_1\sum (X_i - \bar X)(X_i - \bar X)\\
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}.
\end{align}

---

Btw, these equations 

$\hat\beta_0, \hat\beta_1$ are called as coefficients;

\begin{align}
\sum Y_i =& n\hat\beta_0 + \hat\beta_1\sum X_i\\
\sum X_iY_i =& \sum X_i\hat\beta_0 + \sum\hat\beta_1 X_1^2
\end{align}

are called "normal equations."

---

class: small

## Coefficient and Covariance

\begin{align}
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}\\
        =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum (X_i - \bar X)^2}\sqrt{\sum (Y_i - \bar Y)^2}}\cdot\frac{\sqrt{\sum (Y_i - \bar Y)^2}}{\sqrt{\sum (X_i - \bar X)^2}}\\
        =& \rho_{X, Y}\frac{s_Y}{s_X}
\end{align}

So when the variance of Y increases, $\beta_1$ increases.

--

.magenta[Special case]: Standardized X and Y, i.e., $s_Y, s_X$ are 1s, then,

$$\beta_1 = \rho_{X, Y}\frac{s_Y}{s_X} = \rho_{X,Y}.$$

---

class: small

## Back to Linearity

Remember $\sum X_i(Y_i - \bar Y) = \sum(X_i - \bar X)(Y_i - \bar Y).$

\begin{align}
\hat\beta_1 =& \frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sum (X_i - \bar X)^2}\\
            =& \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)Y_i\\
            =& \frac{1}{\sum (X_i - \bar X)^2}\sum (X_i - \bar X)(\beta_0 + \beta_1X_i + u_i)\\
            =& \frac{1}{\sum (X_i - \bar X)^2}[\sum (X_i - \bar X)(\beta_0 + \beta_1X_i) + \sum (X_i - \bar X)u_i]\\
            =& \frac{\sum (X_i - \bar X)(\beta_0 + \beta_1X_i)}{\sum (X_i - \bar X)^2} + \frac{\sum (X_i - \bar X)u_i}{\sum (X_i - \bar X)^2}
\end{align}

Let $k_i=\frac{X_i - \bar X}{\sum (X_i - \bar X)^2},$ then $\hat\beta_1 = \beta_1\sum k_iX_i + \sum k_iu_i$, i.e., a linear combination of errors; the min/max(X) influences the estimation a lot.

---

class: small

## Feature of Regression Coefficients

1. Calculated using observed data
1. Unique point estimates
1. SRF passes through $(\bar X, \bar Y)$
1. $\bar{\hat Y} (\text{predicted}) = \hat Y (\text{observed})$, $\frac{\sum\hat Y_i}{n} = \frac{\sum Y_i}{n}$
1. $\bar{\hat u_i} = \frac{\sum{\hat u_i}}{n} = 0\\$, Proof:
\begin{align}
\frac{\partial\sum[Y_i - (\hat\beta_0 + \hat\beta_1X_i)]^2}{\partial\hat\beta_0} = -2\sum(Y_i - \hat{\beta_0} - \hat{\beta_1}X_i) =& 0\\
\sum (Y_i-\hat Y_i) =& 0\\
\sum{\hat u_i} =& 0\\
\Rightarrow \frac{\sum{\hat u_i}}{n} = \bar{\hat u_i} =& 0
\end{align}
1. $\sum X_i\hat u_i =  \sum(X_i - \bar X)(\hat u_i - \bar{\hat u_i})\Rightarrow \frac{ \sum(X_i - \bar X)(\hat u_i - \bar{\hat u_i})}{n-1} = cov(X_i, u_i) = 0$

???

\begin{align}
\sum X_i\hat u_i =& \sum(X_i - \bar X)(\hat u_i - \bar{\hat u_i})\\
                 =& \sum(X_i - \bar X)(\hat u_i - \bar{\hat u_i})\\
                 =& \sum [X_i (\hat u_i - \bar{\hat u_i}) - \bar X(\hat u_i - \bar{\hat u_i})]\\
                 =& \sum X_i (\hat u_i - \bar{\hat u_i}) - \sum\bar X(\hat u_i - \bar{\hat u_i})\\
                 =& \sum [X_i\hat u_i - X_i\bar{\hat u_i}] - \sum\bar X(\hat u_i - \bar{\hat u_i})\\
                 =& \sum X_i\hat u_i - \sum\bar X\hat u_i + \sum\bar{\hat u_i} \\
                 =& \sum X_i\hat u_i - n\bar X\sum\hat u_i + \sum\bar{\hat u_i}
\end{align}

---

class: small

## Standard Error of OLS Estimators

\begin{align}
\sigma^2 =& var(u_i|X)\\
         =& var(Y_i - \hat\beta_0 - \hat\beta_1X_i|X)\\
         =& \frac{\sum(\hat u_i^2)}{n - 2}\\
         =& \hat\sigma^2
\end{align}

n - 2: $\hat\beta_0, \hat\beta_1$

---
class: small

\begin{align}
var(\hat \beta_1|X) =& var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X)\\
                  =& var(\beta_1\sum k_iX_i + \sum k_iu_i|X)\\
                  =& var(\sum k_iu_i|X), \text{given}\ \beta_1\sum k_iX_i \text{is constant}\\
                  =& \sum var(k_iu_i|X), \text{assuming}\ u_i\ \text{independent}\\
                  =& \sum k_i^2 var(u_i|X)\\
                  =& \sum[\frac{X_i - \bar X}{\sum (X_i - \bar X)^2}]^2\sigma^2\\
                  =& \frac{\sum(X_i - \bar X)^2}{[\sum (X_i - \bar X)^2]^2}\sigma^2 = \frac{\sigma^2}{\sum (X_i - \bar X)^2}
\end{align}

If $u_i$ is not independent, then $cov(k, u) > 0$, and thus this estimator is underestimated.

---

class: small

\begin{align}
var(\hat \beta_0|X) =& var(\bar Y - \hat\beta_1\bar X|X)\\
                   =& var[\frac{\sum(\beta_0 + \beta_1X_i + u_i)}{n} - \hat\beta_1\bar X|X]\\
                  =& var(\frac{\sum u_i}{n}|X) + var(\hat\beta_1\bar X|X), \beta_0 + \beta_1X_i\text{constant and indp}\\
                  =& var(\frac{\sum u_i}{n}|X) + var(\hat\beta_1\bar X|X)\\
                  =& \frac{var(\sum u_i|X)}{n^2} + \bar X^2var(\hat\beta_1|X)\\
                  =& \frac{n\sigma^2}{n^2} + \frac{\bar X^2\sigma^2}{\sum (X_i - \bar X)^2}\\
                  =& \sigma^2[\frac{1}{n} + \frac{\bar X^2}{\sum (X_i - \bar X)^2}]\\
                  =& \sigma^2[\frac{\sum (X_i - \bar X)^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}] = \sigma^2[\frac{\sum X_i^2 - \sum\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}]\\
                  =& \sigma^2[\frac{\sum X_i^2 - n\bar X^2 + n\bar X^2}{n\sum (X_i - \bar X)^2}]= \sigma^2\frac{\sum X_i^2}{n\sum (X_i - \bar X)^2}
\end{align}