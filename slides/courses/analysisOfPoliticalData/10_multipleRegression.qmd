---
title: "Multiple Regression"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
knitr: 
    opts_chunk: 
      echo: false
editor_options: 
  chunk_output_type: console
format: 
  revealjs:
    
    number-sections: true
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: true
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
revealjs-plugins:
  - spotlight

lightbox: 
  match: auto
  effect: fade

spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---


## Overview {.unnumbered}


```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools
) 


# Functions preload
set.seed(114)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```


:::{.nonincremental}
1. Multiple regression
1. Goodness of fit: R<sup>2</sup>
1. Post-estimation inferences
:::

# Multiple regression

## Expression

:::{style="text-align:center"}
Terminology: Multivariate vs. Multiple
:::

:::: {.columns}

::: {.column width="50%"}
:::{.fragment}
Population Regression Function (PRF, two explanatory variables): $$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i.$$
:::


:::{.fragment style="text-align:center"}
According to CLRM: $E[\epsilon_i|\boldsymbol{X}] = 0.$
:::


:::{.fragment}
Sample Regression Function (SRF): $$Y_i = \hat\beta_0 + \hat\beta_1X_{1i} + \hat\beta_2X_{2i} + \hat \epsilon_i$$
:::
:::

::: {.column width="50%"}
![](https://drhuyue.site:10002/sammo3182/figure/mr_2x.png){.fragment fig-align="center" height=500}
:::

::::


## How multiple regressions work intuitively

![](https://drhuyue.site:10002/sammo3182/figure/mr_fit.gif){.r-stretch fig-align="center"}



## Meaning of &beta;<sub>1</sub>{.smaller}

:::: {.columns}

::: {.column .fragment width="20%"}
To be [BLUE]{.blue},

$min_{\hat\beta_0,\hat\beta_1,\hat\beta_2}\sum \hat \epsilon_i^2$

\begin{align}
\Rightarrow\frac{\partial \sum \hat \epsilon_i^2}{\partial\hat\beta_0}\to& \color{darkred}{0}\\
\frac{\partial \sum \hat \epsilon_i^2}{\partial\hat\beta_1}\to& \color{darkred}{0}\\
\frac{\partial \sum \hat \epsilon_i^2}{\partial\hat\beta_2}\to& \color{darkred}{0}
\end{align}
:::

::: {.column .fragment width="80%"}
When being [BLUE]{.blue}:

\begin{align}
\hat\beta_0 =& \bar Y - (\bar\beta_1X_{1i} + \bar\beta_2X_{2i}),\\
\hat\beta_1 =& \frac{[\sum(Y_i - \bar Y)(X_{1i} - \bar X_1)][\sum(X_{2i} - \bar X_2)^2-\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]}{\sum(X_{1i} - \bar X_1)^2\sum(X_{2i} - \bar X_2)^2 - [\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]^2}\\
            =& \frac{\sum\hat r_{1i}(Y_i - \bar Y)}{\hat r_{1i}^2},
\end{align} where the $\hat r_{1i}$ are the errors from the regression of $X_{1i}$ on $X_{2i}$ (i.e., $X_{1i} = \hat\delta_0 + \hat\delta_1X_{2i} + \hat r_{1i}$), **the proportion that $X_2$cannot explain.**

$\hat\sigma^2 = \frac{\sum\hat \epsilon_i^2}{n - 3}.$
:::

::::


:::{.callout-important .fragment .large}
## Interpretation

Every unit change in X<sub>1</sub> leads to &beta;<sub>1</sub> changes in Y [on average]{.red}, [ceteris paribus/holding everything else constant]{.red}.
:::

:::{.fragment style="text-align:center"}
*Constant how? Sufficient or necessary condition of the variance of Y?* 
:::



:::{.notes}
changed the same amount on average
hold on mean or median
:::


# Goodness of Fit

## R<sup>2</sup>: Multiple Coefficient of Determination

\begin{align}
Y_i =& \hat Y_i + \hat \epsilon_i;\\
Y_i - \bar Y =& \hat Y_i  - \bar Y + \hat \epsilon_i;\\
\Rightarrow (Y_i - \bar Y)^2 =& (\hat Y_i  - \bar Y + \hat \epsilon_i)^2,\\
                         =& (\hat Y_i  - \bar Y)^2 + \hat \epsilon_i^2 +, 2\hat\epsilon_i(\hat Y_i  - \bar Y).\\
\text{Sum up, } \Rightarrow \sum(Y_i - \bar Y)^2 =& \sum(\hat Y_i  - \bar Y)^2 + \sum\hat\epsilon_i^2.\\
SST =& SSR + SSE;\\
1 =& \frac{SSR}{SST} + \frac{SSE}{SST}.\\
\text{In which, } R^2 =& \frac{\sum(\hat{Y} - \bar Y)^2}{\sum(Y - \bar Y)^2} = \frac{SSR}{SST} = \frac{\sum(\hat Y_i  - \bar Y)^2}{SST},\\
                      =& \frac{\sum[\hat\beta_0 + (\hat\beta_1X_1 +\cdots +\hat\beta_nX_n)  - \bar Y]^2}{SST}.
\end{align}

:::{.notes}
- SST: Sum of Squares Total
- SSR: Sum of Squares Regression
- SSE: Sum of Squares Error.

Different from ANOVA:

- SST: Sum of square treatment
- SSE: Sum of square error
- SS: Sum of square
:::

---

![](https://drhuyue.site:10002/sammo3182/figure/mr_bad4ya.jpg){.r-stretch fig-align="center" height=600}


## Why Is R<sup>2</sup> Bad? 

> Reason 1: Can be very [low]{.red} for a correct model

:::: {.columns}

::: {.column width="50%"}

\begin{align}
R^2 =& \frac{\sum(\hat Y_i  - \bar Y)^2}{SST}, \\
    =& \frac{\sum[(Y_i - \color{darkred}{\epsilon_i}) - \bar Y]^2}{SST}.
\end{align}

:::

::: {.column width="50%"}
When the residual (thus &sigma;, estimated by &epsilon;<sub>i</sub> in a sample) is large enough, R<sup>2</sup> could approach a very low score towards zero.
:::

::::


```{r rsmall}
#| fig-height: 3
#| fig-align: "center"


r2 <- function(sig){
  x <- seq(1,10,length.out = 1000)        # our predictor
  y <- 2 + 1.2*x + rnorm(1000,0,sd = sig) # our response; a function of x plus some random noise
  summary(lm(y ~ x))$r.squared           # print the R-squared value
}

df_r <- tibble(sigma = seq(0.5,20,length.out = 20), 
               rout = map_dbl(sigma, r2))

ggplot(data = df_r, aes(x = sigma, y = rout)) +
  geom_point() + 
  ylab(expression(R^2)) + 
  xlab(expression(sigma)) +
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold")
)
```

## Why Is R<sup>2</sup> Bad? 

> Reason 2: Can be very [high]{.red} for a misspecified model

```{r rlarge, fig.align='center'}
df_rLarge <-
    tibble(x = rexp(50, rate = 0.005),
           # from an exponential distribution
           y = (x - 1) ^ 2 * runif(50, min = 0.8, max = 1.2))# non-linear data generation

rout <-
    summary(lm(y ~ x, data = df_rLarge))$r.squared %>% round(digits = 2)

ggplot(data = df_rLarge, aes(x, y)) +
    geom_point() +
    xlab("X") +
    ylab("Y") +
    labs(
        title = expression("True model:" ~ Y == (X - 1) ^ 2),
        subtitle = expression("Estimated model:" ~ Y == beta[0] + beta[1] *
                                  X + sigma),
        caption = bquote(R ^ 2 == ~ .(rout))
    ) +
    theme(
        plot.title = element_text(size = 28),
        plot.subtitle = element_text(size = 20),
        plot.caption = element_text(size = 20),
        axis.text = element_text(size = 20),
        axis.title = element_text(size = 20, face = "bold")
    )
```

## Why Is R<sup>2</sup> Bad?

> Reason 3: Can be very [high]{.red} for a redundant model

$$R^2 = \frac{\sum(\hat Y_i - \bar Y)^2}{SST} = \frac{\sum[\hat\beta_0 + (\hat\beta_1X_1 +\cdots +\hat\beta_nX_n) - \bar Y]^2}{SST}.$$

Therefore, the more Xs are added, the larger SSR (and thus R<sup>2</sup>) is, a.k.a., the "trash-can" model.

:::{.notes}
$\hat Y_i  - \bar Y = 0$
:::

:::{.fragment}
**Revised statistics:**

$$\text{Adj. } R^2 = 1 - (1 - R^2)\frac{n - 1}{n - k - 1}.$$
:::

## So, is Adj. R<sup>2</sup> good?


$$\text{Adj.} R^2 = 1 - (1 - R^2)\frac{n - 1}{n - k - 1}.$$

:::: {.columns}

::: {.column width="50%"}
**Adjusted**

X booming
:::

::: {.column .nonincremental width="50%"}
**Not adjusted**

* Goodness of fit;
* Predictive error;
* Model comparison;
* X's explanatory power.
:::

::::

:::{style="text-align:center; margin-top: 2em"}
*When can R<sup>2</sup> be useful then?*
:::


:::{.notes}
Model comparison
:::


# Post-Estimation Inferences

## Predicted Value 

**Goal**

1. Forecast
1. Interpretation: 
    + How is the model close to the reality?
    + What extent of *substantive* changes can Xs make?

:::{.fragment style="text-align:right"}
**Approach**

1. Expected value (average) of $\hat Y$
1. A one-time draw of $\hat Y$
1. Hypothesis testing
:::


## Forecasting: Expected Value

Let X<sub>0</sub> be the values of interest, and then calculate the $\hat{Y}$:

\begin{align}
E(\hat Y_0|X_0) =& E(\hat Y_0|X = X_0) = \boldsymbol{X_0\beta}\\
var(\hat Y_0|X_0) =& var(\hat\beta_0) + var(\hat\beta_1)X_0^2 + 2cov(\hat\beta_0, \hat\beta_1)X_0\\
                  =& \sigma^2[\frac{1}{n} + \frac{(X_0 - \bar X)^2}{\sum(X_i - \bar X)^2}].
\end{align}

```{r predicted, fig.align='center'}
#| fig-height: 3

ggplot(mtcars, aes(x = mpg, y = wt)) +
    geom_smooth(method = lm, se = TRUE) +
    ylab(bquote(hat(Y[0]))) +
    xlab(bquote(hat(X[0]))) +
    labs(caption = "The ribbon represents the standard error.") +
    theme(
        plot.title = element_text(size = 28),
        axis.text = element_text(size = 20),
        axis.title = element_text(size = 20, face = "bold")
    )
```

:::{.fragment style="text-align:center"}
Why is the ribbon wider at the two ends?
:::



:::{.notes}
Fewer observations at two terminals, thus wider rainbow.
:::

## Forcasting: Single-Point Forecast

\begin{align}
\hat Y_0 =& \hat\beta_0 + \hat\beta_1X_0 + \hat u\\
var(\hat Y_0|X_0) =& \sigma^2[\color{darkred}{1} + \frac{1}{n} + \frac{(X_0 - \bar X)^2}{\sum(X_i - \bar X)^2}].
\end{align}

:::{.fragment}
There is an extra error term to account for. In other words, single prediction is [more uncertain]{.red} than the average prediction.
:::

## Hypothesis Test: Coefficient and variance

Let &alpha; = 0.05.

:::: {.columns}

::: {.column width="50%"}

*1. On the coefficient*

Hypothesis:

\begin{align}
H_0: \beta =& \beta^*;\\
H_1: \beta \neq& \beta^*.
\end{align}


Statistics:

$$\frac{\hat\beta - \beta^*}{\sqrt{\frac{\hat\sigma^2}{\sum(X_i - \bar X)^2}}}\sim t_{n-k}.$$

:::

::: {.column .fragment width="50%"}
*2. On the variance*

Hypothesis:

\begin{align}
H_0: \sigma =& \sigma^*;\\
H_1: \sigma \neq& \sigma^*.
\end{align}


Statistics:

$$(n - k)\frac{\hat\sigma^2}{\sigma^2}\sim\chi^2.$$
:::

::::


## {.smaller .unnumbered}

[3. Model structure: Restricted Model]{.nonincremental}

Let's set &alpha; = 0.05, $H_0: \beta_1 + 2\beta_2 = 3\Rightarrow \beta_1 = 3 - 2\beta_2; H_1: \beta_1 + 2\beta_2 \neq 3.$

:::{.fragment}
Then, 

\begin{align}
Y =& \beta_0 + \beta_1X_1 + \beta_2X_2 + u, \text{(unrestricted)},\\
  =& \beta_0 + 3X_1 + \beta_2(X_2 - 2X_1) + u;\\
\Leftrightarrow Y - 3X_1 =& \beta_0 + \beta_2(X_2 - 2X_1) + u;\\
Y^* =& \beta_0' + \beta_2'Z + u, \text{(restricted)},
\end{align} where $Y^*=Y - 3X_1; Z = X_2 - 2X_1.$

The test is thus transformed to $H_0: \beta_2' = \beta_2; \beta_0' = \beta_0$.
:::


:::{.fragment}
Statistics:

$$\frac{\frac{SSR_R - SSR_U}{\Delta k}}{\frac{SSR_U}{n - k_U - 1}} = \frac{\frac{R_U^2 - R_R^2}{\Delta k}}{\frac{1 - R_U^2}{n - k_U - 1}}\sim F_{\Delta k, n - k - 1}$$
:::


:::{.callout-important .fragment}
If the hypothesis is rejected, the unrestricted model is [better]{.red}.
:::


:::{.notes}
Hint: When testing general linear restrictions, that is, comparing the pair of nested models with different Ys, one should not use R<sup>2</sup> to deduct the F test, because $SST_U \neq SST_R$ in this case.
:::



## "When" to use OLS

{{< video https://drhuyue.site:10002/sammo3182/video/mr_linearRegression.mp4 title="Just kidding" height=600 preload="auto" controls allowfullscreen>}}


## Take-home point

![](https://drhuyue.site:10002/sammo3182/figure/mr_mindmap.png){.r-stretch}

# Appendix: Latin phrases {.appendix .unnumbered}

## Common Latin Phrases <img src = "https://drhuyue.site:10002/sammo3182/figure/ci_fsmrof.png" height = 30>{.unnumbered}

- *a fortiori*: Not even saying.    
- *ad hoc*: To this, immediate purpose.   
- *ibid.*: "ibidem," in the same place.   
- [*ceteris paribus*]{.red}: with other conditions - remaining the same.    
- *c.f.*: "confer," compare   
- [*e.g.*]{.red}: "exempli gratia," for the sake of - example.    
- *etc.*: "et cetera," and the rest.    
- *i.e.*: "id est," that is.    
- [*n.b./NB*]{.red}: "nota bene," note well.    
- *per se*: By/of/for/in itself.    
- [*QED*]{.red}: $\blacksquare$ "quod erat demonstrandum," that which was to have been shown.    

# Appendix {.appendix .unnumbered}

## Stretch {.unnumbered}

{{< video https://drhuyue.site:10002/sammo3182/video/relax.mp4 title="Yoyo孙佳祺:拯救久坐党体态！办公间隙的5分钟拉伸，重新挺拔起来～！" height=600 preload="auto" controls allowfullscreen>}}

:::{.notes}
https://www.bilibili.com/video/BV1E54y1r76o/?buvid=Y045340D2FD9FA3A46419EEFE4578279ECBD&from_spmid=main.space-search.0.0&is_story_h5=false&mid=7RnjBONLRMus4FQZFBWD2g%3D%3D&p=2&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=79FE08C9-F2B1-4C18-A16C-B31179817B42&share_source=WEIXIN&share_tag=s_i&timestamp=1728732278&unique_k=skSWw7j&up_id=390316092&vd_source=f38aeefd0d38cecba9017eeee43e71c8
:::

## Meditation {.unnumbered}

:::{style="text-align:center; margin-top: 2em"}
![松茸的世界：5分钟正念冥想-自信之心](https://drhuyue.site:10002/sammo3182/figure/ci_songrong.jpg){fig-align="center" height=400}

<audio controls preload="auto" src="https://drhuyue.site:10002/sammo3182/video/meditation.m4a" width=900></audio>
:::


:::{.notes}
https://www.xiaoyuzhoufm.com/episode/65b752ed0bef6c207457f51b?s=eyJ1IjogIjYwNDA1OGJiZTBmNWU3MjNiYmY3MjZiZSJ9
:::