---
title: "Principles of<br>Text Analysis"
subtitle: "Class Advanced Methods School, CityU"
# date: "2023-06-13"
# date-format: "iso"
author: "Yue Hu"
institute: 
  - Department of Political Science
  - Tsinghua University

bibliography: pre_cams.bib

format: 
  revealjs:
    css: https://sammo3182.github.io/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    embed-resources: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    callout-appearance: simple
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    showNotes: false # 'true' or 'separate-page' if you want notes on a separate page
    code-fold: true
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png

knitr: 
    opts_chunk: 
      dev.args:
        bg: transparent

execute:
  echo: true
  message: false
  warning: false

revealjs-plugins:
  - spotlight
lightbox:
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
  
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include = FALSE

library(pacman)

p_load(
  icons,
  tidyverse, 
  flextable, gridExtra,
  rvest, stringr, broom, jiebaR, tidytext, tidyverse
)

# Functions preload
set.seed(313)

theme_set(theme_minimal())
```

# Outline

- Text-As-Data Approach
- DGP for Text-As-Data
- Analyzing text data

# Text-As-Data Approach

## When text becomes (big) data

- Yearly: Social media users increase 56,530,000 in 2017--18.
- Daily: 
    + Baidu ("Chinese google") [day]{.red} user search request, it takes [1.7 days]{.red} to scan once;
    + Wechat ("Chinese whatsup") [day]{.red} added data [500TB]{.red}---more than all human books.
- Timely: 2.9 million emails around the world [every second]{.red}
    - A person's [5.5 years]{.red} of day and night to read them all.


:::{.fragment}

:::{.callout-tip collapse="true"}
By 2020, the size of global data has reached 40ZB (5.2TB/person).^[
[<sup>1</sup> 1ZB = 1,024EB = 1,024<sup>2</sup>PB = 1,024<sup>3</sup>TB  = 1024<sup>4</sup>GB---reference: 1080p HD 2hrs：6GB]{.fragment}
]
:::

:::

:::{.notes}
http://pdf.dfcfw.com/pdf/H3_AP201903111304577789_1.pdf

Giga Byte < Tera Byte < Peta Byte < 
Exa Byte < Zetta Byte < Yotta Byte
:::

## History of text as data

- "Ancient" studies
    - How the law resolves externality problems [@Coase1960]
    - Construction of policy surprises via historical documents [@FriedmanSchwartz2008]

:::{.fragment}
- Difficulties
    - Maintaining and update
    - Analytic methods
    - Time consumption
    - Generalization
:::


## Boom of Text Analysis in the age of big data

+ Text data grows [exponentially]{.red}
+ Large-scale text data [collection]{.red}
+ Enhanced [storage and management]{.red} capabilities
+ Extensive [social meaning]{.red} expressed through text
+ Scalable, well-developed and economic [methods]{.red}

## Text-as-data perspective

:::: {.columns}

::: {.column width="20%"}
*Objective*

~~Text~~ Language
:::

::: {.column width="80%"}
*Type*

* Text analysis vs. content analysis
* Representational analysis vs. Instrumental analysis
* Thematic analysis vs. semantic analysis

:::{.notes}
Representational 精确解码，关注外显（manifest）；
Instrumental 探索意图，关注隐含（latent）
Thematic 概念是否出现及何种关系，基于词频和向量；
Semantic 识别主题间的具体关系；考虑语法、逻辑等

语法学(syntax, how to say it)
语义学(semantic, what is said)
语用学(pragmatic, what is implicated)
:::
:::


::::


## Illustration I

@Grimmer2010

- Objective
    - The priorities political actors emphasize in statements
- Data
    - An original collection of over 24,000 Senate press releases in 2007
- Method
    - Bayesian Hierarchical Topic Model

## Findings

::: {.panel-tabset}
### Focus


![Committee leaders focus on their committee's issues](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_grimmer1.png){.lightbox fig-align="center" height=500}

:::{.notes}
Fig. 4 Chairman and ranking members of committees allocate more attention to issues under their committees' jurisdiction than other senators. **This figure compares the attention that Senate committee leaders---chairs or ranking members---dedicate to topics under their committee jurisdictions to the attention allocated by the rest of the Senate.** The solid dots represent the expected difference, the thick lines are 80% credible intervals, and the thin lines are 95% intervals. Along the left-hand vertical axis, the topics are listed, and on the right-hand side, the corresponding committee names are listed. In all but seven cases, the dot is to the right of the zero line, indicating that leaders of committees discuss issues that highlight their power in the institution more often than other senators.
:::


### Cluster

![Expressed agendas cluster geographically](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_grimmer2.png){fig-align="center" height=500}



:::{.notes}
Fig. 5 Attention to issues follows expected geographic patterns. **This figure demonstrates that senators' expressed agendas are grouped geographically.** The left-hand plot shows that senators from western states allocate substantial attention to public-land issues. Darker shades indicate that the average expected attention from the state's delegation to public-land issues is larger. The center plot carries out a comparison of three different regional issues: public-land and western states (top estimate), hurricanes and gulf coast states (middle estimate), and border-security and states that share an international border (bottom estimate). The point in each plot represents the expected difference between the attention to senators in a geographic area allocate to an issue and the attention senators from other areas of the country dedicate to the same issue. The thick and thin lines are 80% and 95% HPD intervals for this difference. Each point is to the right of the zero, indicating that the issues receive more attention in the geographic areas we would expect. The right-hand plot shows that senators from states with a large number of farms per person also tend to allocate more attention to agriculture issues. The horizontal axis represents the number of farms per resident of the state (one measure of agriculture's importance to a state), and the vertical axis indicates the proportion of press releases allocated to agricultural issues. The gray lines are lowess curves indicates the relationship between the number of farms per capita and the attention to agriculture, whereas the black line is the average relationship.
:::

### Predict

![Attention to appropriations predicts opposition to earmark reform](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_grimmer3.png){fig-align="center" height=500}



:::{.notes}
An earmark is a provision inserted into a discretionary spending appropriations bill that directs funds to a specific recipient while bypassing the merit-based or competitive funds allocation process. These provisions are often associated with legislation specifying certain congressional spending priorities or in revenue bills that apply to a limited number of individuals or entities. 

The DeMint-McCain amendment was a significant proposal related to earmark reform. Senator Jim DeMint introduced a one-year earmark moratorium amendment, which was co-sponsored by both Republicans and Democrats. The amendment aimed to end the era of earmarks and was seen as a step towards increased transparency, accountability, and fiscal responsibility in Congress. Despite its support from some senators and advocacy groups, the amendment was defeated in a late-night Senate vote, with a result of 29-71.

Fig. 6 Senators who dedicate more attention to appropriations were more likely to oppose Demint-McCain. This figure shows that **senators who dedicate more attention to appropriations in their press releases are more likely to oppose the Demint-McCain amendment.** The vertical axis plots the vote on the amendment, and along the horizontal axis is the average proportion of press releases dedicate to discussing appropriations secured for fire departments. To generate the light gray lines, I took 100 draws from each senator's posterior expressed agenda and then regressed the earmark vote on the draw from the posterior. The gray lines represent the expected probability of supporting the DemintMcCain amendment, and the solid black line is the expected value of the relationship, averaged over the draws from the posterior distribution on the expressed agenda. 

The left-hand figure shows that senators who discuss fire department grants more often were more likely to oppose the DemintMcCain amendment, and the center plot shows that this relationship was even stronger for an aggregate appropriations category. The right-hand plot shows that the relationship remains even after conditioning upon estimated ideal points of senators, suggesting that consistency explains components of voting behavior beyond ideal point estimates.
:::
:::


## Illustration II

@BenoitEtAl2016

- Objective
    - Professionals vs. Masses
- Method
    - Crowd-sourced identification
- Data
    - 18,263 natural sentences from *British Conservative, Labour and Liberal Democrat* manifestos
    

## Operation

::: {.panel-tabset}
### Coding

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_crowd-sourced.jpg){fig-align="center" height=500}

:::{.notes}
Figure 1: Hierarchical coding scheme for two policy domains with ordinal positioning.
:::

### Results

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_expert-crowd.png){fig-align="center" height=500}

:::{.notes}
Figure 2. British party positions on economic and social policy 1987 – 2010; sequential expert text processing (vertical axis) and independent expert surveys (horizontal).
:::

:::



## Illustration III

@DietrichEtAl2019

- Objective
    - Speakers' emotional state
- Method
    - Analyses of vocal pitch
- Data
    - 74,158 Congressional floor speeches

## Operation

:::{.r-stack}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_vocalPitch_hcy.png){fig-align="center" height="500"}

:::{.fragment}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_vocalPitch.png){.lightbox fig-align="center" height="600"}
:::

:::


## Illustration IV

@ZhangPan2019

- Objective
    - Group activities from social media
- Method
    - CNN for images; CNN-RNN for texts
- Data
    - A random sample of 20,000 geocoded posts from Weibo, 2010--2017

:::{.notes}
CNN: Convolutional Neural Network

RNN: Recurrent Neural Network
:::

## Operation

:::{.r-stack}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_casm.png){fig-align="center" width="1000"}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_runningMan.png){.fragment fig-align="center"}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_banner.png){.fragment fig-align="center" height=500}
:::


## Challenges

:::: {.columns}

::: {.column width="50%"}
*Theory*

- Single causal mechanism?
    - Intentional writing vs. measurement errors

*Assumption*

- A bag of words (elaborated later)
:::

::: {.column width="50%"}
*Data*

- Unstructuralized data
- Manifest and latent dimension reduction
- Concept relations and context ignorance
- DGP
    - Only posted
    - One-time censor?
    - Random sampling?
:::

:::{.notes}
审查已经使用 machine learning @SuEtAl2020
:::

::::

## Wrap it up

- History of text-as-data
    - Long and short
- Text-as-data perspectives
    - Objective: language
    - Types
- Application
    - Text &rarr; audio, video

# DGP for Text-As-Data

## Points to cover

1. Where to find text resources?
    1. First-hand sources
    1. Second-hand sources
1. How to get text resources?
1. How to deal with text information?
1. What can text data do?



## First-hand sources

:::{style="text-align:center; margin-top: 2em"}
*Text from authors*

* Email/SMS
* Website HTML
* RSS feeds
* Internet social media
* Internet Forum
* Network question and answer platform
* Media database
* Internet transaction behavior       
......
:::

## Open Platforms

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_zhongsheng_index.png){fig-align="center" height=600}

## Media database

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_renminribao.png){fig-align="center" height=600}

## E-Government

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_henan-Egovernment.png){fig-align="center" height=600}

## Social Media

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_social-media.gif){fig-align="center" height=600}

:::{.notes}
新浪微博1.4亿，微信用户5.5亿;
微信日增数据500TB，QQ日增数据200TB。
:::

## Reddit

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_zhihu.png){fig-align="center" height=600}

:::{.notes}
Quora, Stack Overflow
:::

## Open question

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_survey_openQuestion.png){fig-align="center" height=600}


## Second-hand sources

:::{style="text-align:center; margin-top: 2em"}
*Text from archives*

* CNKI and other databases (journals, newspapers, yearbooks, etc.)
* Google Books, Baidu Scholar
* Google Trend, Baidu Index
* JSTOR Data for Research   
......
:::

## Indicators

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_baidu_zhishu.png){fig-align="center" height=600}

## Digitalized documents

:::{.r-stack}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_hongkong_lib.png){fig-align="center" height=600}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_cbdb.png){.fragment fig-align="center" height=600}
:::

## Text data collection

- First-hand: Spider/crawler/scraper
- Second-hand: archive and digital database


:::{.r-stack}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_bazhuayu.png){.fragment fig-align="center" height=450}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_huochetou.png){.fragment fig-align="center" height=450}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_gooseeker.png){.fragment fig-align="center" height=450}
:::

## Scraper operation

:::{.r-stack}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_gooseekerI.png){.fragment fig-align="center" height=600}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_gooseekerII.png){.fragment fig-align="center" height=600}

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_gooseekerIII.png){.fragment fig-align="center" height=600}
:::

## Customized Scrapping

`SelectorGadget` (Chrome add-in)

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_zhongsheng_page.png){fig-align="center" height=500}

## Scrapping with `rvest`

```{r zhongsheng-eg}
#| eval: false

ls_zhongsheng <-
  read_html("http://politics.people.com.cn/GB/8198/426918/index.html") |> # index page
  html_nodes("h5 a") |> # the nodes of the links
  html_attr("href") |> # just the links
  str_replace("^/n1", "http://politics.people.com.cn/n1")

df_zhongsheng <- map_df(ls_zhongsheng, function(link) {
  zs_article <- read_html(link, encoding = "GB18030") # read the html
  
  zs_title <- html_nodes(zs_article, "h1") |>
    html_text
  
  zs_time <- html_nodes(zs_article, ".box01 .fl") |>
    html_text |>
    str_extract("\\d{4}年\\d{2}月\\d{2}日")
  
  zs_content <- html_nodes(zs_article, "#rwb_zw p") |>
    html_text |>
    str_c(collapse = "") |> # combined the paragraphs
    str_remove_all("\\s|\\n|\\t") # remove the horizontal spaces
  
  zs_data <- data.frame(title = zs_title,
                        time = zs_time,
                        content = zs_content)
})
```

## Regular Expression

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_regular_expression.png){fig-align="center" height=600}

## Structuralization

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_zhongsheng_output.png){fig-align="center" width=1000}

## Wrap it up

- Source of text data
    - First-hand
    - Second-hand
- Data collection
    - First-hand: Scrapping
        - Foolproof software
        - Programming
            - Regular expression
    - Second-hand: Archives
- Structuralization

# Analyzing text data

## An (Classic) Overview [@GrimmerStewart2013]

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_analysis-method.png){fig-align="center" height=600}

## Level of Analysis

:::{style="text-align:center; margin-top: 2em"}
:::{.fragment .fade-in-then-semi-out}
*Description*

Frequency, word cloud (👎), network
:::


:::{.fragment .fade-in-then-semi-out}
*Classification*

Supervised, unsupervised, self-supervised
:::

:::{.fragment}
*Semantics*

Sentiment analysis, word embedding, word sequence, relationship
:::
:::

## Tenet of Text-as-data analysis [@GrimmerStewart2013]

:::{style="text-align:center; margin-top: 2em"}
1. All quantitative models of language are [wrong]{.red}---but some are useful. 
1. Quantitative methods for text amplify resources and [augment humans]{.red}. 
1. There is [no globally best]{.red} method for automated text analysis. 
1. [Validate, Validate, Validate.]{.fragment .highlight-blue}
:::

## Unit of Analysis

:::: {.columns}

::: {.column width="50%"}
- Corpus
    - Document (volumn, chapter, section)
        - Paragraph
            - Sentence [&check;]{.green}
                - Clause [&check;]{.green}
                    - **Word (Unigram)** [&check;]{.green}
                        - **Token** [&check;]{.green}
:::

::: {.column .fragment width="50%"}
:::{.callout-tip}
## Token

Sequence of linguistic features
:::

- Token in a document: term
- Token in a group: N-gram 
    - A word = a Unigram

:::

::::

## Unrealistic Assumption

> Where the "all models are wrong" starts

:::{.callout-important}
**Bag of word**

A text is represented as the bag (multiset) of its words.
:::

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_bagOfWords.png){.fragment fig-align="center" height=300}

## From natural language to computational language

![Document-Term Matrix (DTM)](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_bagOfWords2.jpg){fig-align="center" height=600}


:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## Preprocessing

:::{style="text-align:center; margin-top: 2em"}
- Segmentation
- Tokenization
- Stopwords/function words removing   
- Other project-oriented wrangling
:::


## Segmentation

:::: {.columns}

::: {.column width="30%"}
*Scriptio discreta* (e.g., English)

Document &rarr; paragraphs/sentences

:::{.notes}
discrete script
:::

:::

::: {.column .fragment width="70%"}
*Scriptio continua* (e.g, CJK) 

```{r segment}
#| code-fold: false

library(jiebaR)

zhongsheng <- "近年来，美国一些政客被“美国优先”遮住了双眼，大搞贸易保护主义、单边主义，肆意挥舞关税大棒，全然不顾中美两国人民和全世界人民的强烈反对。"
cutter <- worker() # segment engine

segment(zhongsheng, cutter)
```
:::

:::{.notes}
continuous script

《人民日报》（2019年05月30日02版）
:::


::::


## Tokenization

- Goal: Removing syntax
- Removing: capitalization, punctuation, non-alphanumeric characters (@#$%……&*), stop words

:::{.fragment}

Examples of Stopwords*

```{r stop-words}
#| echo: false

read_lines(STOPPATH)[883:903]
read_lines(STOPPATH)[157:177]
```
:::


:::{.fragment}
:::{.callout-tip}
## Stopwords sources for Chinese

* 百度停词表
* 哈工大停词表
* 网络搜集停词表，如["最全中文停用词表整理（1893个）"](https://blog.csdn.net/shijiebei2009/article/details/39696571)
:::
:::


## Dimension reduction

> Issue: Sparse matrix

- Lemmatization: (happy, happier, happiness) &rarr; happy
- Stemming: (happy, happier, happiness) &rarr; happi

:::{.fragment}
:::{.callout-tip}
## More Examples

- Lemmization: (went, leaves, geese, unhappy) &rarr; (go, leaf, goose, unhappy)
- Stemming: (went, leaves, geese, unhappy) &rarr; (went, leav, geese , unhappi)
:::
:::

:::{.fragment}
- Labeling
    - Content vs. function
    - Linguistic features: n., v., adj., adv., prep., conj.......
:::

## A comprehensive application

![2019-05-14 ~ 05-22: China-US trade war](http://103.238.162.29:10002/webdav/sammo3182/figure/text_zhongsheng.png){fig-align="center" height=600}

## Data collection

*Raw Data*

```{r zhongsheng-clean}
df_zhongsheng <- readRDS(url("http://103.238.162.29:10002/webdav/sammo3182/data/zhongsheng.RDS"))

df_zhongsheng$segmented <- map_chr(df_zhongsheng$content, function(content){
  segment(content, cutter) |> paste(collapse = " ")
})

df_zhongsheng$phase <- "US_fail"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-22"] <- "theory_test"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-11"] <- "reassessment"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-08"] <- "optimism"


df_token <- df_zhongsheng |> 
  select(-content) |> 
  unnest_tokens(word, segmented) # tokenization

# Show the word counts
select(df_token, -phase) |> head()
```

:::{.fragment}
*Removing the stopwords*

```{r zhongsheng-stop}
#| code-fold: true

# removing the stop words
df_stopWords <- tibble(word = read_lines(STOPPATH))

df_token <- df_token |> 
  anti_join(df_stopWords) 

select(df_token, -phase) |> head()
```

:::

## Frequency Analysis

```{r zhongsheng-frequency, fig.align='center'}
df_plot <- df_token |> count(word, sort = TRUE) |> 
  filter(n > 151) |>
  mutate(word = reorder(word, n))

ggplot(df_plot) +
  geom_col(aes(word, n)) +
  ylab("Word Frequency") + 
  theme(axis.text = element_text(size = 20)) +
  coord_flip()
```

## Frequency Dynamics

```{r zhongsheng-phase-freq, out.width="100%"}
df_plot <- df_token |> 
  group_by(phase) |> 
  count(word, sort = TRUE) |> 
  top_n(10) |>
  ungroup() |> 
  mutate(word = reorder(word, n),
         word = factor(word, levels = rev(unique(word))))

ggplot(df_plot, aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("Word FRequency") + 
  theme(axis.text = element_text(size = 12)) +
  facet_wrap(~ phase, scales = "free")
```

:::{.notes}
The Economist journalist Simon Rabinovitch
:::

## Frequency Correlations

```{r zhongsheng-fre-compare, out.width="100%"}
frequency <- df_token |> 
  count(phase, word) |>
  group_by(phase) |>
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  spread(phase, proportion) |> 
  gather(phase, proportion, optimism:theory_test)

library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `US_fail`, color = abs(`US_fail` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, family = "WenQuanYi Micro Hei") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~phase, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "US Inevitable Failure", x = NULL) +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

:::{.notes}
x = proportion, y = `US_fail`

proportion = n / sum(n)
:::

## Keyword analysis

Keywords for articles published on 2019-05-07

```{r keyword}
extractor_keyword <- worker("simhash", topn = 5)

simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-07"], extractor_keyword)
```

. . .

Keywords for articles published on 2019-05-30

```{r keword2}
simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"], extractor_keyword)
```


## Similarity analysis

Measuring the distance between texts

> Hamming distance: the distance between two strings of equal length is the number of [positions]{.blue} at which the [corresponding symbols are different]{.red}. 

:::{.r-stack}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_hammingDis_org.png){fig-align="center" height=num}
![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_hammingDis_comp.png){fig-align="center" height=num}
:::

e.g., H(100→011) = 3; H(010→111) = 2.

:::{.notes}
It measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.
:::

## Application

:::: {.columns}

::: {.column width="50%"}
*05-30 vs. 05-[22]{.red}*

```{r keyword-distnace}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-22"],
         extractor_keyword)
```
:::

::: {.column width="50%"}
*05-30 vs. 05-[23]{.red}*

```{r keyword-distance2}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-23"][1],
         extractor_keyword)
```
:::

::::

## Beyond "Bag of Words"

> Bring the context back

- Analysis of function words (stopwords, parts of speech)
- Neighbor words:  Markov Model of Order N
    + Unigram: 清华 大学 政治 系
    + Bigram: 清华大学 政治系/清华 大学政治 系
    + Trigram：清华大学政治 系/清华 大学政治系


## Frequency to Weighted Frequency

Cons of DTM

1. Not distinguish important words
2. Over-emphasize common words
3. Under-emphasize rare words

[Solution: Word weights: e.g., term frequency-inverse document frequency (TF-IDF)]{.fragment}

## Word Weight

- Pros: 
  1. Identify important words
  2. Reduces the weight of common words
  3. Improve the performance of machine learning models

:::{.fragment}
- Cons:
  1. Assume independence of terms (same as DTM)
  2. Give high weight to rare words that is very rare in the corpus but appears a few times in a particular document
  3. Sensitive to the size and diversity of the corpus
:::



## Word embedding: 

> Words' meanings depend not just on immediate neighbors

![](http://103.238.162.29:10002/webdav/sammo3182/figure/theory_wordEmbedding.png){.lightbox fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.
:::

## Topic modeling

<video width="1000" height="600" controls preload>
<source src="http://103.238.162.29:10002/webdav/sammo3182/video/theory_topicModeling.webm" type="video/webm">
</video>

:::{.notes}
LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V × K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## Attention Function

> Word embedding weights all the words equally in the context[🤦‍♂️]{.large}

:::{.fragment .fade-in-then-semi-out}
"Attention is all you need" [@VaswaniEtAl2017]

> As a leading firm in the [MASK] sector, we hire highly skilled software engineers.    
> As a leading firm in the [MASK] sector, we hire highly skilled petroleum engineers.

What's should be filled into the "[MASK]"? How do you figure out?
:::


:::{.fragment}
As a leading firm in the [*information technology*] sector, we hire highly skilled [software]{.red} engineers.

As a leading firm in the [*energy*] sector, we hire highly skilled [petroleum]{.red} engineers.

:::


## Word Sequence

:::{.callout-note}
## Self-attention function

Take as input a sequence of initial token embeddings and outputs a sequence of new token embeddings that [allow the initial embeddings to interact]{.red}.
:::

- Massive neural networks composed of stacked attention and feed forward neural network layerscan be efficiently parallelized for training using specialized processors.
    - A.k.a., the [Transformer]{.red}
- Common transformer models
    - BERT
        - RoBERTa, PALM
    - The [GPT]{.red .large} family


## Wrap it up

:::: {.columns}

::: {.column width="50%"}
- Principles
    1. All models are wrong
    1. Aim for assist human
    1. No best method
    1. Validation, validation, validation
- Levels of analysis
    - Description
    - Classification
    - Semantics
:::

::: {.column width="50%"}
- Procedure
    - Preprocessing
        - Segmentation
        - Tokenization
    - Frequency analysis
        - Frequency, dynamics, correlation
        - Keyword
    - Similarity analysis
    - Semantic analysis
        - Word embedding, sequence
            - Topic modeling
:::

::::


# Thank you {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  
![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://sammo3182.github.io/](https://sammo3182.github.io/)
:::

## References