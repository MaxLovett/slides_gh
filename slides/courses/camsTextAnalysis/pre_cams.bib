@article{RobertsEtAl2013,
  title = {The Structural Topic Model and Applied Social Science},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Airoldi, Edoardo M.},
  date = {2013},
  journaltitle = {Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation},
  volume = {1737},
  pages = {1--4},
  owner = {Sammo},
  timestamp = {2019-10-29T01:14:50Z}
}

@article{RobertsEtAl2014,
  title = {Stm: R Package for Structural Topic Models},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin},
  date = {2014},
  journaltitle = {R Package},
  volume = {1},
  pages = {12},
  owner = {Sammo},
  timestamp = {2019-10-29T01:14:50Z}
}

@article{RobertsEtAl2014a,
  ids = {robertsₛtructural₂014,robertsₛtructural₂014-1},
  title = {Structural Topic Models for Open-Ended Survey Responses},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  date = {2014},
  journaltitle = {American Journal of Political Science},
  volume = {58},
  number = {4},
  pages = {1064--82},
  publisher = {Wiley Online Library},
  owner = {Sammo},
  timestamp = {2019-10-29T01:14:50Z}
}

@article{RobertsEtAl2020,
  title = {Adjusting for Confounding with Text Matching},
  author = {Roberts, Margaret E and Stewart, Brandon M and Nielsen, Richard A},
  date = {2020},
  journaltitle = {American Journal of Political Science},
  pages = {Forthcoming},
  abstract = {We identify situations in which conditioning on text can address confounding in observational studies. We argue that a matching approach is particularly well-suited to this task, but existing matching methods are ill-equipped to handle high-dimensional text data. Our proposed solution is to estimate a low-dimensional summary of the text covariates and condition on this summary via matching. We propose a method of text matching, Topical Inverse Regression Matching, that allows the analyst to match both on the topical content of confounding documents and the probability that each of these documents is treated. We validate and illustrate the importance of conditioning on text to address confounding with two applications: the effect of perceptions of author gender on citation counts in academia and the effects of censorship on Chinese social media users.},
  langid = {english},
  timestamp = {2020-03-27T06:40:58Z},
  file = {D:\zotero_system\storage\GK26D6SV\Robertset alAdjusting for Confounding with Text Matching.pdf}
}

@article{ZhangPan2019,
  title = {CASM: A Deep-Learning Approach for Identifying Collective Action Events with Text and Image Data from Social Media},
  shorttitle = {CASM},
  author = {Zhang, Han and Pan, Jennifer},
  date = {2019-08},
  journaltitle = {Sociological Methodology},
  volume = {49},
  number = {1},
  pages = {1--57},
  issn = {0081-1750},
  doi = {10.1177/0081175019860244},
  abstract = {Protest event analysis is an important method for the study of collective action and social movements and typically draws on traditional media reports as the data source. We introduce collective action from social media (CASM)---a system that uses convolutional neural networks on image data and recurrent neural networks with long short-term memory on text data in a two-stage classifier to identify social media posts about offline collective action. We implement CASM on Chinese social media data and identify more than 100,000 collective action events from 2010 to 2017 (CASM-China). We evaluate the performance of CASM through cross-validation, out-of-sample validation, and comparisons with other protest data sets. We assess the effect of online censorship and find it does not substantially limit our identification of events. Compared to other protest data sets, CASM-China identifies relatively more rural, land-related protests and relatively few collective action events related to ethnic and religious conflict.},
  langid = {english},
  timestamp = {2019-10-04T02:18:45Z},
  file = {D:\zotero_system\storage\N7MQGA2J\Zhang_Pan2019_CASM - A Deep-Learning Approach for Identifying Collective Action Events with.pdf}
}

@article{SuEtAl2020,
  title = {A Pattern Recognition Framework for Detecting Changes in Chinese Internet Management System},
  author = {Su, Yu-Sung and Ruan, Yanqin and Sun, Siyu and Chang, Yu-Tzung},
  date = {2020-09},
  journaltitle = {Journal of Social Computing},
  volume = {1},
  number = {1},
  pages = {28--39},
  issn = {2688-5255},
  doi = {10.23919/JSC.2020.0004},
  abstract = {Past studies on the Chinese Internet management system have revealed a smart Internet management system that takes advantage of time to filter content with collective action potential. How and why such a system was institutionalized? We offer a historical institutional analysis to explain the way in which the system evolved. We implement social network analysis to examine the Weibo posts of recurrent events, the elections in Area A in 2016 and 2018, to identify pattern changes in the system. There are two aspects of the changes: the centralization of the command line to a single authority and the implementation of a discriminatory strategy to deal with the various online expressions together forming this intelligent system. The improved Chinese information surveillance system demonstrates both a top-down information management and a bottom-up opinion formation.},
  keywords = {bottom-up opinion formation,Chinese information surveillance system,Chinese Internet management system,Cyberspace,devolution paradox,historical institutional analysis,Image color analysis,information management,Information management,intelligent system,Internet,internet censorship,pattern changes,pattern recognition,pattern recognition framework,Regulation,smart Internet management system,social network analysis,social networking (online),Social networking (online),top-down information management},
  timestamp = {2020-12-12T00:31:08Z},
  file = {D:\zotero_system\storage\4ZDHV3WC\Suet al2020_A Pattern Recognition Framework for Detecting Changes in Chinese Internet.pdf}
}

@article{GriffithsSteyvers2004,
  title = {Finding Scientific Topics},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  date = {2004},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {101},
  pages = {5228--5235},
  publisher = {National Acad Sciences},
  issue = {suppl 1},
  owner = {Sammo},
  timestamp = {2019-10-29T01:14:50Z},
  file = {D:\Nutstore\01_Literature\Griffiths_Steyvers2004_Finding scientific topics.pdf}
}

@article{Grimmer2010,
  title = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases},
  shorttitle = {A Bayesian Hierarchical Topic Model for Political Texts},
  author = {Grimmer, Justin},
  date = {2010},
  journaltitle = {Political Analysis},
  volume = {18},
  number = {1},
  pages = {1--35},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpp034},
  abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
  langid = {english},
  timestamp = {2019-05-30T02:36:53Z},
  file = {D:\Nutstore\01_Literature\Grimmer2010_A Bayesian Hierarchical Topic Model for Political Texts - Measuring Expressed.pdf}
}

@article{GrimmerStewart2013,
  ids = {grimmerₜext₂013},
  title = {Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts},
  author = {Grimmer, Justin and Stewart, Brandon M.},
  date = {2013},
  journaltitle = {Political Analysis},
  volume = {21},
  number = {3},
  pages = {267--297},
  publisher = {SPM-PMSAPSA},
  timestamp = {2019-10-29T01:14:50Z},
  file = {D:\Nutstore\01_Literature\Grimmer_Stewart2013_Text as Data - The Promise and Pitfalls of Automatic Content Analysis Methods2.pdf}
}

@article{DietrichEtAl2019,
  title = {Pitch Perfect: Vocal Pitch and the Emotional Intensity of Congressional Speech},
  author = {Dietrich, Bryce J. and Hayes, Matthew and O'Brien, Diana Z.},
  date = {2019},
  journaltitle = {American Political Science Review},
  pages = {Forthcoming},
  urldate = {2019-05-30},
  abstract = {Though audio archives are available for a number of political institutions, the data they provide receive scant attention from researchers. Yet, audio data offer important insights, including information about speakers' emotional states. Using one of the largest collections of natural audio ever compiled---74,158 Congressional floor speeches---we introduce a novel measure of legislators' emotional intensity: small changes in vocal pitch that are difficult for speakers to control. Applying our measure to MCs' floor speeches about women, we show that female MCs speak with greater emotional intensity when talking about women as compared to both their male colleagues and their speech on other topics. Our two supplementary analyses suggest that increased vocal pitch is consistent with legislators' broader issue commitments, and that emotionally intense speech may affect other lawmakers' behavior. More generally, by demonstrating the utility of audio-as-data approaches, our work highlights a new way of studying political speech.},
  timestamp = {2019-05-30T02:50:44Z},
  file = {D:\Nutstore\01_Literature\Dietrichet al2019_Pitch Perfect - Vocal Pitch and the Emotional Intensity of Congressional Speech.pdf}
}

@article{BleiEtAl2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {993--1022},
  issn = {ISSN 1533-7928},
  urldate = {2019-02-10},
  issue = {Jan},
  timestamp = {2019-09-13T01:24:25Z},
  file = {D:\Nutstore\01_Literature\BleiLatent Dirichlet Allocation.pdf}
}

@article{BenoitEtAl2016,
  title = {Crowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data},
  shorttitle = {Crowd-Sourced Text Analysis},
  author = {Benoit, Kenneth and Conway, Drew and Lauderdale, Benjamin E. and Laver, Michael and Mikhaylov, Slava},
  date = {2016-05},
  journaltitle = {American Political Science Review},
  volume = {110},
  number = {2},
  pages = {278--295},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055416000058},
  abstract = {Empirical social science often relies on data that are not observed in the field, but are transformed into quantitative variables by expert researchers who analyze and interpret qualitative raw sources. While generally considered the most valid way to produce data, this expert-driven process is inherently difficult to replicate or to assess on grounds of reliability. Using crowd-sourcing to distribute text for reading and interpretation by massive numbers of nonexperts, we generate results comparable to those using experts to read and interpret the same texts, but do so far more quickly and flexibly. Crucially, the data we collect can be reproduced and extended transparently, making crowd-sourced datasets intrinsically reproducible. This focuses researchers' attention on the fundamental scientific objective of specifying reliable and replicable methods for collecting the data needed, rather than on the content of any particular dataset. We also show that our approach works straightforwardly with different types of political text, written in different languages. While findings reported here concern text analysis, they have far-reaching implications for expert-generated data in the social sciences.},
  langid = {english},
  timestamp = {2019-05-30T02:39:34Z},
  file = {D:\Nutstore\01_Literature\Benoitet al2016_Crowd-Sourced Text Analysis - Reproducible and Agile Production of Political Data.pdf}
}

@article{Liu2022,
  title = {Pronoun Usage as a Measure of Power Personalization: A General Theory with Evidence from the Chinese-Speaking World},
  shorttitle = {Pronoun Usage as a Measure of Power Personalization},
  author = {Liu, Amy H.},
  date = {2022-07},
  journaltitle = {British Journal of Political Science},
  volume = {52},
  number = {3},
  pages = {1258--1275},
  publisher = {Cambridge University Press},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S0007123421000181},
  urldate = {2022-07-06},
  abstract = {How can the growing personalization of power be identified and measured ex ante? Extant measures in the authoritarian literature have traditionally focused on institutional constraints and more recently on individual behaviour -- such as purging opposition members from (and packing allies into) government bodies. This article offers a different strategy that examines leaders' individual rhetoric. It focuses on patterns of pronoun usage for the first person. The author argues that as leaders personalize power, they are less likely to use `I' (a pronoun linked to credit claiming and blame minimizing) and more likely to use `we' (the leader speaks for -- or with -- the populace). To test this argument, the study focuses on all major, scheduled speeches by all chief executives in the entire Chinese-speaking world -- that is, China, Singapore and Taiwan -- since independence. It finds a robust pattern between first-person pronouns and political constraints. To ensure the results are not driven by the Chinese sample, the rhetoric of four other political leaders is considered: Albania's Hoxha, North Korea's Kim Il Sung, Hungary's Orb\'an and Ecuador's Correa. The implications of this project suggest that how leaders talk can provide insights into how they perceive their rule.},
  issue = {3},
  langid = {english},
  file = {D\:\\Nutstore\\01_Literature\\Liu2022_Pronoun Usage as a Measure of Power Personalization.pdf;D\:\\Nutstore\\01_Literature\\Liuundefineded_Pronoun Usage as a Measure of Power Personalization.pdf;D\:\\zotero_system\\storage\\SGWK626T\\Liu2022_Pronoun Usage as a Measure of Power Personalization.pdf}
}

@article{EshimaEtAl2023,
  title = {Keyword-Assisted Topic Models},
  author = {Eshima, Shusei and Imai, Kosuke and Sasaki, Tomoya},
  date = {2023},
  journaltitle = {American Journal of Political Science},
  pages = {Early View},
  issn = {1540-5907},
  doi = {10.1111/ajps.12779},
  urldate = {2023-05-01},
  abstract = {In recent years, fully automated content analysis based on probabilistic topic models has become popular among social scientists because of their scalability. However, researchers find that these models often fail to measure specific concepts of substantive interest by inadvertently creating multiple topics with similar content and combining distinct themes into a single topic. In this article, we empirically demonstrate that providing a small number of keywords can substantially enhance the measurement performance of topic models. An important advantage of the proposed keyword-assisted topic model (keyATM) is that the specification of keywords requires researchers to label topics prior to fitting a model to the data. This contrasts with a widespread practice of post hoc topic interpretation and adjustments that compromises the objectivity of empirical findings. In our application, we find that keyATM provides more interpretable results, has better document classification performance, and is less sensitive to the number of topics.},
  langid = {english},
  file = {D\:\\Nutstore\\01_Literature\\Eshima et al2023_Keyword-Assisted Topic Models.pdf;D\:\\Nutstore\\01_Literature\\Eshima et al2023_Keyword-Assisted Topic Models2.pdf}
}

@article{KingEtAl2017,
  title = {Computer-Assisted Keyword and Document Set Discovery from Unstructured Text},
  author = {King, Gary and Lam, Patrick and Roberts, Margaret E.},
  date = {2017},
  journaltitle = {American Journal of Political Science},
  volume = {61},
  number = {4},
  pages = {971--988},
  issn = {1540-5907},
  doi = {10.1111/ajps.12291},
  urldate = {2023-05-03},
  abstract = {The (unheralded) first step in many applications of automated text analysis involves selecting keywords to choose documents from a large text corpus for further study. Although all substantive results depend on this choice, researchers usually pick keywords in ad hoc ways that are far from optimal and usually biased. Most seem to think that keyword selection is easy, since they do Google searches every day, but we demonstrate that humans perform exceedingly poorly at this basic task. We offer a better approach, one that also can help with following conversations where participants rapidly innovate language to evade authorities, seek political advantage, or express creativity; generic web searching; eDiscovery; look-alike modeling; industry and intelligence analysis; and sentiment and topic analysis. We develop a computer-assisted (as opposed to fully automated or human-only) statistical approach that suggests keywords from available text without needing structured data as inputs. This framing poses the statistical problem in a new way, which leads to a widely applicable algorithm. Our specific approach is based on training classifiers, extracting information from (rather than correcting) their mistakes, and summarizing results with easy-to-understand Boolean search strings. We illustrate how the technique works with analyses of English texts about the Boston Marathon bombings, Chinese social media posts designed to evade censorship, and others.},
  issue = {4},
  langid = {english},
  keywords = {Keyword},
  file = {D:\Nutstore\01_Literature\King et al2017_Computer-Assisted Keyword and Document Set Discovery from Unstructured Text3.pdf}
}

@article{WatanabeZhou2022,
  title = {Theory-Driven Analysis of Large Corpora: Semisupervised Topic Classification of the UN Speeches},
  shorttitle = {Theory-Driven Analysis of Large Corpora},
  author = {Watanabe, Kohei and Zhou, Yuan},
  date = {2022-04-01},
  journaltitle = {Social Science Computer Review},
  volume = {40},
  number = {2},
  pages = {346--366},
  publisher = {SAGE Publications Inc},
  issn = {0894-4393},
  doi = {10.1177/0894439320907027},
  urldate = {2023-05-03},
  abstract = {There is a growing interest in quantitative analysis of large corpora among the international relations (IR) scholars, but many of them find it difficult to perform analysis consistently with existing theoretical frameworks using unsupervised machine learning models to further develop the field. To solve this problem, we created a set of techniques that utilize a semisupervised model that allows researchers to classify documents into predefined categories efficiently. We propose a dictionary making procedure to avoid inclusion of words that are likely to confuse the model and deteriorate the its classification performance classification accuracy using a new entropy-based diagnostic tool. In our experiments, we classify sentences of the United Nations General Assembly speeches into six predefined categories using the seeded Latent Dirichlet allocation and Newsmap, which were trained with a small ?seed word dictionary? that we created following the procedure. The result shows that, while keyword dictionary can only classify 25\% of sentences, Newsmap can classify over 60\% of them accurately correctly and; its accuracy exceeds 70\% when contextual information is taken into consideration by kernel smoothing of topic likelihoods. We argue that once seed word dictionaries are created by the international relations community, semisupervised models would become more useful than unsupervised models for theory-driven text analysis.},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Watanabe_Zhou2022_Theory-Driven Analysis of Large Corpora.pdf}
}

@article{RobertsEtAl2016a,
  title = {A Model of Text for Experimentation in the Social Sciences},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
  date = {2016-07-02},
  journaltitle = {Journal of the American Statistical Association},
  volume = {111},
  number = {515},
  pages = {988--1003},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1141684},
  urldate = {2023-05-17},
  abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
  langid = {english},
  file = {D:\zotero_system\storage\4Z9I6K3P\Roberts et al2016_A Model of Text for Experimentation in the Social Sciences.pdf}
}

@article{AshHansen2023,
  title = {Text Algorithms in Economics},
  author = {Ash, Elliott and Hansen, Stephen},
  date = {2023},
  journaltitle = {Annual Review of Economics},
  pages = {Forthcoming},
  abstract = {This paper provides an overview of the methods used for algorithmic text analysis in economics, with a focus on three key contributions. First, the paper introduces methods for representing documents as high-dimensional count vectors over vocabulary terms, for representing words as vectors, and for representing word sequences as embedding vectors. Second, the paper defines four core empirical tasks that encompass most text-as-data research in economics, and enumerates the various approaches that have been taken so far for these tasks. Finally, the paper flags limitations in the current literature, with a focus on the challenge of validating algorithmic output.},
  langid = {italian},
  file = {D:\Nutstore\01_Literature\Ash_Hansen2023_Text Algorithms in Economics.pdf}
}

@article{AshEtAl2023,
  title = {Relatio: Text Semantics Capture Political and Economic Narratives},
  shorttitle = {Relatio},
  author = {Ash, Elliott and Gauthier, Germain and Widmer, Philine},
  date = {2023-04-28},
  journaltitle = {Political Analysis},
  pages = {1--18},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2023.8},
  urldate = {2023-05-19},
  abstract = {Social scientists have become increasingly interested in how narratives---the stories in fiction, politics, and life---shape beliefs, behavior, and government policies. This paper provides an unsupervised method to quantify latent narrative structures in text documents. Our new software package relatio identifies coherent entity groups and maps explicit relations between them in the text. We provide an application to the U.S. Congressional Record to analyze political and economic narratives in recent decades. Our analysis highlights the dynamics, sentiment, polarization, and interconnectedness of narratives in political discourse.},
  langid = {english},
  file = {D\:\\Nutstore\\01_Literature\\Ash et al2023_Relatio.pdf;D\:\\Nutstore\\01_Literature\\Ash et al2023_Relatio3.pdf}
}

@article{Coase1960,
  title = {The Problem of Social Cost},
  author = {Coase, R. H.},
  date = {1960},
  journaltitle = {The Journal of Law \& Economics},
  volume = {56},
  number = {4},
  eprint = {10.1086/674872},
  eprinttype = {jstor},
  pages = {837--877},
  publisher = {[The University of Chicago Press, The Booth School of Business, University of Chicago, The University of Chicago Law School]},
  issn = {0022-2186},
  doi = {10.1086/674872},
  urldate = {2023-05-27},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Coase2013_The Problem of Social Cost.pdf}
}

@book{FriedmanSchwartz2008,
  title = {A Monetary History of the United States, 1867-1960},
  author = {Friedman, Milton and Schwartz, Anna Jacobson},
  date = {2008-09-02},
  eprint = {Q7J_EUM3RfoC},
  eprinttype = {googlebooks},
  publisher = {Princeton University Press},
  abstract = {Writing in the June 1965 issue of theEconomic Journal, Harry G. Johnson begins with a sentence seemingly calibrated to the scale of the book he set himself to review: "The long-awaited monetary history of the United States by Friedman and Schwartz is in every sense of the term a monumental scholarly achievement--monumental in its sheer bulk, monumental in the definitiveness of its treatment of innumerable issues, large and small . . . monumental, above all, in the theoretical and statistical effort and ingenuity that have been brought to bear on the solution of complex and subtle economic issues." Friedman and Schwartz marshaled massive historical data and sharp analytics to support the claim that monetary policy--steady control of the money supply--matters profoundly in the management of the nation's economy, especially in navigating serious economic fluctuations. In their influential chapter 7, The Great Contraction--which Princeton published in 1965 as a separate paperback--they address the central economic event of the century, the Depression. According to Hugh Rockoff, writing in January 1965: "If Great Depressions could be prevented through timely actions by the monetary authority (or by a monetary rule), as Friedman and Schwartz had contended, then the case for market economies was measurably stronger." Milton Friedman won the Nobel Prize in Economics in 1976 for work related to A Monetary History as well as to his other Princeton University Press book, A Theory of the Consumption Function (1957).},
  isbn = {978-1-4008-2933-0},
  langid = {english},
  pagetotal = {889}
}

@inproceedings{PenningtonEtAl2014,
  title = {GloVe: Global Vectors for Word Representation},
  shorttitle = {GloVe},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014-10},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2023-05-27},
  eventtitle = {EMNLP 2014},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Pennington et al2014_GloVe.pdf}
}

@inproceedings{BengioEtAl2000,
  title = {A neural probabilistic language model},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bengio, Yoshua and Ducharme, R\'ejean and Vincent, Pascal},
  date = {2000},
  volume = {13},
  publisher = {MIT Press},
  urldate = {2023-05-27},
  abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
  langid = {catalan},
  file = {D:\Nutstore\01_Literature\Bengio et al2000_A Neural Probabilistic Language Model.pdf}
}

@article{Watanabe2018,
  title = {Newsmap: A semi-supervised approach to geographical news classification},
  author = {Watanabe, Kohei},
  date = {2018},
  journaltitle = {Digital Journalism},
  volume = {6},
  number = {4},
  pages = {294--309},
  doi = {10.1080/21670811.2017.1293487},
  urldate = {2023-06-07},
  abstract = {This paper presents the results of an evaluation of three different types of geographical news classification methods: (1) simple keyword matching, a popular method in media and communications rese...},
  langid = {indonesian}
}

@article{Watanabe2021,
  title = {Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages},
  shorttitle = {Latent Semantic Scaling},
  author = {Watanabe, Kohei},
  date = {2021-04-03},
  journaltitle = {Communication Methods and Measures},
  volume = {15},
  number = {2},
  pages = {81--102},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312458.2020.1832976},
  urldate = {2023-06-08},
  abstract = {Many social scientists recognize that quantitative text analysis is a useful research methodology, but its application is still concentrated in documents written in European languages, especially English, and few sub-fields of political science, such as comparative politics and legislative studies. This seems to be due to the absence of flexible and cost-efficient methods that can be used to analyze documents in different domains and languages. Aiming to solve this problem, this paper proposes a semisupervised document scaling technique, called Latent Semantic Scaling (LSS), which can locate documents on various pre-defined dimensions. LSS achieves this by combining user-provided seed words and latent semantic analysis (word embedding). The article demonstrates its flexibility and efficiency in large-scale sentiment analysis of New York Times articles on the economy and Asahi Shimbun articles on politics. These examples show that LSS can produce results comparable to that of the Lexicoder Sentiment Dictionary (LSD) in both English and Japanese with only small sets of sentiment seed words. A new heuristic method that assists LSS users to choose a near-optimal number of singular values to obtain word vectors that best capture differences between documents on target dimensions is also presented.},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Watanabe2021_Latent Semantic Scaling.pdf}
}

@inproceedings{LuEtAl2011,
  title = {Multi-Aspect Sentiment Analysis with Topic Models},
  booktitle = {2011 IEEE 11th International Conference on Data Mining Workshops},
  author = {Lu, Bin and Ott, Myle and Cardie, Claire and Tsou, Benjamin K.},
  date = {2011-12},
  pages = {81--88},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2011.125},
  abstract = {We investigate the efficacy of topic model based approaches to two multi-aspect sentiment analysis tasks: multi-aspect sentence labeling and multi-aspect rating prediction. For sentence labeling, we propose a weakly-supervised approach that utilizes only minimal prior knowledge - in the form of seed words - to enforce a direct correspondence between topics and aspects. This correspondence is used to label sentences with performance that approaches a fully supervised baseline. For multi-aspect rating prediction, we find that overall ratings can be used in conjunction with our sentence labelings to achieve reasonable performance compared to a fully supervised baseline. When gold-standard aspect-ratings are available, we find that topic model based features can be used to improve unsophisticated supervised baseline performance, in agreement with previous multi-aspect rating prediction work. This improvement is diminished, however, when topic model features are paired with a more competitive supervised baseline - a finding not acknowledged in previous work.},
  eventtitle = {2011 IEEE 11th International Conference on Data Mining Workshops},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Lu et al2011_Multi-aspect Sentiment Analysis with Topic Models.pdf}
}

@inproceedings{WilsonChew2010,
  title = {Term Weighting Schemes for Latent Dirichlet Allocation},
  booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  author = {Wilson, Andrew T. and Chew, Peter A.},
  date = {2010-06},
  pages = {465--473},
  publisher = {Association for Computational Linguistics},
  location = {Los Angeles, California},
  urldate = {2023-06-08},
  eventtitle = {NAACL-HLT 2010},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Wilson_Chew2010_Term Weighting Schemes for Latent Dirichlet Allocation.pdf}
}

@inproceedings{ArunEtAl2010,
  title = {On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations},
  shorttitle = {On Finding the Natural Number of Topics with Latent Dirichlet Allocation},
  booktitle = {Advances in Knowledge Discovery and Data Mining},
  author = {Arun, R. and Suresh, V. and Veni Madhavan, C. E. and Narasimha Murthy, M. N.},
  editor = {Zaki, Mohammed J. and Yu, Jeffrey Xu and Ravindran, B. and Pudi, Vikram},
  date = {2010},
  series = {Lecture Notes in Computer Science},
  pages = {391--402},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-13657-3_43},
  abstract = {It is important to identify the ``correct'' number of topics in mechanisms like Latent Dirichlet Allocation(LDA) as they determine the quality of features that are presented as features for classifiers like SVM. In this work we propose a measure to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the corpus. We show the merit of the measure by applying it on real-world as well as synthetic data sets(both text and images). In proposing this measure, we view LDA as a matrix factorization mechanism, wherein a given corpus C is split into two matrix factors M1 and M2 as given by Cd*w= M1d*tx Qt*w. Where d is the number of documents present in the corpus and w is the size of the vocabulary. The quality of the split depends on ``t'', the right number of topics chosen. The measure is computed in terms of symmetric KL-Divergence of salient distributions that are derived from these matrix factors. We observe that the divergence values are higher for non-optimal number of topics -- this is shown by a 'dip' at the right value for 't'.},
  isbn = {978-3-642-13657-3},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Arun et al2010_On Finding the Natural Number of Topics with Latent Dirichlet Allocation.pdf}
}

@article{CaoEtAl2009,
  title = {A Density-Based Method for Adaptive LDA Model Selection},
  author = {Cao, Juan and Xia, Tian and Li, Jintao and Zhang, Yongdong and Tang, Sheng},
  date = {2009-03-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Advances in Machine Learning and Computational Intelligence},
  volume = {72},
  number = {7},
  pages = {1775--1781},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2008.06.011},
  urldate = {2023-06-08},
  abstract = {Topic models have been successfully used in information classification and retrieval. These models can capture word correlations in a collection of textual documents with a low-dimensional set of multinomial distribution, called ``topics''. However, it is important but difficult to select the appropriate number of topics for a specific dataset. In this paper, we study the inherent connection between the best topic structure and the distances among topics in Latent Dirichlet allocation (LDA), and propose a method of adaptively selecting the best LDA model based on density. Experiments show that the proposed method can achieve performance matching the best of LDA without manually tuning the number of topics.},
  langid = {english}
}

@article{DeveaudEtAl2014,
  title = {Accurate and Effective Latent Concept Modeling for Ad Hoc Information Retrieval},
  author = {Deveaud, Romain and SanJuan, Eric and Bellot, Patrice},
  date = {2014-04-30},
  journaltitle = {Document num\'erique},
  shortjournal = {Document num\'erique},
  volume = {17},
  number = {1},
  pages = {61--84},
  issn = {12795127},
  doi = {10.3166/dn.17.1.61-84},
  urldate = {2023-06-08},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Deveaud et al2014_Accurate and effective latent concept modeling for ad hoc information retrieval.pdf}
}

@article{Ponweiser2012,
  title = {Latent Dirichlet Allocation in R},
  author = {Ponweiser, Martin},
  date = {2012-05-01},
  journaltitle = {Latent Dirichlet Allocation in R},
  series = {Theses / Institute for Statistics and Mathematics},
  publisher = {{WU Vienna University of Economics and Business}},
  location = {Vienna},
  abstract = {Topic models are a new research field within the computer sciences information retrieval and text mining. They are generative probabilistic models of text corpora inferred by machine learning and they can be used for retrieval and text mining tasks. The most prominent topic model is latent Dirichlet allocation (LDA), which was introduced in 2003 by Blei et al. and has since then sparked off the development of other topic models for domain-specific purposes. This thesis focuses on LDA's practical application. Its main goal is the replication of the data analyses from the 2004 LDA paper "Finding scientific topics" by Thomas Griffiths and Mark Steyvers within the framework of the R statistical programming language and the R\textasciitilde package topicmodels by Bettina Gr\"un and Kurt Hornik. The complete process, including extraction of a text corpus from the PNAS journal's website, data preprocessing, transformation into a document-term matrix, model selection, model estimation, as well as presentation of the results, is fully documented and commented. The outcome closely matches the analyses of the original paper, therefore the research by Griffiths/Steyvers can be reproduced. Furthermore, this thesis proves the suitability of the R environment for text mining with LDA.},
  langid = {ngerman}
}

@article{LaverEtAl2003,
  title = {Extracting Policy Positions from Political Texts Using Words as Data},
  author = {Laver, Michael and Benoit, Kenneth and Garry, John},
  date = {2003-05},
  journaltitle = {American Political Science Review},
  volume = {97},
  number = {2},
  pages = {311--331},
  publisher = {Cambridge University Press},
  issn = {1537-5943, 0003-0554},
  doi = {10.1017/S0003055403000698},
  urldate = {2023-06-09},
  abstract = {We present a new way of extracting policy positions from political texts that treats texts not as discourses to be understood and interpreted but rather, as data in the form of words. We compare this approach to previous methods of text analysis and use it to replicate published estimates of the policy positions of political parties in Britain and Ireland, on both economic and social policy dimensions. We ``export'' the method to a non-English-language environment, analyzing the policy positions of German parties, including the PDS as it entered the former West German party system. Finally, we extend its application beyond the analysis of party manifestos, to the estimation of political positions from legislative speeches. Our ``language-blind'' word scoring technique successfully replicates published policy estimates without the substantial costs of time and labor that these require. Furthermore, unlike in any previous method for extracting policy positions from political texts, we provide uncertainty measures for our estimates, allowing analysts to make informed judgments of the extent to which differences between two estimated policy positions can be viewed as significant or merely as products of measurement error.We thank Raj Chari, Gary King, Michael McDonald, Gail McElroy, and three anonymous reviewers for comments on drafts of this paper.},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Laver et al2003_Extracting Policy Positions from Political Texts Using Words as Data.pdf}
}

@article{Lowe2017,
  title = {Understanding Wordscores},
  author = {Lowe, Will},
  date = {2017-01-04},
  journaltitle = {Political Analysis},
  volume = {16},
  number = {4},
  pages = {356--371},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpn004},
  urldate = {2023-06-09},
  abstract = {Wordscores is a widely used procedure for inferring policy positions, or scores, for new documents on the basis of scores for words derived from documents with known scores. It is computationally straightforward, requires no distributional assumptions, but has unresolved practical and theoretical problems. In applications, estimated document scores are on the wrong scale and the theoretical development does not specify a statistical model, so it is unclear what assumptions the method makes about political text and how to tell whether they fit particular text analysis applications. The first part of the paper demonstrates that badly scaled document score estimates reflect deeper problems with the method. The second part shows how to understand Wordscores as an approximation to correspondence analysis which itself approximates a statistical ideal point model for words. Problems with the method are identified with the conditions under which these layers of approximation fail to ensure consistent and unbiased estimation of the parameters of the ideal point model.},
  langid = {afrikaans}
}

@article{SlapinProksch2008,
  title = {A Scaling Model for Estimating Time-Series Party Positions from Texts},
  author = {Slapin, Jonathan B. and Proksch, Sven-Oliver},
  date = {2008},
  journaltitle = {American Journal of Political Science},
  volume = {52},
  number = {3},
  pages = {705--722},
  issn = {1540-5907},
  doi = {10.1111/j.1540-5907.2008.00338.x},
  urldate = {2023-06-09},
  abstract = {Recent advances in computational content analysis have provided scholars promising new ways for estimating party positions. However, existing text-based methods face challenges in producing valid and reliable time-series data. This article proposes a scaling algorithm called WORDFISH to estimate policy positions based on word frequencies in texts. The technique allows researchers to locate parties in one or multiple elections. We demonstrate the algorithm by estimating the positions of German political parties from 1990 to 2005 using word frequencies in party manifestos. The extracted positions reflect changes in the party system more accurately than existing time-series estimates. In addition, the method allows researchers to examine which words are important for placing parties on the left and on the right. We find that words with strong political connotations are the best discriminators between parties. Finally, a series of robustness checks demonstrate that the estimated positions are insensitive to distributional assumptions and document selection.},
  langid = {english},
  file = {D:\Nutstore\01_Literature\Slapin_Proksch2008_A Scaling Model for Estimating Time-Series Party Positions from Texts.pdf}
}

@incollection{King2016,
  title = {Preface: Big Data Is Not about the Data!},
  booktitle = {Computational Social Science: Discovery and Prediction},
  author = {King, Gary},
  editor = {Alvarez, R. Michael},
  date = {2016},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  langid = {english},
  file = {D:\zotero_system\storage\ KingAlvarez2016_Preface big data is not about the data!.pdf}
}

@inproceedings{VaswaniEtAl2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-07},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  langid = {english},
  file = {D:\zotero_system\storage\HPJE38BB\Vaswani et al2017_Attention is All you Need.pdf}
}

@unpublished{King2015,
  type = {Guest talk},
  title = {Big Data Is Not About the Data!},
  author = {King, Gary},
  date = {2015-11-11},
  urldate = {2024-06-17},
  eventtitle = {Talk at the Capital Markets Cooperative Research Centre},
  langid = {english},
  venue = {Sydney, Australia},
  file = {D:\zotero_system\storage\DLKB4RWZ\ KingBig Data is Not About the Data!.pdf}
}

@report{Kemp2024,
  title = {Digital 2024: Global Overview Report},
  author = {Kemp, Simon},
  date = {2024},
  institution = {Meltwater},
  urldate = {2024-06-17},
  file = {D:\zotero_system\storage\NITWKFVL\ 2024年全球数字化营销洞察报告-50亿社交媒体用户_Meltwater融文_2024.pdf.pdf}
}

@report{IDCFutureScape2024,
  title = {2024年中国数据和分析市场十大预测},
  author = {{IDC FutureScape}},
  date = {2024-01-24},
  institution = {IDC Media Center},
  urldate = {2024-06-17},
  abstract = {IDC examines consumer markets by devices, applications, networks, and services to provide complete solutions for succeeding in these expanding markets.},
  file = {D:\zotero_system\storage\UVLB4R2G\getdoc.html}
}

@online{YanZhiHongYanFuJing2024,
  title = {最新报告出炉！2023年我国数据生产总量达32.85ZB},
  author = {{颜之宏} and {严赋憬}},
  date = {2024-05-24},
  urldate = {2024-06-17},
  langid = {chinese},
  organization = {新华网},
  file = {D:\zotero_system\storage\CYXLJ5JC\c.html}
}

@incollection{Gabrielatos2018,
  title = {Keyness Analysis: Nature, Metrics and Techniques},
  shorttitle = {Keyness Analysis},
  booktitle = {Corpus Approaches to Discourse},
  author = {Gabrielatos, Costas},
  editor = {Taylor, Charlotte and Marchi, Anna},
  date = {2018},
  pages = {34--65},
  publisher = {Routledge},
  abstract = {This chapter discusses methodological issues relating to keyness analysis, and addresses a number of interconnected themes. It offers awareness of relevant},
  isbn = {978-1-315-17934-6},
  pagetotal = {34}
}

@online{IBM2021,
  title = {What Is Machine Learning (ML)?},
  shorttitle = {What Is Machine Learning (ML)?},
  author = {{IBM}},
  date = {2021-09-22T00:00:00.000},
  urldate = {2024-08-21},
  abstract = {Machine learning (ML) is a branch of AI and computer science that focuses on the using data and algorithms to enable AI to imitate the way that humans learn.},
  langid = {american},
  organization = {Think: Tech news, education and events}
}
